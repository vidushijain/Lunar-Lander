{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Lunar Lander - Task 1\n",
    "\n",
    "Student Name- Vidushi Jain <br/>\n",
    "Student Number- 18200009 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading https://files.pythonhosted.org/packages/81/a7/4179e6ebfd654bd0eac0b9c06125b8b4c96a9d0a8ff9e9507eb2a26d2d7e/imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/4c/7557e1c2e791bd43878f8c82065bddc5798252084f26ef44527c02262af1/imbalanced_learn-0.4.3-py3-none-any.whl (166kB)\n",
      "\u001b[K    100% |████████████████████████████████| 174kB 4.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /anaconda3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.16.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /anaconda3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (0.20.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /anaconda3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.2.1)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.4.3 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import RMSprop, adam\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import csv\n",
    "import os \n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import scipy as sp\n",
    "import PIL\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Train a supervised machine learning model to control the Lunar Lander craft based on the image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are setting the path to <b>train_data_dir </b>and setting the <b>sample_rate</b> to 0.1. This means our notebook will use only 10% of the data. Also I am using <b>'channels_first'</b> image data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_first\n"
     ]
    }
   ],
   "source": [
    "# dataset name\n",
    "dataset_name = 'LunarLanderFramesPart1'\n",
    "\n",
    "# All data is present in this folder\n",
    "train_data_dir = dataset_name + '/'\n",
    "\n",
    "# Set up some parmaeters for data loading\n",
    "\n",
    "sample_rate = 0.1\n",
    "K.set_image_data_format('channels_first')\n",
    "#I am using TensorFlow Backend\n",
    "#input_shape = (img_width, img_height, Channels)\n",
    "\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code we are reading the images from the training data directory inside a loop and for each image we are first converting the images to <b>greyscale</b> and then <b>shrinking the images</b> to 84*84 from original size of 1200*800. After that we are <b>reshaping</b> the image shape from (84,84) to (1,1,84,84). In our case we are using channels_first architecture, that's why reshaping it to include channel information also. Then we are adding the image data to the data array and normalizing it. We are also <b>extracting the state of the game</b> from the image filename and storing it in y_data. <br/>\n",
    "Please Note- The information given by the 3 channel and the 1 channel gray scale image is same but in terms of weights and biases of the CNN, the CNN coming from 3 channel images will be bigger and so it will take more time to compute. That's why we have taken one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 6367\n",
      "Processed 1000 of 6367\n",
      "Processed 2000 of 6367\n",
      "Processed 3000 of 6367\n",
      "Processed 4000 of 6367\n",
      "Processed 5000 of 6367\n",
      "Processed 6000 of 6367\n"
     ]
    }
   ],
   "source": [
    "# Initialise arrays for data storage\n",
    "#X_data = np.ndarray((0, input_shape[0], input_shape[1], input_shape[2]), dtype=np.float)\n",
    "\n",
    "# desired dimensions of our images.\n",
    "ROWS = 84\n",
    "COLS = 84\n",
    "CHANNELS = 1\n",
    "\n",
    "input_shape=(CHANNELS, ROWS, COLS)\n",
    "\n",
    "# generate filenames from the data folder and do sampling\n",
    "image_filenames = [train_data_dir+i for i in os.listdir(train_data_dir) if not i.startswith('.')] # use this for full dataset\n",
    "image_filenames = random.sample(image_filenames, int(len(image_filenames)*sample_rate))\n",
    "\n",
    "# Create a data array for image data\n",
    "count = len(image_filenames)\n",
    "X_data = np.ndarray((count, CHANNELS, ROWS, COLS), dtype=np.float)\n",
    "y_data= np.ndarray(0, dtype=np.int)\n",
    "# Iterate throuigh the filenames and for each one load the image, resize and normalise\n",
    "for i, image_file in enumerate(image_filenames): \n",
    "    #Reading the image and converting it to grayscale\n",
    "    image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "    #Shrinking the images to 84*84 from original size of 1200*800\n",
    "    image = cv2.resize(image, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "    #Reshaping the image shape from (84,84) to (1,1,84,84)\n",
    "    processed_image = image.reshape((1, CHANNELS, ROWS, COLS))\n",
    "    # Add image data to data array and normalise\n",
    "    X_data[i] = processed_image\n",
    "    X_data[i] = X_data[i]/255\n",
    "        \n",
    "    # Add label to label array\n",
    "    #Extracting the state of the game from the image filename\n",
    "    y_data = np.append(y_data,int(image_file[-6:-5]) )\n",
    "        \n",
    "    if i%1000 == 0: \n",
    "        print('Processed {} of {}'.format(i, count)) #prints progress of precessed images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6367, 1, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape: {}\".format(X_data.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out the distribution of the y_data that contains the various state of the game. <br/>\n",
    "<b>0- none <br/>\n",
    "1- up <br/>\n",
    "2- left <br/>\n",
    "3- right </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2c0af0f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We can see that classes are not balanced. </b> We will perform UnderSampling in Model 2 to handle class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining various dictionaries to store the result of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_accuracy_comparisons = dict()\n",
    "model_test_precision_comparisons = dict()\n",
    "model_test_recall_comparisons = dict()\n",
    "model_test_f1_comparisons = dict()\n",
    "model_train_time_comparisons = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  For this task, we have tried six different model architecture where we experimented with parameters like:\n",
    "1) Handling Class imbalance problem - Model 1 <br/> 2) Changing the number of hidden layers - Model 3 <br/> 3) Adding padding - Model 4 <br/> 4) Adding drop out layer - Model 5 <br/> 5) Changing the kernel size - Model 5 <br/> 6) Changing the number of filters - Model 6 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 \n",
    "<a id=\"model1_architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "We are creating a sequential model in which we are creating the model layer-by-layer. The architecture consists of convolutional layers, max pooling layers, dropout layers and fully connected layers. In Model 1, we have 3 convolutional layers. <br/> <br/>\n",
    "1) <b>First convolutional layer</b> consists consists of <b>32 filters</b> each of <b>size 3 x 3</b>. We have also specified the input shape in the first layer, which is 1 x84 x 84. We are using <b>Rectified linear unit (ReLU)</b> activation function for <b>all the layers</b> except the final output layer. <br/>\n",
    "2) The next layer is a <b>pooling layer</b>. The pooling layers are used to reduce dimension. In our model we are using <b>Max Pooling</b> with a <b>2x2 window </b>. This will considers the maximum value in the 2x2 window. <br/>\n",
    "3) <b>Second convolutional</b> layer consists consists of <b>32 filters</b> each of<b> size 3 x 3</b>. Followed by another max pooling layer of 2x2 window. <br/>\n",
    "4) <b>Third convolutional layer </b>consists consists of <b>64 filters </b>each of size 3 x 3. Followed by another max pooling layer of 2x2 window. Usually, the number of filters in the convolutional layer grows after each layer. The first layers with lower number of filter learns simple features of the images whereas the deeper layers learn more complex features. <br/>\n",
    "5) After that we have added a <b>flatten layer</b> that flattened the 3D feature map output from the convolutional layer to 1D feature vectors before adding in the fully connected layers.<br/>\n",
    "6) Next layer is the <b>dense layer</b> (fully connected layer) that has <b>256 neurons</b>. This is followed by <b>dropout layer</b> with a dropout rate of 0.5. A dropout layer with dropout rate of 0.5 means 50% of the neurons will be turned off randomly. This helps prevent overfitting by making all the neurons learn something about the data and not rely on just a few neurons. Thus generalizing better and prevent overfitting. <br/>\n",
    "7) The final output layer is another <b>dense layer</b> which has number of neurons equal to the number of classes. Since it is a multi-class classification problem, the activation function is set to <b>softmax</b>\n",
    "\n",
    "<b> <font color=\"red\">In Model 1 we are not handling class imbalance problem </font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom split to train, validation, test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X_data, y_data, random_state=0, test_size = 0.30, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4456, 1, 84, 84)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4456,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape output data for use with a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of clases\n",
    "num_classes = len(set(y_data))\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_train_num = y_train_encoder.fit_transform(y_train)\n",
    "y_train_wide = keras.utils.to_categorical(y_train_num, num_classes)\n",
    "\n",
    "y_test_num = y_train_encoder.fit_transform(y_test)\n",
    "y_test_wide = keras.utils.to_categorical(y_test_num, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the number to label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_num_label = dict()\n",
    "\n",
    "for idx, lbl in enumerate(y_train_encoder.classes_):\n",
    "    classes_num_label[idx] = lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a few randomly sampled example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4456, 1, 84, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=4\n",
    "row_images = 3\n",
    "col_images = 5\n",
    "plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "for i in range(row_images * col_images):\n",
    "    i_rand = random.randint(0, X_train.shape[0])\n",
    "    plt.subplot(row_images,col_images,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sp.misc.toimage(X_train[i_rand][0]),cmap='gray')\n",
    "    plt.title((\"Action Number \",y_train[i_rand]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is compiled with binary cross entropy loss function and the Adam optimizer is used. The ‘accuracy’ metric is used to evaluate the model. Adam is an optimization algorithm that updates the network weights in an iterative manner. <b>For Adam Optimizer, we have taken the default value for learning rate (lr=0.001).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 82, 82)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 82, 82)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 41, 41)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 39, 39)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 39, 39)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 19, 19)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 17, 17)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 17, 17)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,077,924\n",
      "Trainable params: 1,077,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. We allow the model to overfit but save the best set of weights based on validation loss as we go. Then at the end of training (assuming the model has overfit) we revert back to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3564 samples, validate on 892 samples\n",
      "Epoch 1/50\n",
      "3564/3564 [==============================] - 43s 12ms/step - loss: 0.4056 - acc: 0.7947 - val_loss: 0.3448 - val_acc: 0.8523\n",
      "Epoch 2/50\n",
      "3564/3564 [==============================] - 48s 13ms/step - loss: 0.3488 - acc: 0.8496 - val_loss: 0.3251 - val_acc: 0.8590\n",
      "Epoch 3/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.3240 - acc: 0.8610 - val_loss: 0.3153 - val_acc: 0.8697\n",
      "Epoch 4/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.3129 - acc: 0.8672 - val_loss: 0.3002 - val_acc: 0.8711\n",
      "Epoch 5/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.3045 - acc: 0.8664 - val_loss: 0.2988 - val_acc: 0.8688\n",
      "Epoch 6/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.2960 - acc: 0.8697 - val_loss: 0.2976 - val_acc: 0.8694\n",
      "Epoch 7/50\n",
      "3564/3564 [==============================] - 48s 14ms/step - loss: 0.2947 - acc: 0.8683 - val_loss: 0.2976 - val_acc: 0.8711\n",
      "Epoch 8/50\n",
      "3564/3564 [==============================] - 48s 14ms/step - loss: 0.2918 - acc: 0.8701 - val_loss: 0.2950 - val_acc: 0.8694\n",
      "Epoch 9/50\n",
      "3564/3564 [==============================] - 48s 14ms/step - loss: 0.2879 - acc: 0.8714 - val_loss: 0.3003 - val_acc: 0.8711\n",
      "Epoch 10/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.2818 - acc: 0.8728 - val_loss: 0.2995 - val_acc: 0.8686\n",
      "Epoch 11/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.2792 - acc: 0.8742 - val_loss: 0.2942 - val_acc: 0.8663\n",
      "Epoch 12/50\n",
      "3564/3564 [==============================] - 51s 14ms/step - loss: 0.2742 - acc: 0.8749 - val_loss: 0.2952 - val_acc: 0.8711\n",
      "Epoch 13/50\n",
      "3564/3564 [==============================] - 50s 14ms/step - loss: 0.2705 - acc: 0.8782 - val_loss: 0.2997 - val_acc: 0.8722\n",
      "Epoch 14/50\n",
      "3564/3564 [==============================] - 52s 15ms/step - loss: 0.2645 - acc: 0.8790 - val_loss: 0.2973 - val_acc: 0.8716\n",
      "Epoch 15/50\n",
      "3564/3564 [==============================] - 49s 14ms/step - loss: 0.2596 - acc: 0.8823 - val_loss: 0.2959 - val_acc: 0.8716\n",
      "Epoch 16/50\n",
      "3564/3564 [==============================] - 49s 14ms/step - loss: 0.2533 - acc: 0.8859 - val_loss: 0.3014 - val_acc: 0.8708\n",
      "Epoch 17/50\n",
      "3564/3564 [==============================] - 54s 15ms/step - loss: 0.2459 - acc: 0.8902 - val_loss: 0.3010 - val_acc: 0.8736\n",
      "Epoch 18/50\n",
      "3564/3564 [==============================] - 53s 15ms/step - loss: 0.2423 - acc: 0.8918 - val_loss: 0.3060 - val_acc: 0.8697\n",
      "Epoch 19/50\n",
      "3564/3564 [==============================] - 48s 14ms/step - loss: 0.2360 - acc: 0.8943 - val_loss: 0.3098 - val_acc: 0.8674\n",
      "Epoch 20/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.2281 - acc: 0.8997 - val_loss: 0.3165 - val_acc: 0.8700\n",
      "Epoch 21/50\n",
      "3564/3564 [==============================] - 45s 13ms/step - loss: 0.2280 - acc: 0.8981 - val_loss: 0.3140 - val_acc: 0.8686\n",
      "Epoch 22/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.2168 - acc: 0.9050 - val_loss: 0.3282 - val_acc: 0.8694\n",
      "Epoch 23/50\n",
      "3564/3564 [==============================] - 44s 12ms/step - loss: 0.2119 - acc: 0.9031 - val_loss: 0.3352 - val_acc: 0.8719\n",
      "Epoch 24/50\n",
      "3564/3564 [==============================] - 44s 12ms/step - loss: 0.2031 - acc: 0.9140 - val_loss: 0.3577 - val_acc: 0.8716\n",
      "Epoch 25/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.2009 - acc: 0.9123 - val_loss: 0.3410 - val_acc: 0.8702\n",
      "Epoch 26/50\n",
      "3564/3564 [==============================] - 45s 13ms/step - loss: 0.1926 - acc: 0.9156 - val_loss: 0.3497 - val_acc: 0.8576\n",
      "Epoch 27/50\n",
      "3564/3564 [==============================] - 46s 13ms/step - loss: 0.1827 - acc: 0.9220 - val_loss: 0.3597 - val_acc: 0.8649\n",
      "Epoch 28/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.1790 - acc: 0.9211 - val_loss: 0.3837 - val_acc: 0.8722\n",
      "Epoch 29/50\n",
      "3564/3564 [==============================] - 48s 13ms/step - loss: 0.1706 - acc: 0.9271 - val_loss: 0.3752 - val_acc: 0.8613\n",
      "Epoch 30/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.1656 - acc: 0.9289 - val_loss: 0.3987 - val_acc: 0.8674\n",
      "Epoch 31/50\n",
      "3564/3564 [==============================] - 48s 14ms/step - loss: 0.1632 - acc: 0.9293 - val_loss: 0.3940 - val_acc: 0.8627\n",
      "Epoch 32/50\n",
      "3564/3564 [==============================] - 48s 13ms/step - loss: 0.1591 - acc: 0.9315 - val_loss: 0.3998 - val_acc: 0.8604\n",
      "Epoch 33/50\n",
      "3564/3564 [==============================] - 46s 13ms/step - loss: 0.1477 - acc: 0.9343 - val_loss: 0.4166 - val_acc: 0.8652\n",
      "Epoch 34/50\n",
      "3564/3564 [==============================] - 45s 13ms/step - loss: 0.1394 - acc: 0.9404 - val_loss: 0.4531 - val_acc: 0.8601\n",
      "Epoch 35/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.1444 - acc: 0.9385 - val_loss: 0.4707 - val_acc: 0.8655\n",
      "Epoch 36/50\n",
      "3564/3564 [==============================] - 48s 13ms/step - loss: 0.1373 - acc: 0.9421 - val_loss: 0.4412 - val_acc: 0.8529\n",
      "Epoch 37/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.1371 - acc: 0.9423 - val_loss: 0.4668 - val_acc: 0.8652\n",
      "Epoch 38/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.1281 - acc: 0.9432 - val_loss: 0.4640 - val_acc: 0.8646\n",
      "Epoch 39/50\n",
      "3564/3564 [==============================] - 48s 14ms/step - loss: 0.1143 - acc: 0.9524 - val_loss: 0.4992 - val_acc: 0.8643\n",
      "Epoch 40/50\n",
      "3564/3564 [==============================] - 49s 14ms/step - loss: 0.1146 - acc: 0.9522 - val_loss: 0.5003 - val_acc: 0.8610\n",
      "Epoch 41/50\n",
      "3564/3564 [==============================] - 50s 14ms/step - loss: 0.1128 - acc: 0.9503 - val_loss: 0.5170 - val_acc: 0.8658\n",
      "Epoch 42/50\n",
      "3564/3564 [==============================] - 49s 14ms/step - loss: 0.1076 - acc: 0.9548 - val_loss: 0.5321 - val_acc: 0.8638\n",
      "Epoch 43/50\n",
      "3564/3564 [==============================] - 46s 13ms/step - loss: 0.1019 - acc: 0.9595 - val_loss: 0.5114 - val_acc: 0.8543\n",
      "Epoch 44/50\n",
      "3564/3564 [==============================] - 45s 13ms/step - loss: 0.0964 - acc: 0.9611 - val_loss: 0.5754 - val_acc: 0.8618\n",
      "Epoch 45/50\n",
      "3564/3564 [==============================] - 48s 13ms/step - loss: 0.0998 - acc: 0.9579 - val_loss: 0.5449 - val_acc: 0.8677\n",
      "Epoch 46/50\n",
      "3564/3564 [==============================] - 49s 14ms/step - loss: 0.0930 - acc: 0.9630 - val_loss: 0.5493 - val_acc: 0.8621\n",
      "Epoch 47/50\n",
      "3564/3564 [==============================] - 45s 13ms/step - loss: 0.0860 - acc: 0.9637 - val_loss: 0.6036 - val_acc: 0.8610\n",
      "Epoch 48/50\n",
      "3564/3564 [==============================] - 46s 13ms/step - loss: 0.0839 - acc: 0.9655 - val_loss: 0.6005 - val_acc: 0.8711\n",
      "Epoch 49/50\n",
      "3564/3564 [==============================] - 47s 13ms/step - loss: 0.0819 - acc: 0.9674 - val_loss: 0.6126 - val_acc: 0.8596\n",
      "Epoch 50/50\n",
      "3564/3564 [==============================] - 48s 14ms/step - loss: 0.0820 - acc: 0.9659 - val_loss: 0.6134 - val_acc: 0.8610\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "# Set up the callback to save the best model based on validaion data\n",
    "best_weights_filepath = './best_weights_notebook_model1.hdf5'\n",
    "mcp = ModelCheckpoint(best_weights_filepath, monitor=\"val_acc\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train_wide,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.2,\n",
    "          shuffle=True,\n",
    "          callbacks=[mcp])\n",
    "end = time.time()\n",
    "timetaken=end - start\n",
    "model_train_time_comparisons['Model 1'] = timetaken\n",
    "#reload best weights\n",
    "model.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded the best model saved by ModelCheckpoint and use the predict function to predict the classes of the images in the array X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73992673992674\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.65      0.73       894\n",
      "           1       0.40      0.07      0.11        60\n",
      "           2       0.69      0.92      0.79       896\n",
      "           3       0.00      0.00      0.00        61\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1911\n",
      "   macro avg       0.48      0.41      0.41      1911\n",
      "weighted avg       0.73      0.74      0.72      1911\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585</td>\n",
       "      <td>4</td>\n",
       "      <td>305</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>825</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>702</td>\n",
       "      <td>10</td>\n",
       "      <td>1199</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1     2   All\n",
       "True                          \n",
       "0          585   4   305   894\n",
       "1           25   4    31    60\n",
       "2           69   2   825   896\n",
       "3           23   0    38    61\n",
       "All        702  10  1199  1911"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test_num, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Model 1\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test_num, y_pred))\n",
    "report = metrics.classification_report(y_test_num, y_pred)\n",
    "model_test_precision_comparisons[\"Model 1\"] = report.split('\\n')[-2].split('      ')[1]\n",
    "model_test_recall_comparisons[\"Model 1\"] = report.split('\\n')[-2].split('      ')[2]\n",
    "model_test_f1_comparisons[\"Model 1\"] = report.split('\\n')[-2].split('      ')[3]\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test_num), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting 73.5% Accuracy by this model. But their is class imbalance problem in the dataset, so we need to fix that problem. We can see from the confusion matrix, our model is not able to predict class 1 and class 3. Instead of them, it is simply predicting the majority class which is class 0 and class 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Persist A Model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model1_part1.mod\"\n",
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "\n",
    "\n",
    "In this we have used the same Architecture as [Model 1 Architecture](#model1_architecture). But we have <b>handled the class imbalance</b> problem by using <b>undersampling of data </b>with the help of imblearn.RandomUnderSampler. In Undersampling we removing some observations of the majority class so as to balance the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom split to train and test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X_data, y_data, random_state=0, test_size = 0.30, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4456, 1, 84, 84)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1911, 1, 84, 84)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape output data for use with a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of clases\n",
    "num_classes = len(set(y_data))\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_train_num = y_train_encoder.fit_transform(y_train)\n",
    "y_train_wide = keras.utils.to_categorical(y_train_num, num_classes)\n",
    "\n",
    "y_test_num = y_train_encoder.fit_transform(y_test)\n",
    "y_test_wide = keras.utils.to_categorical(y_test_num, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x103791160>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4456, 1, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Apply under sampling to balance the training dataset</b>\n",
    "\n",
    "Undersampling in data analysis is a technique used to adjust the class distribution of a data set that is the ratio between the different classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 7056)\n",
      "(576,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(return_indices=True)\n",
    "#We also need to reshape X_train and X_test to use the Random Under Sampler\n",
    "X_train_rus, y_train_rus, idx_resampled =rus.fit_sample(X_train.reshape(len(X_train), ROWS*COLS*CHANNELS), y_train)\n",
    "\n",
    "print(X_train_rus.shape)\n",
    "print(y_train_rus.shape)\n",
    "\n",
    "X_train_rus, y_train_rus = shuffle(X_train_rus, y_train_rus)\n",
    "\n",
    "X_train_rus = X_train_rus.reshape(len(X_train_rus), CHANNELS,ROWS, COLS)\n",
    "\n",
    "# Plot a bar plot of the labels\n",
    "sns.countplot(y_train_rus) \n",
    "#class distribution is adjusted\n",
    "\n",
    "# convert to binary encoded labels\n",
    "y_train_rus_wide = keras.utils.to_categorical(y_train_rus, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 1, 84, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=4\n",
    "row_images = 2\n",
    "col_images = 4\n",
    "plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n",
    "\n",
    "print(X_train_rus.shape)\n",
    "\n",
    "for i in range(row_images * col_images):\n",
    "    i_rand = random.randint(0, X_train_rus.shape[0])\n",
    "    plt.subplot(row_images,col_images,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sp.misc.toimage(X_train_rus[i_rand][0]),cmap='gray')\n",
    "    plt.title((\"Action Number \",y_train_rus[i_rand]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Modelling </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is compiled with binary cross entropy loss function and the Adam optimizer is used. The ‘accuracy’ metric is used to evaluate the model. Adam is an optimization algorithm that updates the network weights in an iterative manner. <b>For Adam Optimizer, we have taken the default value for learning rate (lr=0.001).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 82, 82)        320       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 82, 82)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 41, 41)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 39, 39)        9248      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 39, 39)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 19, 19)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 17, 17)        18496     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64, 17, 17)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,077,924\n",
      "Trainable params: 1,077,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Conv2D(32, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(256))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(num_classes))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. We allow the model to overfit but save the best set of weights based on validation loss as we go. Then at the end of training (assuming the model has overfit) we revert back to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 460 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5624 - acc: 0.7500 - val_loss: 0.5627 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5529 - acc: 0.7500 - val_loss: 0.5529 - val_acc: 0.7500\n",
      "Epoch 3/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.5429 - acc: 0.7500 - val_loss: 0.5336 - val_acc: 0.7565\n",
      "Epoch 4/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5204 - acc: 0.7560 - val_loss: 0.5190 - val_acc: 0.7716\n",
      "Epoch 5/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.5059 - acc: 0.7554 - val_loss: 0.5203 - val_acc: 0.7694\n",
      "Epoch 6/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4829 - acc: 0.7745 - val_loss: 0.5047 - val_acc: 0.7716\n",
      "Epoch 7/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4696 - acc: 0.7685 - val_loss: 0.4938 - val_acc: 0.7672\n",
      "Epoch 8/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4667 - acc: 0.7810 - val_loss: 0.5170 - val_acc: 0.7629\n",
      "Epoch 9/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4530 - acc: 0.7902 - val_loss: 0.4955 - val_acc: 0.7802\n",
      "Epoch 10/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4351 - acc: 0.7957 - val_loss: 0.5003 - val_acc: 0.7780\n",
      "Epoch 11/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4266 - acc: 0.8027 - val_loss: 0.5046 - val_acc: 0.7651\n",
      "Epoch 12/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4129 - acc: 0.8087 - val_loss: 0.5008 - val_acc: 0.7586\n",
      "Epoch 13/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.4161 - acc: 0.8082 - val_loss: 0.4989 - val_acc: 0.7651\n",
      "Epoch 14/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4002 - acc: 0.8103 - val_loss: 0.5144 - val_acc: 0.7522\n",
      "Epoch 15/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3922 - acc: 0.8196 - val_loss: 0.4995 - val_acc: 0.7565\n",
      "Epoch 16/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3793 - acc: 0.8391 - val_loss: 0.5043 - val_acc: 0.7565\n",
      "Epoch 17/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3643 - acc: 0.8408 - val_loss: 0.5217 - val_acc: 0.7586\n",
      "Epoch 18/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3661 - acc: 0.8375 - val_loss: 0.5071 - val_acc: 0.7672\n",
      "Epoch 19/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3544 - acc: 0.8451 - val_loss: 0.5299 - val_acc: 0.7586\n",
      "Epoch 20/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3444 - acc: 0.8473 - val_loss: 0.5374 - val_acc: 0.7716\n",
      "Epoch 21/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3353 - acc: 0.8435 - val_loss: 0.5374 - val_acc: 0.7672\n",
      "Epoch 22/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3246 - acc: 0.8565 - val_loss: 0.5301 - val_acc: 0.7780\n",
      "Epoch 23/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3176 - acc: 0.8641 - val_loss: 0.5463 - val_acc: 0.7586\n",
      "Epoch 24/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3088 - acc: 0.8620 - val_loss: 0.5283 - val_acc: 0.7931\n",
      "Epoch 25/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3032 - acc: 0.8690 - val_loss: 0.5523 - val_acc: 0.7629\n",
      "Epoch 26/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2772 - acc: 0.8826 - val_loss: 0.5699 - val_acc: 0.7543\n",
      "Epoch 27/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2751 - acc: 0.8793 - val_loss: 0.5620 - val_acc: 0.7845\n",
      "Epoch 28/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2742 - acc: 0.8832 - val_loss: 0.5645 - val_acc: 0.7629\n",
      "Epoch 29/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2551 - acc: 0.8957 - val_loss: 0.5753 - val_acc: 0.7845\n",
      "Epoch 30/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2540 - acc: 0.9005 - val_loss: 0.5972 - val_acc: 0.7543\n",
      "Epoch 31/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2457 - acc: 0.8940 - val_loss: 0.5931 - val_acc: 0.7802\n",
      "Epoch 32/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2391 - acc: 0.9027 - val_loss: 0.6186 - val_acc: 0.7565\n",
      "Epoch 33/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2345 - acc: 0.9022 - val_loss: 0.6226 - val_acc: 0.7651\n",
      "Epoch 34/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2206 - acc: 0.9174 - val_loss: 0.6187 - val_acc: 0.7737\n",
      "Epoch 35/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2076 - acc: 0.9245 - val_loss: 0.6636 - val_acc: 0.7586\n",
      "Epoch 36/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1975 - acc: 0.9201 - val_loss: 0.6479 - val_acc: 0.7802\n",
      "Epoch 37/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.1926 - acc: 0.9261 - val_loss: 0.6866 - val_acc: 0.7543\n",
      "Epoch 38/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.1758 - acc: 0.9304 - val_loss: 0.7217 - val_acc: 0.7565\n",
      "Epoch 39/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1791 - acc: 0.9255 - val_loss: 0.7384 - val_acc: 0.7522\n",
      "Epoch 40/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1815 - acc: 0.9315 - val_loss: 0.7186 - val_acc: 0.7780\n",
      "Epoch 41/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1636 - acc: 0.9359 - val_loss: 0.7210 - val_acc: 0.7608\n",
      "Epoch 42/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1613 - acc: 0.9380 - val_loss: 0.7876 - val_acc: 0.7457\n",
      "Epoch 43/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1520 - acc: 0.9424 - val_loss: 0.7975 - val_acc: 0.7457\n",
      "Epoch 44/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1363 - acc: 0.9505 - val_loss: 0.8108 - val_acc: 0.7565\n",
      "Epoch 45/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1442 - acc: 0.9446 - val_loss: 0.8504 - val_acc: 0.7414\n",
      "Epoch 46/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1497 - acc: 0.9364 - val_loss: 0.8541 - val_acc: 0.7565\n",
      "Epoch 47/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1320 - acc: 0.9446 - val_loss: 0.8765 - val_acc: 0.7435\n",
      "Epoch 48/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1436 - acc: 0.9413 - val_loss: 0.8653 - val_acc: 0.7629\n",
      "Epoch 49/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1253 - acc: 0.9538 - val_loss: 0.9076 - val_acc: 0.7478\n",
      "Epoch 50/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1131 - acc: 0.9614 - val_loss: 0.8936 - val_acc: 0.7478\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "# Set up the callback to save the best model based on validaion data\n",
    "best_weights_filepath = './best_weights_notebook_model2.hdf5'\n",
    "mcp = ModelCheckpoint(best_weights_filepath, monitor=\"val_acc\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model2.fit(X_train_rus, y_train_rus_wide,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.2,\n",
    "          shuffle=True,\n",
    "          callbacks=[mcp])\n",
    "\n",
    "end = time.time()\n",
    "timetaken=end - start\n",
    "model_train_time_comparisons['Model 2'] = timetaken\n",
    "\n",
    "#reload best weights\n",
    "model2.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded the best model saved by ModelCheckpoint and use the predict function to predict the classes of the images in the array X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5013082155939299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.57      0.65       894\n",
      "           1       0.11      0.57      0.18        60\n",
      "           2       0.67      0.42      0.52       896\n",
      "           3       0.09      0.52      0.16        61\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      1911\n",
      "   macro avg       0.40      0.52      0.38      1911\n",
      "weighted avg       0.67      0.50      0.56      1911\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>120</td>\n",
       "      <td>158</td>\n",
       "      <td>104</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161</td>\n",
       "      <td>149</td>\n",
       "      <td>380</td>\n",
       "      <td>206</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>688</td>\n",
       "      <td>311</td>\n",
       "      <td>565</td>\n",
       "      <td>347</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3   All\n",
       "True                               \n",
       "0          512  120  158  104   894\n",
       "1            8   34   13    5    60\n",
       "2          161  149  380  206   896\n",
       "3            7    8   14   32    61\n",
       "All        688  311  565  347  1911"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model2.predict_classes(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test_num, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Model 2\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test_num, y_pred))\n",
    "report = metrics.classification_report(y_test_num, y_pred)\n",
    "model_test_precision_comparisons[\"Model 2\"] = report.split('\\n')[-2].split('      ')[1]\n",
    "model_test_recall_comparisons[\"Model 2\"] = report.split('\\n')[-2].split('      ')[2]\n",
    "model_test_f1_comparisons[\"Model 2\"] = report.split('\\n')[-2].split('      ')[3]\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test_num), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are getting 44.7% Accuracy by handling the class imbalance problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Persist A Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model2_part1.mod\"\n",
    "model2.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "<a id=\"model3_architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "In Model 3, we have added another convolutional layer to [Model 1 Architecture](#model1_architecture) <br/> <br/>\n",
    "1) <b>First convolutional layer</b> consists consists of <b>32 filters</b> each of <b>size 3 x 3</b>. We are using <b>Rectified linear unit (ReLU)</b> activation function for <b>all the layers</b> except the final output layer. Followed by max pooling layer of 2x2 window. <br/>\n",
    "2) <b>Second convolutional</b> layer consists consists of <b>32 filters</b> each of<b> size 3 x 3</b>. Followed by another max pooling layer of 2x2 window. <br/>\n",
    "3) <b>Third convolutional</b> layer consists consists of <b>32 filters</b> each of<b> size 3 x 3</b>. Followed by another max pooling layer of 2x2 window. <br/>\n",
    "4) <b>Fourth convolutional layer </b>consists consists of <b>64 filters </b>each of size 3 x 3. Followed by another max pooling layer of 2x2 window. <br/>\n",
    "5) Next layer is the <b>dense layer</b> (fully connected layer) that has <b>256 neurons</b>. This is followed by <b>dropout layer</b> with a dropout rate of 0.5.<br/>\n",
    "6) The final output layer is another <b>dense layer</b> which has number of neurons equal to the number of classes. Since it is a multi-class classification problem, the activation function is set to <b>softmax</b>\n",
    "\n",
    "<b> <font color=\"red\">In Model 3 we are handling class imbalance problem </font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom split to train and test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X_data, y_data, random_state=0, test_size = 0.30, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4456, 1, 84, 84)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1911, 1, 84, 84)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of clases\n",
    "num_classes = len(set(y_data))\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_train_num = y_train_encoder.fit_transform(y_train)\n",
    "y_train_wide = keras.utils.to_categorical(y_train_num, num_classes)\n",
    "\n",
    "y_test_num = y_train_encoder.fit_transform(y_test)\n",
    "y_test_wide = keras.utils.to_categorical(y_test_num, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a5a6cc898>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Apply under sampling to balance the training dataset</b>\n",
    "\n",
    "Undersampling in data analysis is a technique used to adjust the class distribution of a data set that is the ratio between the different classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 7056)\n",
      "(576,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD6BJREFUeJzt3XusZWV9xvHvAwPeCdA52OkMdNBMbKnVQk8IlUSNaArWAjHaQKtOlGbalCq2tYo1EXsx0Wi13koyFWRoCJSCFmppK6Eo0Qp6QOQ2KhNsYQSZYxEVbdSxv/6x1zjH8YXZM+esvc7l+0l29lrvevdav6xM5jnvuqaqkCRpTwcMXYAkaXEyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqWjV0AfOxevXqWr9+/dBlSNKScvPNN3+jqqb21m9JB8T69euZmZkZugxJWlKS/Pc4/TzEJElqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJalrSd1KP41f/9OKhS1g0bn7Xq+b1+3v/4pcXqJKl76i33j7vdZz4gRMXoJLl4TOv/cy81/Gp5z5vASpZHp53w6cWZD2OICRJTb0FRJILk+xIckdj2RuSVJLV3XySvD/JtiS3JTmur7okSePpcwRxEXDyno1JjgReBNw7p/kUYEP32QSc32NdkqQx9BYQVXUD8FBj0XuBNwI1p+004OIauRE4NMmavmqTJO3dRM9BJDkV+FpVfXGPRWuB++bMb+/aWuvYlGQmyczs7GxPlUqSJhYQSZ4IvAV4a2txo60abVTV5qqarqrpqam9vu9CkrSfJnmZ69OBo4EvJgFYB9yS5HhGI4Yj5/RdB9w/wdokSXuY2Aiiqm6vqiOqan1VrWcUCsdV1deBq4FXdVcznQB8q6oemFRtkqSf1udlrpcCnwWekWR7krMeo/s1wD3ANuDvgD/oqy5J0nh6O8RUVWfuZfn6OdMFnN1XLZKkfeed1JKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUlNvAZHkwiQ7ktwxp+1dSb6U5LYkH0ty6Jxlb06yLcmXk/x6X3VJksbT5wjiIuDkPdquBZ5ZVc8CvgK8GSDJMcAZwC91v/nbJAf2WJskaS96C4iqugF4aI+2T1TVzm72RmBdN30acFlVfb+qvgpsA47vqzZJ0t4NeQ7iNcC/dtNrgfvmLNvetf2UJJuSzCSZmZ2d7blESVq5BgmIJG8BdgKX7GpqdKvWb6tqc1VNV9X01NRUXyVK0oq3atIbTLIReAlwUlXtCoHtwJFzuq0D7p90bZKk3SY6gkhyMvAm4NSq+t6cRVcDZyR5XJKjgQ3A5yZZmyTpJ/U2gkhyKfB8YHWS7cB5jK5aehxwbRKAG6vq96vqziSXA3cxOvR0dlX9qK/aJEl711tAVNWZjeYLHqP/24G391WPJGnfeCe1JKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpKbeAiLJhUl2JLljTtvhSa5Ncnf3fVjXniTvT7ItyW1JjuurLknSePocQVwEnLxH27nAdVW1Abiumwc4BdjQfTYB5/dYlyRpDL0FRFXdADy0R/NpwJZuegtw+pz2i2vkRuDQJGv6qk2StHeTPgfx1Kp6AKD7PqJrXwvcN6ff9q5NkjSQxXKSOo22anZMNiWZSTIzOzvbc1mStHJNOiAe3HXoqPve0bVvB46c028dcH9rBVW1uaqmq2p6amqq12IlaSWbdEBcDWzspjcCV81pf1V3NdMJwLd2HYqSJA1jVV8rTnIp8HxgdZLtwHnAO4DLk5wF3Au8vOt+DfBiYBvwPeDVfdUlSRpPbwFRVWc+yqKTGn0LOLuvWiRJ+26xnKSWJC0yBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpaayASHLdOG2SpOXjMd9JneTxwBOB1UkOA9ItOgT4uZ5rkyQN6DEDAvg94PWMwuBmdgfEt4EP7e9Gk/wR8LtAAbcDrwbWAJcBhwO3AK+sqh/s7zYkSfPzmIeYqup9VXU08IaqelpVHd19nl1VH9yfDSZZC7wOmK6qZwIHAmcA7wTeW1UbgG8CZ+3P+iVJC2NvIwgAquoDSZ4DrJ/7m6q6eB7bfUKSHzI6hPUA8ALgt7vlW4C3Aefv5/olSfM0VkAk+Xvg6cCtwI+65gL2OSCq6mtJ3g3cC/wv8AlGh68erqqdXbftwNp9XbckaeGMFRDANHBMVdV8N9id7D4NOBp4GPhH4JRG1+a2kmwCNgEcddRR8y1HkvQoxr0P4g7gZxdomy8EvlpVs1X1Q+CjwHOAQ5PsCqx1wP2tH1fV5qqarqrpqampBSpJkrSncUcQq4G7knwO+P6uxqo6dT+2eS9wQpInMjrEdBIwA1wPvIzRlUwbgav2Y92SpAUybkC8baE2WFU3JbmC0aWsO4EvAJuBfwEuS/JXXdsFC7VNSdK+G/cqpk8t5Ear6jzgvD2a7wGOX8jtSJL237hXMX2H3SeNDwYOAr5bVYf0VZgkaVjjjiCeMnc+yen4174kLWv79TTXqvonRje2SZKWqXEPMb10zuwBjO6LmPc9EZKkxWvcq5h+c870TuC/GN3sJklapsY9B/HqvguRJC0u474waF2SjyXZkeTBJFcmWdd3cZKk4Yx7kvojwNWM3guxFvjnrk2StEyNGxBTVfWRqtrZfS4CfBCSJC1j4wbEN5K8IsmB3ecVwP/0WZgkaVjjBsRrgN8Cvs7o5T4vY/SaUEnSMjXuZa5/CWysqm8CJDkceDej4JAkLUPjjiCetSscAKrqIeDYfkqSJC0G4wbEAd2b4IAfjyDGHX1Ikpagcf+T/2vgP7v3OBSj8xFv760qSdLgxr2T+uIkM4we0BfgpVV1V6+VSZIGNfZhoi4QDAVJWiH263HfkqTlz4CQJDUZEJKkpkECIsmhSa5I8qUkW5P8WpLDk1yb5O7u+7C9r0mS1JehRhDvA/6tqn4BeDawFTgXuK6qNgDXdfOSpIFMPCCSHAI8F7gAoKp+UFUPM3pD3Zau2xbg9EnXJknabYgRxNOAWeAjSb6Q5MNJngQ8taoeAOi+jxigNklSZ4iAWAUcB5xfVccC32UfDicl2ZRkJsnM7OxsXzVK0oo3REBsB7ZX1U3d/BWMAuPBJGsAuu8drR9X1eaqmq6q6akp31kkSX2ZeEBU1deB+5I8o2s6idEd2lcDG7u2jcBVk65NkrTbUE9kfS1wSZKDgXsYvXzoAODyJGcB9wIvH6g2SRIDBURV3QpMNxadNOlaJElt3kktSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1DRYQSQ5M8oUkH+/mj05yU5K7k/xDkoOHqk2SNOwI4hxg65z5dwLvraoNwDeBswapSpIEDBQQSdYBvwF8uJsP8ALgiq7LFuD0IWqTJI0MNYL4G+CNwP918z8DPFxVO7v57cDa1g+TbEoyk2Rmdna2/0olaYWaeEAkeQmwo6puntvc6Fqt31fV5qqarqrpqampXmqUJMGqAbZ5InBqkhcDjwcOYTSiODTJqm4UsQ64f4DaJEmdiY8gqurNVbWuqtYDZwD/UVW/A1wPvKzrthG4atK1SZJ2W0z3QbwJ+OMk2xidk7hg4HokaUUb4hDTj1XVJ4FPdtP3AMcPWY8kabfFNIKQJC0iBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDVNPCCSHJnk+iRbk9yZ5Jyu/fAk1ya5u/s+bNK1SZJ2G2IEsRP4k6r6ReAE4OwkxwDnAtdV1Qbgum5ekjSQiQdEVT1QVbd0098BtgJrgdOALV23LcDpk65NkrTboOcgkqwHjgVuAp5aVQ/AKESAI4arTJI0WEAkeTJwJfD6qvr2PvxuU5KZJDOzs7P9FShJK9wgAZHkIEbhcElVfbRrfjDJmm75GmBH67dVtbmqpqtqempqajIFS9IKNMRVTAEuALZW1XvmLLoa2NhNbwSumnRtkqTdVg2wzROBVwK3J7m1a/sz4B3A5UnOAu4FXj5AbZKkzsQDoqo+DeRRFp80yVokSY/OO6klSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqSmRRcQSU5O8uUk25KcO3Q9krRSLaqASHIg8CHgFOAY4MwkxwxblSStTIsqIIDjgW1VdU9V/QC4DDht4JokaUVabAGxFrhvzvz2rk2SNGGrhi5gD2m01U90SDYBm7rZR5J8ufeq5m818I2hi8i7Nw5dwkIZfn+e1/qnuiQNvy+BvM79uaCy1/358+OsZrEFxHbgyDnz64D753aoqs3A5kkWNV9JZqpqeug6lgv358JxXy6s5bY/F9shps8DG5IcneRg4Azg6oFrkqQVaVGNIKpqZ5I/BP4dOBC4sKruHLgsSVqRFlVAAFTVNcA1Q9exwJbUIbElwP25cNyXC2tZ7c9U1d57SZJWnMV2DkKStEgYED3z0SELJ8mFSXYkuWPoWpa6JEcmuT7J1iR3Jjln6JqWsiSPT/K5JF/s9uefD13TQvAQU4+6R4d8BXgRo0t4Pw+cWVV3DVrYEpXkucAjwMVV9cyh61nKkqwB1lTVLUmeAtwMnO6/zf2TJMCTquqRJAcBnwbOqaobBy5tXhxB9MtHhyygqroBeGjoOpaDqnqgqm7ppr8DbMWnFuy3Gnmkmz2o+yz5v74NiH756BAteknWA8cCNw1bydKW5MAktwI7gGurasnvTwOiX3t9dIg0pCRPBq4EXl9V3x66nqWsqn5UVb/C6AkQxydZ8odBDYh+7fXRIdJQumPlVwKXVNVHh65nuaiqh4FPAicPXMq8GRD98tEhWpS6k6oXAFur6j1D17PUJZlKcmg3/QTghcCXhq1q/gyIHlXVTmDXo0O2Apf76JD9l+RS4LPAM5JsT3LW0DUtYScCrwRekOTW7vPioYtawtYA1ye5jdEfhtdW1ccHrmnevMxVktTkCEKS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkpv8Hsg7iYRJ9ieYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_train_rus, y_train_rus, idx_resampled =rus.fit_sample(X_train.reshape(len(X_train), ROWS*COLS*CHANNELS), y_train)\n",
    "\n",
    "print(X_train_rus.shape)\n",
    "print(y_train_rus.shape)\n",
    "\n",
    "X_train_rus, y_train_rus = shuffle(X_train_rus, y_train_rus)\n",
    "\n",
    "X_train_rus = X_train_rus.reshape(len(X_train_rus), CHANNELS,ROWS, COLS)\n",
    "#print(X_train_rus.shape) #(2144, 1, 64, 64)\n",
    "\n",
    "# Plot a bar plot of the labels\n",
    "#seaborn.countplot - Show value counts for a single categorical variable:\n",
    "sns.countplot(y_train_rus) #class distribution is adjusted\n",
    "\n",
    "# convert to binary encoded labels\n",
    "y_train_rus_wide = keras.utils.to_categorical(y_train_rus, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 1, 84, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=4\n",
    "row_images = 2\n",
    "col_images = 4\n",
    "plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n",
    "\n",
    "print(X_train_rus.shape)\n",
    "\n",
    "for i in range(row_images * col_images):\n",
    "    i_rand = random.randint(0, X_train_rus.shape[0])\n",
    "    plt.subplot(row_images,col_images,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sp.misc.toimage(X_train_rus[i_rand][0]),cmap='gray')\n",
    "    plt.title((\"Action Number \",y_train_rus[i_rand]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Modelling </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is compiled with binary cross entropy loss function and the Adam optimizer is used. The ‘accuracy’ metric is used to evaluate the model. Adam is an optimization algorithm that updates the network weights in an iterative manner. <b>For Adam Optimizer, we have taken the default value for learning rate (lr=0.001).</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 82, 82)        320       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32, 82, 82)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 32, 41, 41)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 39, 39)        9248      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 32, 39, 39)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 19, 19)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 17, 17)        9248      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 17, 17)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 32, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 6, 6)          18496     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 64, 3, 3)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               147712    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 186,052\n",
      "Trainable params: 186,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Conv2D(32, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Conv2D(32, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(256))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(num_classes))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. We allow the model to overfit but save the best set of weights based on validation loss as we go. Then at the end of training (assuming the model has overfit) we revert back to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 460 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5620 - acc: 0.7500 - val_loss: 0.5613 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.5572 - acc: 0.7500 - val_loss: 0.5555 - val_acc: 0.7500\n",
      "Epoch 3/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.5488 - acc: 0.7500 - val_loss: 0.5437 - val_acc: 0.7500\n",
      "Epoch 4/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5287 - acc: 0.7511 - val_loss: 0.5167 - val_acc: 0.7629\n",
      "Epoch 5/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5061 - acc: 0.7663 - val_loss: 0.5142 - val_acc: 0.7672\n",
      "Epoch 6/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4982 - acc: 0.7647 - val_loss: 0.5276 - val_acc: 0.7543\n",
      "Epoch 7/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4891 - acc: 0.7658 - val_loss: 0.5152 - val_acc: 0.7608\n",
      "Epoch 8/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4826 - acc: 0.7696 - val_loss: 0.5083 - val_acc: 0.7608\n",
      "Epoch 9/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4711 - acc: 0.7788 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 10/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.4587 - acc: 0.7832 - val_loss: 0.5261 - val_acc: 0.7435\n",
      "Epoch 11/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4477 - acc: 0.7886 - val_loss: 0.5140 - val_acc: 0.7608\n",
      "Epoch 12/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4458 - acc: 0.7853 - val_loss: 0.5041 - val_acc: 0.7694\n",
      "Epoch 13/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4296 - acc: 0.7967 - val_loss: 0.5086 - val_acc: 0.7651\n",
      "Epoch 14/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4378 - acc: 0.7924 - val_loss: 0.5053 - val_acc: 0.7716\n",
      "Epoch 15/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4164 - acc: 0.8071 - val_loss: 0.5000 - val_acc: 0.7543\n",
      "Epoch 16/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4129 - acc: 0.8098 - val_loss: 0.4962 - val_acc: 0.7522\n",
      "Epoch 17/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4011 - acc: 0.8092 - val_loss: 0.4915 - val_acc: 0.7716\n",
      "Epoch 18/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3876 - acc: 0.8255 - val_loss: 0.5095 - val_acc: 0.7651\n",
      "Epoch 19/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3880 - acc: 0.8293 - val_loss: 0.5192 - val_acc: 0.7586\n",
      "Epoch 20/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3668 - acc: 0.8359 - val_loss: 0.5104 - val_acc: 0.7694\n",
      "Epoch 21/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3718 - acc: 0.8359 - val_loss: 0.5332 - val_acc: 0.7629\n",
      "Epoch 22/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3778 - acc: 0.8397 - val_loss: 0.4990 - val_acc: 0.7651\n",
      "Epoch 23/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.3721 - acc: 0.8255 - val_loss: 0.5025 - val_acc: 0.7478\n",
      "Epoch 24/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3616 - acc: 0.8348 - val_loss: 0.5224 - val_acc: 0.7586\n",
      "Epoch 25/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3452 - acc: 0.8571 - val_loss: 0.5288 - val_acc: 0.7543\n",
      "Epoch 26/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3383 - acc: 0.8478 - val_loss: 0.5457 - val_acc: 0.7543\n",
      "Epoch 27/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3379 - acc: 0.8576 - val_loss: 0.5168 - val_acc: 0.7672\n",
      "Epoch 28/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3255 - acc: 0.8582 - val_loss: 0.5365 - val_acc: 0.7435\n",
      "Epoch 29/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3255 - acc: 0.8641 - val_loss: 0.5342 - val_acc: 0.7629\n",
      "Epoch 30/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3099 - acc: 0.8696 - val_loss: 0.5747 - val_acc: 0.7414\n",
      "Epoch 31/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3108 - acc: 0.8571 - val_loss: 0.5590 - val_acc: 0.7522\n",
      "Epoch 32/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3030 - acc: 0.8679 - val_loss: 0.5569 - val_acc: 0.7565\n",
      "Epoch 33/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2862 - acc: 0.8804 - val_loss: 0.5706 - val_acc: 0.7565\n",
      "Epoch 34/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2904 - acc: 0.8755 - val_loss: 0.5582 - val_acc: 0.7629\n",
      "Epoch 35/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2847 - acc: 0.8745 - val_loss: 0.5654 - val_acc: 0.7478\n",
      "Epoch 36/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2656 - acc: 0.8957 - val_loss: 0.5887 - val_acc: 0.7629\n",
      "Epoch 37/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2753 - acc: 0.8755 - val_loss: 0.6169 - val_acc: 0.7349\n",
      "Epoch 38/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2687 - acc: 0.8793 - val_loss: 0.5925 - val_acc: 0.7543\n",
      "Epoch 39/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2617 - acc: 0.8897 - val_loss: 0.6047 - val_acc: 0.7586\n",
      "Epoch 40/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2352 - acc: 0.9038 - val_loss: 0.6325 - val_acc: 0.7457\n",
      "Epoch 41/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2303 - acc: 0.9098 - val_loss: 0.6384 - val_acc: 0.7543\n",
      "Epoch 42/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2388 - acc: 0.9005 - val_loss: 0.6569 - val_acc: 0.7435\n",
      "Epoch 43/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2186 - acc: 0.9179 - val_loss: 0.6503 - val_acc: 0.7435\n",
      "Epoch 44/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2079 - acc: 0.9179 - val_loss: 0.6445 - val_acc: 0.7629\n",
      "Epoch 45/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2064 - acc: 0.9168 - val_loss: 0.6686 - val_acc: 0.7457\n",
      "Epoch 46/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1949 - acc: 0.9272 - val_loss: 0.6617 - val_acc: 0.7586\n",
      "Epoch 47/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2007 - acc: 0.9207 - val_loss: 0.7012 - val_acc: 0.7500\n",
      "Epoch 48/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1804 - acc: 0.9277 - val_loss: 0.7203 - val_acc: 0.7500\n",
      "Epoch 49/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1884 - acc: 0.9234 - val_loss: 0.7457 - val_acc: 0.7543\n",
      "Epoch 50/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1888 - acc: 0.9234 - val_loss: 0.7677 - val_acc: 0.7349\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "# Set up the callback to save the best model based on validaion data\n",
    "best_weights_filepath = './best_weights_notebook_model3.hdf5'\n",
    "mcp = ModelCheckpoint(best_weights_filepath, monitor=\"val_acc\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model3.fit(X_train_rus, y_train_rus_wide,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.2,\n",
    "          shuffle=True,\n",
    "          callbacks=[mcp])\n",
    "\n",
    "end = time.time()\n",
    "timetaken=end - start\n",
    "model_train_time_comparisons['Model 3'] = timetaken\n",
    "\n",
    "#reload best weights\n",
    "model3.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We have loaded the best model saved by ModelCheckpoint and use the predict function to predict the classes of the images in the array X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5170068027210885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.54      0.62       894\n",
      "           1       0.10      0.50      0.17        60\n",
      "           2       0.70      0.50      0.58       896\n",
      "           3       0.08      0.41      0.13        61\n",
      "\n",
      "   micro avg       0.52      0.52      0.52      1911\n",
      "   macro avg       0.40      0.49      0.38      1911\n",
      "weighted avg       0.68      0.52      0.57      1911\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>158</td>\n",
       "      <td>167</td>\n",
       "      <td>84</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159</td>\n",
       "      <td>84</td>\n",
       "      <td>448</td>\n",
       "      <td>205</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>662</td>\n",
       "      <td>288</td>\n",
       "      <td>641</td>\n",
       "      <td>320</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3   All\n",
       "True                               \n",
       "0          485  158  167   84   894\n",
       "1            9   30   15    6    60\n",
       "2          159   84  448  205   896\n",
       "3            9   16   11   25    61\n",
       "All        662  288  641  320  1911"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model3.predict_classes(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test_num, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Model 3\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test_num, y_pred))\n",
    "report = metrics.classification_report(y_test_num, y_pred)\n",
    "model_test_precision_comparisons[\"Model 3\"] = report.split('\\n')[-2].split('      ')[1]\n",
    "model_test_recall_comparisons[\"Model 3\"] = report.split('\\n')[-2].split('      ')[2]\n",
    "model_test_f1_comparisons[\"Model 3\"] = report.split('\\n')[-2].split('      ')[3]\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test_num), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding another convolutional layer, we were able to increase the increase to 52.17%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Persist A Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model3_part1.mod\"\n",
    "model3.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "In Model 4, we have introduced padding to [Model3 Architecture](#model3_architecture) <br/>\n",
    "\n",
    "1) <b>First convolutional layer</b> consists consists of <b>32 filters</b> each of <b>size 3 x 3.</b> We have set padding to same. It is followed by max pooling layer of 2x2 window. <br/>\n",
    "2) <b>Second convolutional</b> layer consists consists of <b>32 filters</b> each of<b> size 3 x 3.</b> We have set padding to same. It is followed by another max pooling layer of 2x2 window. <br/>\n",
    "3) <b>Third convolutional</b> layer consists consists of <b>32 filters</b> each of<b> size 3 x 3.</b> We have set padding to same. It is followed by another max pooling layer of 2x2 window. <br/>\n",
    "4) <b>Fourth convolutional layer </b>consists consists of <b>64 filters </b>each of size 3 x 3. We have set padding to same. It is followed by another max pooling layer of 2x2 window. <br/>\n",
    "5) Next layer is the <b>dense layer</b> (fully connected layer) that has <b>256 neurons</b>. This is followed by <b>dropout layer</b> with a dropout rate of 0.5.<br/>\n",
    "6) The final output layer is another <b>dense layer</b> which has number of neurons equal to the number of classes. Since it is a multi-class classification problem, the activation function is set to <b>softmax</b>\n",
    "\n",
    "In convolution networks, we can either decrease the image size as we go from one network layer to another, or keep the size of the image same. Without padding the image size will get reduced for the next layer. Whereas with the introduction of adequate padding the size of the image remains same. In this way, we won't loss information on the corner of the image. \n",
    "\n",
    "<b> <font color=\"red\">In Model 4 we are handling class imbalance problem </font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom split to train, validation, test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X_data, y_data, random_state=0, test_size = 0.30, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of clases\n",
    "num_classes = len(set(y_data))\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_train_num = y_train_encoder.fit_transform(y_train)\n",
    "y_train_wide = keras.utils.to_categorical(y_train_num, num_classes)\n",
    "\n",
    "y_test_num = y_train_encoder.fit_transform(y_test)\n",
    "y_test_wide = keras.utils.to_categorical(y_test_num, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Apply under sampling to balance the training dataset</b>\n",
    "\n",
    "Undersampling in data analysis is a technique used to adjust the class distribution of a data set that is the ratio between the different classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_train_rus, y_train_rus, idx_resampled =rus.fit_sample(X_train.reshape(len(X_train), ROWS*COLS*CHANNELS), y_train)\n",
    "\n",
    "X_train_rus, y_train_rus = shuffle(X_train_rus, y_train_rus)\n",
    "X_train_rus = X_train_rus.reshape(len(X_train_rus), CHANNELS,ROWS, COLS)\n",
    "\n",
    "# Plot a bar plot of the labels\n",
    "#seaborn.countplot - Show value counts for a single categorical variable:\n",
    "sns.countplot(y_train_rus) #class distribution is adjusted\n",
    "\n",
    "# convert to binary encoded labels\n",
    "y_train_rus_wide = keras.utils.to_categorical(y_train_rus, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 1, 84, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=4\n",
    "row_images = 2\n",
    "col_images = 4\n",
    "plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n",
    "\n",
    "print(X_train_rus.shape)\n",
    "\n",
    "for i in range(row_images * col_images):\n",
    "    i_rand = random.randint(0, X_train_rus.shape[0])\n",
    "    plt.subplot(row_images,col_images,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sp.misc.toimage(X_train_rus[i_rand][0]),cmap='gray')\n",
    "    plt.title((\"Action Number \",y_train_rus[i_rand]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Modelling </b>\n",
    "\n",
    "The model is compiled with binary cross entropy loss function and the Adam optimizer is used. The ‘accuracy’ metric is used to evaluate the model. Adam is an optimization algorithm that updates the network weights in an iterative manner. <b>For Adam Optimizer, we have taken the default value for learning rate (lr=0.001).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 32, 84, 84)        320       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 32, 84, 84)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 32, 42, 42)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 32, 42, 42)        9248      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32, 42, 42)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 32, 21, 21)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 21, 21)        9248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 32, 21, 21)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 32, 10, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 10, 10)        18496     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 64, 10, 10)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 64, 5, 5)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               409856    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 448,196\n",
      "Trainable params: 448,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Introduced Padding\n",
    "model4 = Sequential()\n",
    "model4.add(Conv2D(32, (3, 3),padding='same', input_shape=input_shape))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model4.add(Conv2D(32, (3, 3),padding='same'))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model4.add(Conv2D(32, (3, 3),padding='same'))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model4.add(Conv2D(64, (3, 3),padding='same'))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(256))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "model4.add(Dense(num_classes))\n",
    "model4.add(Activation('softmax'))\n",
    "\n",
    "model4.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. We allow the model to overfit but save the best set of weights based on validation loss as we go. Then at the end of training (assuming the model has overfit) we revert back to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 460 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.5644 - acc: 0.7500 - val_loss: 0.5616 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.5589 - acc: 0.7500 - val_loss: 0.5583 - val_acc: 0.7500\n",
      "Epoch 3/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.5536 - acc: 0.7500 - val_loss: 0.5501 - val_acc: 0.7500\n",
      "Epoch 4/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.5421 - acc: 0.7500 - val_loss: 0.5296 - val_acc: 0.7500\n",
      "Epoch 5/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.5312 - acc: 0.7467 - val_loss: 0.5147 - val_acc: 0.7522\n",
      "Epoch 6/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5102 - acc: 0.7511 - val_loss: 0.5108 - val_acc: 0.7500\n",
      "Epoch 7/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4998 - acc: 0.7543 - val_loss: 0.5143 - val_acc: 0.7478\n",
      "Epoch 8/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4925 - acc: 0.7538 - val_loss: 0.5021 - val_acc: 0.7500\n",
      "Epoch 9/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4804 - acc: 0.7592 - val_loss: 0.5054 - val_acc: 0.7522\n",
      "Epoch 10/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.4736 - acc: 0.7679 - val_loss: 0.5064 - val_acc: 0.7672\n",
      "Epoch 11/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4543 - acc: 0.7810 - val_loss: 0.4992 - val_acc: 0.7629\n",
      "Epoch 12/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4504 - acc: 0.7826 - val_loss: 0.4961 - val_acc: 0.7672\n",
      "Epoch 13/50\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.4384 - acc: 0.7935 - val_loss: 0.5030 - val_acc: 0.7586\n",
      "Epoch 14/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4418 - acc: 0.7810 - val_loss: 0.4922 - val_acc: 0.7694\n",
      "Epoch 15/50\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.4354 - acc: 0.7924 - val_loss: 0.5012 - val_acc: 0.7586\n",
      "Epoch 16/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.4176 - acc: 0.8076 - val_loss: 0.5163 - val_acc: 0.7543\n",
      "Epoch 17/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4171 - acc: 0.8027 - val_loss: 0.5052 - val_acc: 0.7694\n",
      "Epoch 18/50\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.4157 - acc: 0.8038 - val_loss: 0.5172 - val_acc: 0.7608\n",
      "Epoch 19/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3832 - acc: 0.8272 - val_loss: 0.5443 - val_acc: 0.7414\n",
      "Epoch 20/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3907 - acc: 0.8201 - val_loss: 0.5269 - val_acc: 0.7478\n",
      "Epoch 21/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3780 - acc: 0.8332 - val_loss: 0.5390 - val_acc: 0.7543\n",
      "Epoch 22/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3622 - acc: 0.8342 - val_loss: 0.5609 - val_acc: 0.7392\n",
      "Epoch 23/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3584 - acc: 0.8348 - val_loss: 0.5425 - val_acc: 0.7586\n",
      "Epoch 24/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3561 - acc: 0.8440 - val_loss: 0.5526 - val_acc: 0.7522\n",
      "Epoch 25/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3310 - acc: 0.8549 - val_loss: 0.6065 - val_acc: 0.7392\n",
      "Epoch 26/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3286 - acc: 0.8522 - val_loss: 0.5927 - val_acc: 0.7349\n",
      "Epoch 27/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3114 - acc: 0.8587 - val_loss: 0.6394 - val_acc: 0.7478\n",
      "Epoch 28/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3152 - acc: 0.8533 - val_loss: 0.6087 - val_acc: 0.7414\n",
      "Epoch 29/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3029 - acc: 0.8679 - val_loss: 0.6148 - val_acc: 0.7371\n",
      "Epoch 30/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.2997 - acc: 0.8712 - val_loss: 0.6542 - val_acc: 0.7349\n",
      "Epoch 31/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.2730 - acc: 0.8853 - val_loss: 0.6978 - val_acc: 0.7543\n",
      "Epoch 32/50\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.2610 - acc: 0.8973 - val_loss: 0.7243 - val_acc: 0.7177\n",
      "Epoch 33/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.2761 - acc: 0.8696 - val_loss: 0.6794 - val_acc: 0.7328\n",
      "Epoch 34/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.2690 - acc: 0.8826 - val_loss: 0.7092 - val_acc: 0.7263\n",
      "Epoch 35/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.2555 - acc: 0.8913 - val_loss: 0.7268 - val_acc: 0.7328\n",
      "Epoch 36/50\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.2325 - acc: 0.9011 - val_loss: 0.7955 - val_acc: 0.7263\n",
      "Epoch 37/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.2353 - acc: 0.8946 - val_loss: 0.8081 - val_acc: 0.7263\n",
      "Epoch 38/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2296 - acc: 0.9060 - val_loss: 0.7726 - val_acc: 0.7241\n",
      "Epoch 39/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1995 - acc: 0.9239 - val_loss: 0.8062 - val_acc: 0.7328\n",
      "Epoch 40/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.2064 - acc: 0.9109 - val_loss: 0.8890 - val_acc: 0.7198\n",
      "Epoch 41/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.2005 - acc: 0.9179 - val_loss: 0.8659 - val_acc: 0.7177\n",
      "Epoch 42/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1700 - acc: 0.9299 - val_loss: 0.8621 - val_acc: 0.7306\n",
      "Epoch 43/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.1717 - acc: 0.9255 - val_loss: 0.9138 - val_acc: 0.7134\n",
      "Epoch 44/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1659 - acc: 0.9342 - val_loss: 0.9481 - val_acc: 0.7112\n",
      "Epoch 45/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1719 - acc: 0.9375 - val_loss: 0.9851 - val_acc: 0.7004\n",
      "Epoch 46/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1669 - acc: 0.9293 - val_loss: 0.9310 - val_acc: 0.7263\n",
      "Epoch 47/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1474 - acc: 0.9364 - val_loss: 1.0250 - val_acc: 0.7220\n",
      "Epoch 48/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1457 - acc: 0.9457 - val_loss: 1.0603 - val_acc: 0.7284\n",
      "Epoch 49/50\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.1501 - acc: 0.9380 - val_loss: 1.0496 - val_acc: 0.7069\n",
      "Epoch 50/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1303 - acc: 0.9473 - val_loss: 1.0667 - val_acc: 0.7069\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "# Set up the callback to save the best model based on validaion data\n",
    "best_weights_filepath = './best_weights_notebook_model4.hdf5'\n",
    "mcp = ModelCheckpoint(best_weights_filepath, monitor=\"val_acc\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "start = time.time()\n",
    "history = model4.fit(X_train_rus, y_train_rus_wide,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.2,\n",
    "          shuffle=True,\n",
    "          callbacks=[mcp])\n",
    "end = time.time()\n",
    "timetaken=end - start\n",
    "model_train_time_comparisons['Model 4'] = timetaken\n",
    "\n",
    "#reload best weights\n",
    "model4.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4VEXbwOHfJCQECCFUgQQIIKAkBAgh9CZFioIoVRFBBUGRLqKC8iL6ofIq4gvYAAsdVESq0qRJR0I3SEvozYQSEkLm+2OWEGB3s8FsdpM893XttW3mnGdS9tkzc86M0lojhBBCAHi4OgAhhBDuQ5KCEEKIFJIUhBBCpJCkIIQQIoUkBSGEECkkKQghhEghSUEIIUQKSQpCCCFSSFIQQgiRIperA0ivIkWK6KCgIFeHIYQQWcr27dvPa62LplUuyyWFoKAgtm3b5uowhBAiS1FKHXOknHQfCSGESCFJQQghRApJCkIIIVJkuTEFa27cuEFMTAzXr193dSginXx8fAgMDMTLy8vVoQghyCZJISYmhvz58xMUFIRSytXhCAdprblw4QIxMTGULVvW1eEIIcgm3UfXr1+ncOHCkhCyGKUUhQsXliM8IdxItkgKgCSELEp+b0K4l2yTFIQQIruKS4hj+IrhHLl0xOn7kqSQAS5cuEC1atWoVq0axYsXJyAgIOV5YmKiQ9vo2bMnBw8etFtm4sSJzJgxIyNCpn79+vz5558Zsi0hRPrcTL7J+Wvn0yyXrJOZtnMaFT+ryAcbPmD538udHlu2GGh2tcKFC6d8wI4aNQpfX1+GDh16RxmtNVprPDys5+Fp06aluZ9XXnnl3wcrhHCpZJ3M0z8+zby982gU1IhuVbrRoXIHCvgUuKPcH9F/0H9Zf7ad3EadwDr80vUXagbUdHp8cqTgRIcOHSIkJIQ+ffoQFhbGqVOn6N27N+Hh4QQHBzN69OiUsre+uSclJeHv78/w4cOpWrUqderU4ezZswCMGDGC8ePHp5QfPnw4ERERVKpUiY0bNwJw9epVnnrqKapWrUrXrl0JDw93+IggPj6e5557jipVqhAWFsbatWsB2L17NzVr1qRatWqEhoZy+PBhLl++TKtWrahatSohISHMnz8/I390QmRb/1nzH+bunUuHyh04EXeCF395kQfGPUCneZ345eAvHLl0hG4/dqPu1LqcvHyS6e2ns+H5DZmSECAbHikMHAgZ3StSrRpYPovTbd++fUybNo3PP/8cgLFjx1KoUCGSkpJo0qQJHTp0oHLlynfUiY2NpVGjRowdO5bBgwczdepUhg8ffs+2tdZs2bKFhQsXMnr0aJYtW8Znn31G8eLF+eGHH9i1axdhYWEOxzphwgS8vb3ZvXs3e/fupXXr1kRFRTFp0iSGDh1K586dSUhIQGvNzz//TFBQEEuXLk2JWQhh36zdsxi9djQ9q/VkStspAGw9uZXpkdOZtWcW8/bNAyC3Z25GNBjB6/Vfx9fbN1NjlCMFJytfvjw1a97O8LNmzSIsLIywsDD279/Pvn377qmTJ08eWrVqBUCNGjU4evSo1W0/+eST95RZv349Xbp0AaBq1aoEBwc7HOv69et59tlnAQgODqZkyZIcOnSIunXrMmbMGD788EOio6Px8fEhNDSUZcuWMXz4cDZs2ECBAgXS2LoQOduWE1vo+XNP6peuz+Q2k1FKoZQiIiCCCa0mcHLwSRZ1XcQ7jd5h/yv7efeRdzM9IUA2PFK432/0zpIvX76Ux1FRUXz66ads2bIFf39/unXrZvUcfW9v75THnp6eJCUlWd127ty57ymjtb7vWG3VffbZZ6lTpw6LFy+mefPmfPvttzRs2JBt27axZMkSXnvtNR577DHefPPN+963ENlZTFwM7Wa3o0T+EvzY6Udy58p9TxkvTy/aVGxDm4ptXBDhbXKkkIni4uLInz8/fn5+nDp1iuXLM/5Mgvr16zN37lzAjAVYOxKxpWHDhilnN+3fv59Tp07x4IMPcvjwYR588EEGDBhAmzZtiIyM5MSJE/j6+vLss88yePBgduzYkeFtESI7uJp4lbaz2nI18Sq/dP2FovnSXNLApbLdkYI7CwsLo3LlyoSEhFCuXDnq1auX4ft49dVX6d69O6GhoYSFhRESEmKza+fRRx9NmXOoQYMGTJ06lZdeeokqVarg5eXFd999h7e3NzNnzmTWrFl4eXlRsmRJxowZw8aNGxk+fDgeHh54e3unjJkIIW5L1sl0X9CdXWd28UvXXwgpFuLqkNKk/k13gyuEh4fruxfZ2b9/Pw8//LCLInIvSUlJJCUl4ePjQ1RUFC1atCAqKopcudw3/8vvT2RX7619jxGrR/DfFv9lcJ3BLo1FKbVdax2eVjn3/aQQ9+XKlSs0bdqUpKQktNZ88cUXbp0QhMiubibfZMKWCbSu0JpBtQe5OhyHyadFNuPv78/27dtdHYYQOd7mE5s5e/Us3ap0y1JzfDltoFkpNVUpdVYptcfG+0opNUEpdUgpFamUcvyEeiGEcHMLDizAy8OL1hVauzqUdHHm2UffAC3tvN8KqGC59QYmOzEWIYTINFprfjrwE03KNrln+gp357SkoLVeC1y0U6Qd8J02NgH+SqkSzopHCCEyy/7z+zl08RBPVHrC1aGkmyuvUwgAolM9j7G8dg+lVG+l1Dal1LZz585lSnBCCHG/FhxYAEDbSm1dHEn6uTIpWBt5sXp+rNb6S611uNY6vGhR97vwo3HjxvdciDZ+/Hhefvllu/V8fc0l7CdPnqRDhw42t333Kbh3Gz9+PNeuXUt53rp1a/755x9HQrdr1KhRjBs37l9vR4icZsGBBUQERBDgZ/V7rltzZVKIAUqleh4InHRRLP9K165dmT179h2vzZ49m65duzpUv2TJkv9qltG7k8KSJUvw9/e/7+0JIe6ltWbfuX0k62S75WLiYth6cmuW7DoC1yaFhUB3y1lItYFYrfUpF8Zz3zp06MCiRYtISEgA4OjRo5w8eZL69eunXDcQFhZGlSpV+Pnnn++pf/ToUUJCzJWO8fHxdOnShdDQUDp37kx8fHxKub59+6ZMu/3OO+8AZmbTkydP0qRJE5o0aQJAUFAQ58+bBTw+/vhjQkJCCAkJSZl2++jRozz88MP06tWL4OBgWrRoccd+0mJtm1evXqVNmzYpU2nPmTMHgOHDh1O5cmVCQ0PvWWNCiKzky+1fEjwpmLHrx9ot9/MB8z/e/uH2mRFWhnPadQpKqVlAY6CIUioGeAfwAtBafw4sAVoDh4BrQM+M2O/AZQP583TGzp1drXg1xre0PdNe4cKFiYiIYNmyZbRr147Zs2fTuXNnlFL4+Pjw008/4efnx/nz56lduzZt27a1ed7y5MmTyZs3L5GRkURGRt4x9fV7771HoUKFuHnzJk2bNiUyMpL+/fvz8ccfs3r1aooUKXLHtrZv3860adPYvHkzWmtq1apFo0aNKFiwIFFRUcyaNYuvvvqKTp068cMPP9CtW7c0fxa2tnn48GFKlizJ4sWLATOV9sWLF/npp584cOAASqkM6dISwhX+iP6DV5e+ipeHF2PXj6VXWC+bcxgtOLiASoUr8VCRhzI5yozhzLOPumqtS2itvbTWgVrrKVrrzy0JActZR69orctrratore13nLu51F1IqbuOtNa8+eabhIaG0qxZM06cOMGZM2dsbmft2rUpH86hoaGEhoamvDd37lzCwsKoXr06e/fuTXOyu/Xr19O+fXvy5cuHr68vTz75JOvWrQOgbNmyVKtWDbA/Pbej26xSpQorVqzg9ddfZ926dRQoUAA/Pz98fHx48cUX+fHHH8mbN69D+xDCnZy6fIqn5j5FqQKlWNNjDVdvXGXM2jFWy16Kv8Sao2t44qGs2XUE2fCKZnvf6J3piSeeSJktND4+PuUb/owZMzh37hzbt2/Hy8uLoKAgq9Nlp2btKOLIkSOMGzeOrVu3UrBgQXr06JHmduzNa3Vr2m0wU2872n1ka5sVK1Zk+/btLFmyhDfeeIMWLVrw9ttvs2XLFlauXMns2bP53//+x6pVqxzajxDuIPFmIh3ndSQ2IZbl3ZZT5YEqvFD9BSZvm8yA2gMoV7DcHeWXRC0hKTkpSycFmTo7g/j6+tK4cWOef/75OwaYY2NjKVasGF5eXqxevZpjx47Z3U7q6av37NlDZGQkYKbdzpcvHwUKFODMmTMpK54B5M+fn8uXL1vd1oIFC7h27RpXr17lp59+okGDBv+qnba2efLkSfLmzUu3bt0YOnQoO3bs4MqVK8TGxtK6dWvGjx/v8LKgQriLQcsGsSF6A1PbTqXKA1UAGNV4FLk8cvHWqrfuKb/g4AKK+xYnIiAis0PNMNnuSMGVunbtypNPPnnHmUjPPPMMjz/+OOHh4VSrVo2HHrLfz9i3b1969uxJaGgo1apVIyLC/HFVrVqV6tWrExwcfM+0271796ZVq1aUKFGC1atXp7weFhZGjx49Urbx4osvUr16dYe7igDGjBmTMpgMEBMTY3Wby5cv57XXXsPDwwMvLy8mT57M5cuXadeuHdevX0drzSeffOLwfoVwtWk7pzFp2ySG1hlK55DOKa+XzF+SwXUG89669xhSZwjhJc3Eo9eTrrM0aindQrvhobLu922ZOlu4nPz+hLvZemIrDaY1oH7p+izrtoxcHnd+f45LiKP8hPJUKVaFld1XopRi8V+LeWzWYyx9ZiktH7Q3w49rODp1dtZNZ0II4QRnr57lyblPUty3OLM7zL4nIQD45fbj7YZvs/roapYdWgaYC9bye+enSVCTzA45Q0lSEEIIi/gb8bSb3Y7z187zY+cfKZK3iM2yL4W/RPmC5Xl9xevcuHmDhX8tpE3FNlbXX85Ksk1SyGrdYMKQ35twF8k6mR4/92BTzCamt59OWAn7s/l7e3rz3iPvsfvsbl5Z8gpnr57Nslcxp5YtkoKPjw8XLlyQD5gsRmvNhQsX8PHxcXUoQjBy1Ujm7p3Lh80+5KnKTzlUp2NwR8JLhvPVjq/w8vCiVYVWTo7S+bLF2UeBgYHExMQgM6hmPT4+PgQGBro6DJHDTds5jffXv0+vsF4Mrev4dCweyoMPm33II989QtNyTfHL7efEKDNHtkgKXl5elC1b1tVhCCGyoFVHVtF7UW+al2vOxNYT0710ZpOyTfjk0U+oW6qukyLMXNkiKQghxP3Yf24/T855koqFKzKv4zy8PL3uazsDaw/M4MhcJ1uMKQghRHqdvXqWNjPN2UKLn16c5ZbNdBY5UhBC5EgvLXqJU1dOsea5NQT5B7k6HLchRwpCiBxn9ZHVLDiwgJENR1IrsJarw3ErkhSEEDnKzeSbDFo+iDIFyjCo9iBXh+N2pPtICJGjTPtzGrvO7GJOhznk8crj6nDcjhwpCCFyjLiEON5a9Rb1StWjY+WOrg7HLUlSEEJkGVEXogj8OJAP1n9wXzMY/N+6/+Ps1bN88ugn6b4eIaeQpCCEyDKm7pzKicsnGL5yOB3mdSAuIc7hukcuHeHjTR/TvWp3agbUdGKUWZskBSFElpCsk5m5ZyatHmzFuObj+PnAz9T6uhb7z+13qP6wFcPI5ZGL9x9538mRZm2SFIQQWcKG4xs4HnucZ6o8w5C6Q1jRfQUXrl0g4usI5u+bb7fuumPrmL9vPq/Xe50Av4BMijhrkqQghMgSZuyeQV6vvLR7qB0AjYMas+OlHQQXDabjvI4M+20YVxKv3FMvWSczcPlAAv0C0zXZXU4lp6QKIdxe4s1E5u2bxxMPPYGvt2/K64F+gfze43cGLhvIRxs/4qONH1E0b1HKFixLWf+ylCtYjiuJV9hxagfT208nr1deF7Yia5CkIIRwe8sPLedi/EWeqfLMPe/lzpWbyY9N5qnKT7Ht5DYOXzrMkX+OsPXkVn7Y/wNJyUnUCaxD1ypdXRB51iNJQQjh9mbsnkGRvEVoXq65zTLNyjWjWblmd7yWlJxETFwMRfMWxUNJb7kj5KckhHBrlxMus/DgQjpV7pTuqa1zeeQiyD+IfN75nBRd9iNJQQjh1n468BPxSfE8E3pv15HIeJIUhBBubcbuGZT1L0udwDquDiVHkKQghHBbp6+cZsXhFTxd5WmZliKTSFIQQritOXvmkKyTrZ51JJxDkoIQwm3N3DOT6sWr83DRh10dSo4hSUEI4ZaiLkSx5cQWOUrIZJIUhBBuaebumSgUXUK6uDqUHMWpSUEp1VIpdVApdUgpNdzK+6WVUquVUjuVUpFKqdbOjEcIkTVorZmxewaNgxrLBHaZzGlJQSnlCUwEWgGVga5Kqcp3FRsBzNVaVwe6AJOcFY8QIvPF34hnyo4pRHwVwcBlAx1eGGfrya1EXYySriMXcOY0FxHAIa31YQCl1GygHbAvVRkN+FkeFwBOOjEeIUQmOR57nMlbJ/Plji+5GH+RkvlL8unmTwnyD2Jg7YF2616Kv0SPBT3w9/HnqcpPZVLE4hZnJoUAIDrV8xig1l1lRgG/KqVeBfIBzRBCZFmbYjYxbuM4FhxYgEbT/qH2vBrxKg3KNKDD3A4M+XUIDxV5iJYPtrRaPyEpgfZz2vP3pb/5tduv+Pv4Z3ILhDPHFKxdaXL3sWNX4ButdSDQGvheqXtnrVJK9VZKbVNKbTt37pwTQhVC/FuHLx2mwbQGrD66mqF1h3K4/2Hmd5pPo6BGeCgPvmv/HVWKVaHz/M4cOH/gnvrJOpkeP/fg92O/8027b2gU1MgFrRDOTAoxQKlUzwO5t3voBWAugNb6D8AHKHL3hrTWX2qtw7XW4UWLFnVSuEKIf2PilokARPaJZGyzsZTxL3PH+77evizsuhCfXD48PutxLsZfvOP9N1e+yew9sxnbdKxMc+1CzkwKW4EKSqmySilvzEDywrvKHAeaAiilHsYkBTkUECKLuZJ4hSk7p9Cxcke7ZwuVLlCaBZ0XcDz2OB3ndeTGzRsATN46mQ82fEDf8L4Mqzcss8IWVjgtKWitk4B+wHJgP+Yso71KqdFKqbaWYkOAXkqpXcAsoId29PQEIYTb+H7X98QmxNK/Vv80y9YpVYevHv+KVUdWMWDZAH45+Av9lvbjsYqPMaHVBJnjyMVUVvsMDg8P19u2bXN1GEIIC601wZOCyeedjy0vbnH4Q/31317nw40fkssjF9WKV2PNc2tk3QMnUkpt11qHp1VOVl4TQvwrK4+sZP/5/Xz3xHfp+pb/ftP3+fvS3+w5u4dFXRdJQnATkhSEEP/KhM0TKJavGJ2CO6WrnqeHJ/M6ziNZJ+Pp4emk6ER6ydxHQoj7dvjSYRb9tYiXarxE7ly5011fKSUJwc1IUhBC3LeJWybi6eFJn/A+rg5FZBBJCkKI+3LrNNQOlTtQMn9JV4cjMogkBSHEfZkeOd2chhqR9mmoIuuQpCCESDetNRM2TyC8ZDi1A2u7OhyRgeTsIyFEut06DfXbJ76Vi82yGTlSEEKk22dbPqNYvmJ0Du7s6lBEBpOkIEQOcfrKaa4nXf/X2/nl4C/8cvAXeof1vq/TUIV7k6QgRA6w6K9FlPu0HI/Petzh1c/uduTSEdrNbkfb2W2pVKQSr9Z6NYOjFO5AkoIQ2dzUnVN5YvYT+Pv4s+LwCqbsnJKu+vE34vnPmv9QeVJlVh5eyQfNPmBXn10Uy1fMSRELV5KkIEQ2pbVmzNoxvLDwBZqWa8qBfgdoHNSYIb8O4UTcCYe28cvBXwieFMyo30fRrlI7DvQ7wLB6w/D29HZy9MJVJCkIkQ3dTL5JvyX9GLl6JN1Cu/FL11/wy+3H149/zY2bN+izuI/dbiStNQOWDqDt7Lb45PJhZfeVzO4wm0C/wExshXAFSQpCZDPXk67TaX4nJm2bxGt1X+PbJ75N+WZfvlB53m/6Pov+WsTM3TOt1tdaM2j5ICZsmcCAWgPY1WcXj5R9JDObIFxIkoIQ2cj1pOu0nN6SH/f/yMctPubD5h/icdey569GvEqdwDr0X9afM1fO3PGe1pohvw7h082fMrDWQD559BO8PL0yswnCxSQpCJGNvLP6HX4/9jvft/+eQXUGWS3j6eHJ1HZTuZp4lX5L+6W8rrVm2G/D+GTTJ/SP6M/Hj34sF6blQJIUhMgm/oj+g3F/jKNXWC+6hXazW/ahIg/xTqN3mL9vPj/s+wGtNcNXDGfcH+N4peYrjG85XhJCDiXLcQqRDVy7cY3qX1QnISmB3X13kz93/jTr3Lh5g9pTahMTF0PXkK58uvlT+ob3ZWLriZIQsiFHl+OUIwUhsoERq0bw14W/mNpuqkMJAcDL04upbadyMf4in27+lN5hvflf6/9JQsjhZEI8IbK4dcfWMX7TeF6p+Uq6zxKqWrwqXz72Jcdij/F2o7fvGZQWOY90HwmRhV1NvEro56EA7OqzC19vXxdHJNyVo91HcqQgRBb2+orXOXLpCL/3+F0SgsgQcqwoRBa18vBKJm6dyIBaA2hQpoGrwxHZhCQFIbKguIQ4nl/4PBULV+S9pu+5OhyRjUj3kRBZjNaaPov6EBMXw/qe68nrldfVIYlsRJKCEFnM1zu+ZtaeWYxpMoY6peq4OhyRzUj3kRCZKOpCFJcTLt93/cgzkfRf1p/m5ZrzRoM3MjAyIQyHkoJSqrxSKrflcWOlVH+llL9zQxMie9l/bj8hk0OoP60+F+Mvprv+lcQrdJrXiYI+BZn+5HS5pkA4haN/VT8AN5VSDwJTgLKA9Xl3hRD3SNbJvLToJfLkysOB8wdoOb0lcQlxDtfXWtN3cV+iLkYx86mZsuqZcBpHk0Ky1joJaA+M11oPAko4LywhspcpO6aw7vg6Pn70Y+Z3nM/O0ztpM7MNVxOvOlR/2p/TmB45nXcavUPjoMbODVbkaI4mhRtKqa7Ac8Aiy2syyboQDjh95TTDVgyjcVBjelbryeOVHmfmkzPZGL2RdrPbcT3put36e87uod+SfjQt25S3GryVSVGLnMrRpNATqAO8p7U+opQqC0x3XlhCZB+Dlg/i2o1rfN7m85TJ5joGd2Rau2msPLKSDnM7kHgz0WrdW+MIfrn9mPHkDDw9PDMzdJEDOXRKqtZ6H9AfQClVEMivtR6bVj2lVEvgU8AT+NpaHaVUJ2AUoIFdWuunHY5eCDe3NGops/fMZnTj0VQqUumO97pX7c61G9fou7gvT//wNDOfmsmhi4fYemIrW0+a267Tu0i8mciK7it4wPcBF7VC5CQOTYinlFoDtMUkkT+Bc8DvWuvBdup4An8BzYEYYCvQ1ZJgbpWpAMwFHtFaX1JKFdNan7UXi0yIJ9xFQlICvx/7ncZBjVPWQE7tauJVgicFk9crL3/2+dNqGYBP/viEwb8OxtvTO+WIwdfblxolalCzZE3aVGwj4wjiX8voCfEKaK3jlFIvAtO01u8opSLTqBMBHNJaH7YENBtoB+xLVaYXMFFrfQkgrYQghDsZsWoE4/4YR6BfIINrD6ZXjV53TEr3zpp3OBZ7jHU919lMCACD6gyiYJ6CbD+5nfCS4dQMqEmlwpWkq0i4hKNJIZdSqgTQCXB0pCsAiE71PAaodVeZigBKqQ2YLqZRWutlDm5fCJc5+s9RJmyZQIvyLUi8mcjgXwfz7tp36RfRj1cjXiUmLobxm8bTO6w39UvXT3N7Par1oEe1Hs4PXIg0OJoURgPLgQ1a661KqXJAVBp1rC3fdHdfVS6gAtAYCATWKaVCtNb/3LEhpXoDvQFKly7tYMhCOM/I1SPxUB58/fjXlCpQik0xm/hgwwe8u/Zdxm0cR+G8hSmStwhjm6U59CaEW3Ho7COt9TytdajWuq/l+WGt9VNpVIsBSqV6HgictFLmZ631Da31EeAgJkncvf8vtdbhWuvwokWLOhKyEE6z89ROpkdOZ2CtgZQqYP7EawfW5qfOP7Hv5X10CenChWsXmNRmEgXzFHRxtEKkj6MDzYHAZ0A9zLf99cAArXWMnTq5MAPNTYETmIHmp7XWe1OVaYkZfH5OKVUE2AlU01pfsLVdGWgWrtbi+xbsOLWDv/v/TQGfAlbLaK1lrWPhVhwdaHb0OoVpwEKgJGas4BfLazZZroDuh+l22g/M1VrvVUqNVkq1tRRbDlxQSu0DVgOv2UsIQrjar3//ym+Hf2NEwxE2EwIgCUFkWY4eKfypta6W1muZQY4UhKsk62TCvggjLiGO/a/sJ3eu3K4OSQiHZfSRwnmlVDellKfl1g2Qb/QiR5kROYNdZ3bxftP3JSGIbMvRpPA85nTU08ApoANm6ossY/Nm+PBD+O03OHfO1dGIrOZ60nXeWvUW4SXD6RTcydXhCOE0jk5zcRxzRXMKpdRAYLwzgnKGD1Z8yU9nP4D9pWByKXyTAylTsBSVA0pRq1IQrWtWpmIFTzzleqFs7eTlk7yx8g3qlarHi2EvOrwmwWebPyM6Lppvn/hW1jEQ2ZpDYwpWKyp1XGud6RcN3O+YwtKopUzZ9j1/nY4m5nI0/ySfQKuk2wWuFcLzWDNKJT5K7aKPUic4gNBQqFMHcktPQbaw/NBynv3pWc5fO49GU7dUXb547AtCioXYrXcx/iLlJ5Snbqm6LH56cSZFK0TGcnRM4d8khWitdam0S2asjBpoTtbJnLlyhkPnolmz+y+WR63gz8u/ctXjlClwNhgOtaTAwX70bB/Eiy9CcPC/3q1wgaTkJN5e/Tb/t/7/CCkWwpwOc9h6YitDfh1CbEIsw+oOY0TDEeTxynNHveOxx5m7dy7f7vqWfef2savPrjQTiBDuKjOSQpY6UnCE1prdZ3ez7NByFu1fzh8n10GSN3rJBG5u70Ht2opevaBTJ/D1TXt7IuPEXo/lSuIVAvwC0lUvOjaarj90ZUP0BnqF9eLTlp+mfPifv3aeob8O5dtd31KuYDk+b/M5IcVCmL9vPrP3zmZj9EYAapasyYBaA3gm9JkMb5cQmSVDkoJS6jL3Tk0BZgqLPFprR6fJyDCZeUrq8djjPLfgOdYcXUOVXO2Jn/cFh3YVxdcXOneG556D+vVBTkl3rsOXDtP8++ZEx0YzuM5gRjQcccfEc7Ys/msx3Rd0J/FmIl8+9iVdq3TVmxrXAAAddUlEQVS1Wm7VkVX0WdSHqItRKBQaTegDoXQJ7kKn4E6UL1Q+o5skRKZz+pGCq2T2dQrJOplP/viEN1e9SUGfggytMIW9C9owbx5cvQrlykH37uZWtmymhZVjRJ6J5NHpj5J4M5EW5Vswe89sSuYvyUfNP6JrSNd7LhJLSErgh/0/MHnbZNYfX0+14tWY02EOFQtXtLuf60nX+d+W/3E18SqdgjvxcNGHndksITKdo0kBrXWWutWoUUO7QuTpSB06OVQzCv3SLy/pUxcu62+/1fqRR7RWSmvQulEjrefM0To52SUhZjvrj63X/mP9dcB/A/Tes3u11lr/Ef2HrvFFDc0odMNpDfWu07u01lofuXRED/9tuC76YVHNKHT5T8vrcRvG6fgb8a5sghBuA9imHfiMlSOFdEhISmDk6pGM2ziO4r7FeaP+G/Sq0YuzJ334/nv49luIioKOHeGLL6CgzIV235ZELaHD3A4E+gXy27O/Uca/TMp7N5NvMmXnFN5c+SaXrl+iVkAtNsVsQinF4xUf5+WaL9OsXDM5dVSIVKT7yIk2Rm/kjZVvsPbYWgL9AnmrwVs8X/15PPHmo49g5EgoXhy+/x4aN3ZpqFnSzN0zeW7Bc1QpVoVl3ZZRLF8xq+Uuxl9k5KqRrDq6io6VO9IrrFfKrKVCiDtJUnAyrTWrj65m5OqRbIzeSOkCpRnZcCTPVX2OXTu9ePppOHQIhg2D0aPB2/bCW9nW9aTrbD2xlRola5DXK2+a5S8nXOazLZ8xYtUIGpZpyMKuC/HL7ZcJkQqR/UlSyCRaa347/BsjV49ky4kt+OX248FCD1LKtyyHd5Rj9+/lqFCkHB+8U4RcBU9wPPY4x2KPpdyfu3qOVg+24uWaL2ebwc3Em4lM2zmNMevGEBMXQ37v/HQO7kzP6j2pE1jnnsHhXad38fm2z5m+ezpXEq/Q/qH2zHhyxj3XDQgh7p8khUymtWbpoaUs/msxR/45Ym6XjpBwM+Gest6e3pQuUJoyBcqQ1ysvy/9eTuLNRJoENeGVmq/QtlJbvDy9bO4nPime60nXib9hubc8z+eVj0pFKrmsLz0pOYnvd33P6LWjOfrPUeoE1qFPeB9WHVnFvH3zuHbjGhULV6RH1R50Cu7EhugNTN42mU0xm/DJ5UOXkC70qdGHiIAImXpaiAwmScENJOtkTl0+xZaoI0ydc46NywO4eLg0+T2K0eEpD555xow5XLh+lqk7pzJ522SOxx6nZP6S9A7rzQO+DxAdG010nOUWG01MXIzVRHNLoTyFaFC6AQ3LNKRhmYZUK16NXB6OXU6SeDORHad2sDlmM0H+QbSq0MrugvO3JCUnMXfvXEatGUXUxShqlKjBu03epeWDLVM+3C8nXGbevnl88+c3rDu+LqVupcKV6BPeh+eqPierlAnhRJIU3NDNm7BqFcycCT/8AJcvmwHp5s1NcmjQ8Cb7khYzedsklv+9HABP5UmAXwCl/EoR6BdIKb9SFMlbhDxeefDJ5YNPLh/y5DKPL8RfYN2xdaw9vpZDFw8B4OvtS0RABEEFggjwCyAgf0DKfeG8hdl9Zjcbojew/vh6tp7cyvWk6ynxFs5TmC4hXehetTs1S9a849v75YTL/Pr3r/x88GcWRy3mYvxFqhSrwugmo2lXqZ3db/qHLh5i4cGFhJUIo1GZRnJUIEQmkKTg5uLjYfFimDcPVq++PZ13YKBJEFUbnKB5MwgJKo6nR/qnbj15+aRJEMfWsuXkFmLiYjhz5QzaygXquTxyUb14deqXrk+9UvWoFViLyDORfLfrO34++DPXk65TsXBFuod2p1CeQiz8ayGrjqwi8WYihfIUok2FNjz18FM8XulxOQ1UCDclSSEL0Rr274c1a8zt99/h7Fnw8jLXPPTtC/Xq/fvpNG7cvMHpK6c5cfkEJ+JOcPbqWR4q8hARARHk885ntU7s9Vjm75vPd5HfsfbYWgDKFyxPu0rtaPdQO+qWqutw95QQwnUkKWRhWsPu3TBlCnzzDcTFQZUqJjl06wb587smruOxx4m/EU/FwhWly0eILEaSQjZx9SrMmgWTJsHOnWZ21nbtzER89eqZ6bw9pMdGCJEGSQrZjNawZQtMngzLlsGZM+Z1f3+zEFC9etCyJdSo4do4hRDuSZJCNqY1HD4M69fDhg3mtm+fea9PH/jgA/CTC4GFEKk4mhSk4yELUgrKlzfrOXz5JezdCxcuwJAh5nlIiDmaEEKI9JKkkE0UKgTjxpmjBl9faNUKevSAixddHZkQIiuRpJDN1K5tBqRHjIDp081A9Jw5Zgwii/UUCiFcQE4wz4Zy54Z334Unn4SePaFLF/N63rxmdbiyZc2KceXLQ9u2EBTk0nCFEG5EBpqzuRs3YMUKM433kSNmgPrW/ZUrZnyidWszQN2qFXim/+JpIUQW4OhAsxwpZHNeXubD/m5aw9GjMHUqfP01PP44lCkDvXvD88/DAw+Y8Yjjx++85c4NtWqZWzHra98IIbIwOVIQ3LgBCxaYayBWr4ZcucyH/9Wrd5bLnRuSkszEfmC6oWrXNgnikUfMVddCCPck1ymI+3LggJlaIyEBSpc2tzJlzH3RomYivx07YNMm2LzZ3MfEmLo9ephrJOQIQgj3I0lBZJoTJ+B//4P//hfy5YP/+z/o1UvGJ4RwJ3Lxmsg0AQEmEezaBdWrm4n76tSB7dtdHZkQIr1koFlkmIcfhpUrzQR+Q4ZAzZrmlNhKlcw4hZeXub/1OG9ec6Fd/vzm/tatePF/P024EOL+ODUpKKVaAp8CnsDXWuuxNsp1AOYBNbXW0jeUhSkFTz8NbdrAO+/AxIlmcDo9WreGn34C77RXAhVCZDCnjSkopTyBv4DmQAywFeiqtd53V7n8wGLAG+iXVlKQMYWs5caN27ekJHO79fzaNXOtxJUrZmnSK1fMYkPvvw/PPAPffSfTgguRUdzhOoUI4JDW+rAloNlAO2DfXeXeBT4EhjoxFuEiXl7mlh6+vvDmm+Yspv/+V7qShMhMzvweFgBEp3oeY3kthVKqOlBKa73IiXGILGb4cHj1VfjkEzPJnxAi8zjzSMHa97uUviqllAfwCdAjzQ0p1RvoDVC6dOkMCk+4K6Vg/Hgzid+wYebq6u7dXR2VEDmDM48UYoBSqZ4HAidTPc8PhABrlFJHgdrAQqXUPX1eWusvtdbhWuvwokWLOjFk4S48PMyYQtOmZtqNJUvufD8mxrzfsyc8+qiZETYhwTWxCpGdOHOgORdmoLkpcAIz0Py01nqvjfJrgKEy0CxSi4uDxo3h4EEYOxb27IFVq8wEf2DWkfD3NxP8PfCAmdjvpZegRAmXhi2E23H5xWta6ySgH7Ac2A/M1VrvVUqNVkq1ddZ+Rfbi5wdLl5oP+f79YfZscz3Exx/Dn3/CuXMQFWVWmqtRA/7zHzMtR7duZk1rIUT6yDQXIks4exaio6FqVXPxmy1RUWbKjWnTzGmurVqZI4zQ0MyLVQh35PIjBSEyUrFi5kjAXkIAqFABPv3UjDmMHQt//AHVqpmB6mPHMidWIbIySQoiW/Lzg9dfN2MNr70Gc+dCxYpm+o0LF1wdnRDuS7qPRI4QHQ2jRplpwX19zeD1raVJby1PWrasmeVViOxIps4Wwoq9e2/P6HrkyL0LCRUoAEWKmFvhwrcf16wJnTvL1dUi65KkIEQatIbz501yuLVu9enT5rXUt3PnzOJCffrAZ5+lPa4hhDtyh7mPhHBrSpnV5IoWhYgI2+WSk+GNN+DDD03imDvXHFEIkR3JQLMQafDwMMuMfv21uXCuXj04etTVUQnhHJIUhHDQCy/A8uVm+dFatcz61EJkN5IUhEiHRx4x1z7kz2/OYPrmGzh+3Iw5CJEdyJiCEOn00EPmKOGJJ8yEfLf4+prxiWLFzLQcL78MzZu7Lk4h7ockBSHuQ5EiZj3qlSvh1CkzDce5c+b+7FnYuhVatIB27cw8TeXKuTpiIRwjSUGI+5Q7t1lP2pqEBLNI0JgxZgK/IUPManK+vpkboxDpJdcpCOFEJ0+aleS+/x5KljRnMVWvbs5eOnLkzvsSJcy0361bg6eniwMX2Y5cvCaEG/njDzP1991/unnyQFCQme47MtIkkaAg6NvXLC5UpIgrohXZkSQFIdxMcjIsXGjOVCpb1nz4P/DA7akzbtyAn382U3///rvpnurSxawNUa4cBASY14S4H5IUhMjC9uyBSZNMt9OVK7dfL1rUJIfAQDPr68iRZuU5IdIiSUGIbCAuzpz+euKEWSMi9f2ePWYQe9kyM14hhD0y95EQ2YCfnzm11ZoVK6B9e6hbF3791Rw5CPFvyRXNQmRRzZrB6tVw7ZqZj2nrVldHJLIDSQpCZGHh4bBhg7n+oUkT+O03V0cksjpJCkJkcRUqwMaNUL48tGkDs2Y5XjchASZPNlODL14M//zjvDhF1iBJQYhsoEQJWLsW6tSBp582q8StWWMWErLmxg346iuTUF5+GT76CB57DAoVMhfXDRgAP/xgFhkSOYskBSGyiQIFzNTew4aZbqQmTaByZfj0U7h0yZS5edOc5vrQQ9C7tzlracUKuHzZjE+MGmWWIf3qK+jQwSSb556D3btd2jSRieSUVCGyofh4mDfPdA1t2mSunO7QwVxRvX8/VKtm5mVq3dr6utOJibBjh+mK+vprM5jdqhW89pqZMlzWqs56HD0lVY4UhMiG8uSB7t3N9Bo7d5rHP/5o3ps3D7ZvN+MPtj7cvb2hdm1zlBEdbRLI9u1mPYmaNc2SpFns+6RwkBwpCJFD3Lxplha932/516/Dd9/BuHEQFQVPPQXTppkFh4T7kyMFIcQdPD3/XbePj48Zh9i/3wxML1gAERHmucg+JCkIIdLF0xOGDjWD2RcumMQwf76roxIZRZKCEOK+NGliBqNDQqBjR3PWU1KSq6MS/5bMfSSEuG+BgeZ6iMGDTZfS1q3QtKlZojT17fRpKFgQwsLMdRBhYeZWpoycyeRuJCkIIf6V3Llh4kSoVQv69DFJolAhc41D8eLQoIG5P3fOHFksX24GvcEkiieegM8/N2c8CdeTpCCEyBDdu5tuJA8P+4sBxcebab937DCnzE6bZi6umzsXvLwyL15hnYwpCCEyTJ48aa8OlyePudbhpZfgm2/gs8/MmUzPPCNjEu5AjhSEEC7Vr5+Zi2nwYMiVy0zD4enp6qhyLqceKSilWiqlDiqlDimlhlt5f7BSap9SKlIptVIpVcaZ8Qgh3NOgQTB2rJlW4/nnzXrWwjWcdqSglPIEJgLNgRhgq1JqodZ6X6piO4FwrfU1pVRf4EOgs7NiEkK4r9dfN3Muvf22GXT+4gszPpGUBH/+CevXm7Ujdu+GcuWgRo3bt8BAOYspoziz+ygCOKS1PgyglJoNtANSkoLWenWq8puAbk6MRwjh5kaONIlhzBhzKmt8vJnQ79o1836ZMmYyvyNHzBKkt85iKlrULDj01ltmFTpx/5yZFAKA6FTPY4Badsq/ACy19oZSqjfQG6B06dIZFZ8Qwg2NHm26j/77XzP19wsvmA/6evXMEcEt8fGwa5eZ+XX7dnOF9aOPmmRRt67r4s/qnDYhnlKqI/Co1vpFy/NngQit9atWynYD+gGNtNYJ9rYrE+IJkTMkJ5vuI0edOgUNG8LZs7BypTlyELe5w4R4MUCpVM8DgZN3F1JKNQPeAtqmlRCEEDlHehICmIvlVq0yF861aAGRkc6JK7tzZlLYClRQSpVVSnkDXYCFqQsopaoDX2ASwlknxiKEyAFKlTKJIW9eaNZMZnC9H05LClrrJEyX0HJgPzBXa71XKTVaKdXWUuwjwBeYp5T6Uym10MbmhBDCIWXLmsTg4WHmYTp0yNURZS2yyI4QIlvau9csHZonD3z5pTmrKTb2zpuXl5mgr0YNCAqyvTTp7t1mQHvvXrOudXCwmR22TJn0d3O5iqNjCnJFsxAiWwoONmckNWli1pe+m4+PuQbi1tQat2ZxDQszCeJWIoiMNIkBTLfUrdNjAfLlM2dIBQdDjx7QqJGzW+V8cqQghMjWTpwwYwv+/lCgwO1b7tyQkGA+/HfsMKe1bt9unicmgp+fOYMp9S0oyBxh7Ntnjhr27DH3O3fCxYvQty988IF7LlHq6JGCJAUhhEglMRHOnIGAAMe7hq5dgxEjYPx4KF0avv7aDHS7E3c4JVUIIbIcb29zFlN6xgry5oWPPzZTcfj4QPPmZj3r2Nj079/V39NlTEEIITJI3bqmK2nUKBg3DpYuNZP9FSlyu9vKz8/cJydDVBT89Ze5HTxo7mNjzVHHkCGumS1Wuo+EEMIJNm82U3Ts3Zt2WX9/qFQJKlaECxdgyRKoXdusN1GpUsbEI2cfCSGEC9WqZc5cunDBfPuPi7vzXmuoUMEkgsKFb58OqzXMng2vvGIm/3v/fRgwIPNOfZUjBSGEcEOnTplxiUWLzDrXU6fCgw/e//ZkoFkIIbKwEiVg4UL49ltzxFG1KsyZ4/z9SlIQQgg3pRR0727GJZo2NV1NziZjCkII4eYCAsxRQ2aQIwUhhBApJCkIIYRIIUlBCCFECkkKQgghUkhSEEIIkUKSghBCiBSSFIQQQqSQpCCEECJFlpv7SCl1Djh2n9WLAOedXMcd9+GOMWWXfbhjTJmxD3eMKbvs435ickQZrXXRNEtprXPMDdjm7DruuA93jCm77MMdY5J2Z+193E9MGXmT7iMhhBApJCkIIYRIkdOSwpeZUMcd9+GOMWWXfbhjTJmxD3eMKbvs435iyjBZbqBZCCGE8+S0IwUhhBB25JikoJRqqZQ6qJQ6pJQankbZUkqp1Uqp/UqpvUqpAQ7uw1MptVMptcjB8v5KqflKqQOWfdVJo/wgSzx7lFKzlFI+VspMVUqdVUrtSfVaIaXUb0qpKMt9QQfqfGSJK1Ip9ZNSyt9e+VTvDVVKaaVUkbTKK6VetfxO9iqlPnQgpmpKqU1KqT+VUtuUUhGp3rP6O7PVdjvl7bXb7t/F3W23V95W2+3EZbXtSikfpdQWpdQuS/n/WF4vq5TabGn3HKWUdxrlZ1ji2WP52XulislqnVTvf6aUupJWeWW8p5T6y9K+/g7UaaqU2mFp93ql1IN37fuO/zlb7bZT3ma7bdWx1W47+7DZbjt17LbbqVx56lNm3QBP4G+gHOAN7AIq2ylfAgizPM4P/GWvfKp6g4GZwCIH4/oWeNHy2Bvwt1M2ADgC5LE8nwv0sFKuIRAG7En12ofAcMvj4cAHDtRpAeSyPP4gdR1r5S2vlwKWY64jKZLG9psAK4DclufFHIjpV6CV5XFrYE1avzNbbbdT3l67bf5dWGu7nX3YbLudOlbbDijA1/LYC9gM1Lb8fXSxvP450DeN8q0t7ylg1q3y9upYnocD3wNX0ioP9AS+AzystNtWnb+Ahy2vvwx8Y+9/zla77ZS32W57/9fW2m1nHzbbbaeO3XY785ZTjhQigENa68Na60RgNtDOVmGt9Smt9Q7L48vAfsyHsk1KqUCgDfC1IwEppfwwH3xTLPtJ1Fr/k0a1XEAepVQuIC9w0krsa4GLd73cDpOAsNw/kVYdrfWvWusky9NNQGAa+wD4BBgG3DFQZaN8X2Cs1jrBUuasA3U04Gd5XIBU7bfzO7Padlvl02i3vb+Le9pup7zNttupY7Xt2rj1bdXLctPAI8B8K+22Wl5rvcTynga23NVuq3WUUp7AR5Z2k1Z5S7tHa62TrbTbVh2bv/O7/+eUUspWu62Vt+zXZrtt1bHVblvl7bXbTh2b7Xa2nJIUAoDoVM9jSOND/halVBBQHfPNxZ7xmD+SZAdjKgecA6ZZDhu/Vkrls1VYa30CGAccB04BsVrrXx3c1wNa61OW7ZwCijlY75bngaX2Ciil2gIntNa7HNxmRaCB5VD/d6VUTQfqDAQ+UkpFY34Wb9iIJYjbv7M0227nd2yz3anrONL2u/bhUNvvqmOz7Zauhz+Bs8BvmKPif1Iltzv+3u8ur7XenOo9L+BZYNldsVir0w9YeOvn60D58kBnZbq/liqlKjhQ50VgiVIqxhLX2FRV7v6fK2yv3VbKp9631XbbqGOz3TbK2223jTr22u1UOSUpKCuvpXnalVLKF/gBGKi1jrNT7jHgrNZ6ezpiyoXpHpmsta4OXMV0b9jaR0HMt96yQEkgn1KqWzr2d1+UUm8BScAMO2XyAm8Bb6dj07mAgpgugteAuZZvevb0BQZprUsBg7AcZd0Vi0O/s7TK22t36jqWMnbbbmUfabbdSh2bbdda39RaV8N8y40AHrYShrZVXikVkqrcJGCt1nrdHZXvrdMQ6Ah8Zq3NNvaRG7iutQ4HvgKmOlBnENBaax0ITAM+tvx8rP3P2fw/d+B/9J52W6ujlCppq9129mGz3XbqWG13ptCZ1E/lyhtQB1ie6vkbwBtp1PHC9BEPdmD7/4f5VnIUOA1cA6anUac4cDTV8wbAYjvlOwJTUj3vDkyyUTaIO/viDwIlLI9LAAfTqmN57TngDyCvvfJAFcy3u6OWWxLmiKa4nZiWAY1TPf8bKJpGO2K5fRq1AuLS+p3Za7ut33Ea7b6jTlpttxGT3bbbqGO37anKvYNJNOe5PTZyx9+/lfJDUz1egKXv287f4juW2+lU7U7GdNHa3AdwAAhK1YbYNPbxGvB3qtdKA/vs/M/NsNVuG+Wn22u3jTqXbLXb1j7stdtGncW22p0Zt0zZiatvmG9mhzHfsm8NNAfbKa8wA0Pj72NfjXF8oHkdUMnyeBTwkZ2ytYC9mLEEhekvfdVG2SDu/DD9iDsHWz90oE5LYB93fVDbKn/Xe0dJNdBsY/t9MP2sYLpTorF86Nmpsx/LhynQFNie1u/MVtvtlLfZbkf+LlK33c4+bLbdTh2rbQeKYjlBAchj+Zt6DJjHnQOuL6dR/kVgI5YTGe7at9U6d5W5klZ5TBfI86n+T7Y6UOc8UNHy+gvAD/b+52y12055m+125P8aKwPNVvZhs93W6mA+r9Jst7NumbITd7hhzjL4C/Ot7K00ytbHHHZGAn9abq0d3I/VPx4bZasB2yz7WQAUTKP8fzDfOvZgznzIbaXMLMyYww3MN5AXMH2tK4Eoy30hB+ocwnxY3Wr/5/bK37W9o9x59pG17XtjvkXtAXYAjzgQU31gOyapbwZqpPU7s9V2O+XttTvNvwvuTAq29mGz7XbqWG07EArstJTfA7xteb0cZuD0EOaDMnca5ZMw/xu39vl2qpis1rmr3VfSKg/4Y74F78YciVV1oE57S/ldwBqgnL3/OVvttlPeZrsd+b/GsaRgs9126qTZbmfd5IpmIYQQKXLKQLMQQggHSFIQQgiRQpKCEEKIFJIUhBBCpJCkIIQQIoUkBSEslFI3LbNS3rrZnU03ndsOUlZmlRXC3eRydQBCuJF4baZZECLHkiMFIdKglDqqlPpAmfn+t9ya214pVUYptVKZtRdWKqVKW15/QJm1GHZZbnUtm/JUSn2lzHoBvyql8ljK91dK7bNsZ7aLmikEIElBiNTy3NV91DnVe3Fa6wjgf5hZLbE8/k5rHYqZd2eC5fUJwO9a66qYSQ/3Wl6vAEzUWgcD/wBPWV4fDlS3bKePsxonhCPkimYhLJRSV7TWvlZeP4qZiuKwZYrl01rrwkqp85jJ9m5YXj+ltS6ilDoHBGrLegmWbQRhpoOuYHn+OuCltR6jlFoGXMFMdbJA315XQIhMJ0cKQjhG23hsq4w1Cake3+T2mF4bYCJQA9huWURJCJeQpCCEYzqnuv/D8ngj0MXy+BlgveXxSsz6B7cWjrm1gtY9lFIeQCmt9WrMQiv+wD1HK0JkFvlGIsRteSwrf92yTGt967TU3EqpzZgvUl0tr/UHpiqlXsOsotfT8voA4Eul1AuYI4K+mBlfrfEEpiulCmCmzf5Ep70sqxBOI2MKQqTBMqYQrrU+7+pYhHA26T4SQgiRQo4UhBBCpJAjBSGEECkkKQghhEghSUEIIUQKSQpCCCFSSFIQQgiRQpKCEEKIFP8PjEz6iiRvdjQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We have loaded the best model saved by ModelCheckpoint and use the predict function to predict the classes of the images in the array X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5463108320251178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       894\n",
      "           1       0.14      0.50      0.21        60\n",
      "           2       0.69      0.47      0.56       896\n",
      "           3       0.08      0.28      0.12        61\n",
      "\n",
      "   micro avg       0.55      0.55      0.55      1911\n",
      "   macro avg       0.39      0.47      0.39      1911\n",
      "weighted avg       0.64      0.55      0.58      1911\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>575</td>\n",
       "      <td>94</td>\n",
       "      <td>161</td>\n",
       "      <td>64</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255</td>\n",
       "      <td>82</td>\n",
       "      <td>422</td>\n",
       "      <td>137</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>855</td>\n",
       "      <td>220</td>\n",
       "      <td>612</td>\n",
       "      <td>224</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3   All\n",
       "True                               \n",
       "0          575   94  161   64   894\n",
       "1           15   30    9    6    60\n",
       "2          255   82  422  137   896\n",
       "3           10   14   20   17    61\n",
       "All        855  220  612  224  1911"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model4.predict_classes(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test_num, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Model 4\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test_num, y_pred))\n",
    "report = metrics.classification_report(y_test_num, y_pred)\n",
    "model_test_precision_comparisons[\"Model 4\"] = report.split('\\n')[-2].split('      ')[1]\n",
    "model_test_recall_comparisons[\"Model 4\"] = report.split('\\n')[-2].split('      ')[2]\n",
    "model_test_f1_comparisons[\"Model 4\"] = report.split('\\n')[-2].split('      ')[3]\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test_num), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting 46.4% Accuracy when we have introduced padding to Model 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Persist A Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model4_part1.mod\"\n",
    "model4.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Model 5 has four convolutional layers similar to [Model3 Architecture](#model3_architecture). In this Model, we have changed the size of the filter from 3 x 3 to 5 x 5. Also after the third layer, we have introduced a dropout layer to prevent overfitting of the model.\n",
    "\n",
    "1) <b>First convolutional layer</b> consists consists of <b>32 filters</b> each of <b>size 5 x 5</b>. We are using <b>Rectified linear unit (ReLU)</b> activation function for <b>all the layers</b> except the final output layer. Followed by max pooling layer of 2x2 window. <br/>\n",
    "2) <b>Second convolutional</b> layer consists consists of <b>32 filters</b> each of<b> size 5 x 5</b>. Followed by another max pooling layer of 2x2 window<br/>\n",
    "3) <b>Third convolutional</b> layer consists consists of <b>32 filters</b> each of<b> size 5 x 5</b>. Followed by another max pooling layer of 2x2 window. This is followed by <b>dropout layer</b> with a dropout rate of 0.5.<br/>\n",
    "4) <b>Fourth convolutional layer </b>consists consists of <b>64 filters </b>each of size 5 x 5. Followed by another max pooling layer of 2x2 window. <br/>\n",
    "5) Next layer is the <b>dense layer</b> (fully connected layer) that has <b>256 neurons</b>. This is followed by <b>dropout layer</b> with a dropout rate of 0.5.<br/>\n",
    "6) The final output layer is another <b>dense layer</b> which has number of neurons equal to the number of classes. Since it is a multi-class classification problem, the activation function is set to <b>softmax</b>\n",
    "\n",
    "A dropout layer with dropout rate of 0.5 means 50% of the neurons will be turned off randomly. This helps to prevent overfitting by making all the neurons learn something about the data and not rely on just a few neurons. Thus generalizing better and prevent overfitting.\n",
    "\n",
    "<b> <font color=\"red\">In Model 5 we are handling class imbalance problem </font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom split to train, validation, test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X_data, y_data, random_state=0, test_size = 0.30, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of clases\n",
    "num_classes = len(set(y_data))\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_train_num = y_train_encoder.fit_transform(y_train)\n",
    "y_train_wide = keras.utils.to_categorical(y_train_num, num_classes)\n",
    "\n",
    "y_test_num = y_train_encoder.fit_transform(y_test)\n",
    "y_test_wide = keras.utils.to_categorical(y_test_num, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a52f5dbe0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADk9JREFUeJzt3X+s3XV9x/Hny4Jz80co6YV1/bES0yxjv9A1SEaiTDcEkg3mhpFEaRxJ/QOdJssytj+G05iYbLoocyQsVqlRCBkyOtOMNQ1KdENoDUOgOhrm4K4dLXYTmYkL5r0/7uduB7i9PZ/2nvu9p30+kpNzzud+z7nvnjR99vs933tuqgpJksb1sqEHkCRNF8MhSepiOCRJXQyHJKmL4ZAkdTEckqQuhkOS1MVwSJK6GA5JUpczhh5gEtasWVObNm0aegxJmir79u17pqpmjrfdKRmOTZs2sXfv3qHHkKSpkuTfxtnOQ1WSpC6GQ5LUxXBIkroYDklSF8MhSepiOCRJXQyHJKmL4ZAkdTEckqQup+RPjo/jl/9gx9AjrBj7/uzaoUeQNEXc45AkdTEckqQuhkOS1MVwSJK6GA5JUhfDIUnqYjgkSV0MhySpi+GQJHUxHJKkLhMLR5INSe5Nsj/Jo0ne39bPTrI7yePtenVbT5JPJjmQ5OEkrx95rq1t+8eTbJ3UzJKk45vkHsfzwO9X1c8CFwHXJzkfuAHYU1WbgT3tPsDlwOZ22QbcDHOhAW4E3gBcCNw4HxtJ0vKb2IccVtUh4FC7/f0k+4F1wJXAJW2zW4EvA3/Y1ndUVQH3Jzkrydq27e6qOgqQZDdwGXDbpGaXhnTxTRcPPcKK8bX3fW3oEbSAZXmPI8km4HXA14FzW1Tm43JO22wd8NTIw2bb2rHWX/w9tiXZm2TvkSNHlvqPIElqJh6OJK8C7gQ+UFXPLrbpAmu1yPoLF6puqaotVbVlZmbmxIaVJB3XRMOR5EzmovH5qvpiW366HYKiXR9u67PAhpGHrwcOLrIuSRrAJM+qCvBpYH9VfXzkSzuB+TOjtgJ3j6xf286uugj4XjuUdQ9waZLV7U3xS9uaJGkAk/wNgBcD7wK+meShtvbHwEeBO5JcBzwJXN2+tgu4AjgA/AB4N0BVHU3yYeDBtt2H5t8olyQtv0meVfVVFn5/AuAtC2xfwPXHeK7twPalm06SdKL8yXFJUhfDIUnqYjgkSV0MhySpi+GQJHUxHJKkLoZDktTFcEiSuhgOSVIXwyFJ6mI4JEldDIckqYvhkCR1MRySpC6GQ5LUxXBIkroYDklSF8MhSepiOCRJXQyHJKmL4ZAkdTEckqQuhkOS1MVwSJK6GA5JUhfDIUnqYjgkSV0MhySpi+GQJHUxHJKkLoZDktTFcEiSuhgOSVIXwyFJ6mI4JEldDIckqYvhkCR1MRySpC6GQ5LUZWLhSLI9yeEkj4ysfTDJvyd5qF2uGPnaHyU5kOTbSd46sn5ZWzuQ5IZJzStJGs8k9zg+C1y2wPpfVNUF7bILIMn5wDuAn2uP+askq5KsAj4FXA6cD1zTtpUkDeSMST1xVd2XZNOYm18J3F5VPwT+NckB4ML2tQNV9QRAktvbto8t8biSpDEN8R7He5M83A5lrW5r64CnRraZbWvHWn+JJNuS7E2y98iRI5OYW5LE8ofjZuC1wAXAIeBjbT0LbFuLrL90seqWqtpSVVtmZmaWYlZJ0gImdqhqIVX19PztJH8NfKndnQU2jGy6HjjYbh9rXZI0gGXd40iyduTubwHzZ1ztBN6R5MeSnAdsBh4AHgQ2JzkvycuZewN953LOLEl6oYntcSS5DbgEWJNkFrgRuCTJBcwdbvoO8B6Aqno0yR3Mven9PHB9Vf2oPc97gXuAVcD2qnp0UjNLko5vkmdVXbPA8qcX2f4jwEcWWN8F7FrC0SRJJ8GfHJckdTEckqQuhkOS1MVwSJK6GA5JUhfDIUnqYjgkSV0MhySpi+GQJHUxHJKkLoZDktTFcEiSuhgOSVIXwyFJ6mI4JEldDIckqYvhkCR1MRySpC5jhSPJnnHWJEmnvkV/53iSVwA/AaxJshpI+9JrgJ+a8GySpBVo0XAA7wE+wFwk9vH/4XgW+NQE55IkrVCLhqOqPgF8Isn7quqmZZpJkrSCHW+PA4CquinJrwCbRh9TVTsmNJckaYUaKxxJPge8FngI+FFbLsBwSNJpZqxwAFuA86uqJjmMJGnlG/fnOB4BfnKSg0iSpsO4exxrgMeSPAD8cH6xqn5zIlNJklasccPxwUkOIUmaHuOeVfWVSQ8iSZoO455V9X3mzqICeDlwJvDfVfWaSQ0mSVqZxt3jePXo/SRXARdOZCJJ0op2Qp+OW1V/C7x5iWeRJE2BcQ9VvW3k7suY+7kOf6ZDkk5D455V9Rsjt58HvgNcueTTSJJWvHHf43j3pAeRJE2HcX+R0/okdyU5nOTpJHcmWT/p4SRJK8+4b45/BtjJ3O/lWAf8XVuTJJ1mxg3HTFV9pqqeb5fPAjMTnEuStEKNG45nkrwzyap2eSfw3UkOJklamcYNx+8Cbwf+AzgE/A6w6BvmSba390QeGVk7O8nuJI+369VtPUk+meRAkoeTvH7kMVvb9o8n2dr7B5QkLa1xw/FhYGtVzVTVOcyF5IPHecxngctetHYDsKeqNgN72n2Ay4HN7bINuBnmQgPcCLyBuZ9Uv3E+NpKkYYwbjl+sqv+cv1NVR4HXLfaAqroPOPqi5SuBW9vtW4GrRtZ31Jz7gbOSrAXeCuyuqqPt++/mpTGSJC2jccPxstH/6bc9gXF/eHDUuVV1CKBdn9PW1wFPjWw329aOtS5JGsi4//h/DPjHJH/D3EeNvB34yBLOkQXWapH1lz5Bso25w1xs3Lhx6SaTJL3AWHscVbUD+G3gaeAI8Laq+twJfL+n2yEo2vXhtj4LbBjZbj1wcJH1hWa8paq2VNWWmRnPFJakSRn703Gr6rGq+suquqmqHjvB77cTmD8zaitw98j6te3sqouA77VDWfcAlyZZ3Q6VXdrWJEkDOZH3KcaS5DbgEmBNklnmzo76KHBHkuuAJ4Gr2+a7gCuAA8APaKf6VtXRJB8GHmzbfai9MS9JGsjEwlFV1xzjS29ZYNsCrj/G82wHti/haJKkk3BCv8hJknT6MhySpC6GQ5LUxXBIkroYDklSF8MhSepiOCRJXQyHJKmL4ZAkdTEckqQuhkOS1MVwSJK6GA5JUhfDIUnqYjgkSV0MhySpi+GQJHUxHJKkLoZDktTFcEiSuhgOSVIXwyFJ6mI4JEldDIckqYvhkCR1MRySpC6GQ5LUxXBIkroYDklSF8MhSepiOCRJXQyHJKmL4ZAkdTEckqQuhkOS1MVwSJK6GA5JUhfDIUnqYjgkSV0GCUeS7yT5ZpKHkuxta2cn2Z3k8Xa9uq0nySeTHEjycJLXDzGzJGnOkHscv1pVF1TVlnb/BmBPVW0G9rT7AJcDm9tlG3Dzsk8qSfo/K+lQ1ZXAre32rcBVI+s7as79wFlJ1g4xoCRpuHAU8A9J9iXZ1tbOrapDAO36nLa+Dnhq5LGzbU2SNIAzBvq+F1fVwSTnALuTfGuRbbPAWr1ko7kAbQPYuHHj0kwpSXqJQfY4qupguz4M3AVcCDw9fwiqXR9um88CG0Yevh44uMBz3lJVW6pqy8zMzCTHl6TT2rKHI8krk7x6/jZwKfAIsBPY2jbbCtzdbu8Erm1nV10EfG/+kJYkafkNcajqXOCuJPPf/wtV9fdJHgTuSHId8CRwddt+F3AFcAD4AfDu5R9ZkjRv2cNRVU8Av7TA+neBtyywXsD1yzCaJGkMK+l0XEnSFDAckqQuhkOS1MVwSJK6GA5JUhfDIUnqYjgkSV0MhySpi+GQJHUxHJKkLoZDktTFcEiSuhgOSVIXwyFJ6mI4JEldDIckqYvhkCR1MRySpC6GQ5LUxXBIkroYDklSF8MhSepiOCRJXQyHJKmL4ZAkdTEckqQuhkOS1MVwSJK6GA5JUpczhh5Ap4YnP/QLQ4+wImz8k28OPYJe5CtvfNPQI6wYb7rvK0vyPO5xSJK6GA5JUhfDIUnqYjgkSV0MhySpi+GQJHUxHJKkLoZDktTFcEiSukxNOJJcluTbSQ4kuWHoeSTpdDUV4UiyCvgUcDlwPnBNkvOHnUqSTk9TEQ7gQuBAVT1RVf8D3A5cOfBMknRampZwrAOeGrk/29YkSctsWj4dNwus1Qs2SLYB29rd55J8e+JTnbw1wDNDD5E/3zr0CEtl+NfzxoX+qk6twV/P/N4p83oO/loCkOO+nj89ztNMSzhmgQ0j99cDB0c3qKpbgFuWc6iTlWRvVW0Zeo5Tha/n0vL1XDqn2ms5LYeqHgQ2JzkvycuBdwA7B55Jkk5LU7HHUVXPJ3kvcA+wCtheVY8OPJYknZamIhwAVbUL2DX0HEtsqg6tTQFfz6Xl67l0TqnXMlV1/K0kSWqm5T0OSdIKYTgG4keoLJ0k25McTvLI0LNMuyQbktybZH+SR5O8f+iZplmSVyR5IMk/t9fzT4eeaSl4qGoA7SNU/gX4deZONX4QuKaqHht0sCmV5I3Ac8COqvr5oeeZZknWAmur6htJXg3sA67y7+aJSRLglVX1XJIzga8C76+q+wce7aS4xzEMP0JlCVXVfcDRoec4FVTVoar6Rrv9fWA/fkrDCas5z7W7Z7bL1P9v3XAMw49Q0YqXZBPwOuDrw04y3ZKsSvIQcBjYXVVT/3oajmEc9yNUpCEleRVwJ/CBqnp26HmmWVX9qKouYO4TLy5MMvWHUw3HMI77ESrSUNqx+DuBz1fVF4ee51RRVf8FfBm4bOBRTprhGIYfoaIVqb2Z+2lgf1V9fOh5pl2SmSRntds/Dvwa8K1hpzp5hmMAVfU8MP8RKvuBO/wIlROX5Dbgn4CfSTKb5LqhZ5piFwPvAt6c5KF2uWLooabYWuDeJA8z9x/G3VX1pYFnOmmejitJ6uIehySpi+GQJHUxHJKkLoZDktTFcEiSuhgOSVIXwyFJ6mI4JEld/hcHbGJXSVN2SwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply under sampling to balance the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_train_rus, y_train_rus, idx_resampled =rus.fit_sample(X_train.reshape(len(X_train), ROWS*COLS*CHANNELS), y_train)\n",
    "\n",
    "X_train_rus, y_train_rus = shuffle(X_train_rus, y_train_rus)\n",
    "\n",
    "X_train_rus = X_train_rus.reshape(len(X_train_rus), CHANNELS,ROWS, COLS)\n",
    "\n",
    "# Plot a bar plot of the labels\n",
    "#seaborn.countplot - Show value counts for a single categorical variable:\n",
    "sns.countplot(y_train_rus) #class distribution is adjusted\n",
    "\n",
    "# convert to binary encoded labels\n",
    "y_train_rus_wide = keras.utils.to_categorical(y_train_rus, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 1, 84, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=4\n",
    "row_images = 2\n",
    "col_images = 4\n",
    "plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n",
    "\n",
    "print(X_train_rus.shape)\n",
    "\n",
    "for i in range(row_images * col_images):\n",
    "    i_rand = random.randint(0, X_train_rus.shape[0])\n",
    "    plt.subplot(row_images,col_images,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sp.misc.toimage(X_train_rus[i_rand][0]),cmap='gray')\n",
    "    plt.title((\"Action Number \",y_train_rus[i_rand]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Modelling </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 32, 80, 80)        832       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 32, 80, 80)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 32, 40, 40)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 36, 36)        25632     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 32, 36, 36)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 18, 18)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 14, 14)        25632     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 32, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 32, 7, 7)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 7, 7)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 64, 3, 3)          51264     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 64, 3, 3)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 64, 1, 1)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 121,028\n",
      "Trainable params: 121,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Conv2D(32, (5, 5), input_shape=input_shape))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model5.add(Conv2D(32, (5, 5)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model5.add(Conv2D(32, (5, 5)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(Conv2D(64, (5, 5)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(256))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(num_classes))\n",
    "model5.add(Activation('softmax'))\n",
    "\n",
    "model5.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. We allow the model to overfit but save the best set of weights based on validation loss as we go. Then at the end of training (assuming the model has overfit) we revert back to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 460 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.5633 - acc: 0.7500 - val_loss: 0.5619 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.5614 - acc: 0.7500 - val_loss: 0.5604 - val_acc: 0.7500\n",
      "Epoch 3/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.5588 - acc: 0.7500 - val_loss: 0.5538 - val_acc: 0.7500\n",
      "Epoch 4/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.5488 - acc: 0.7500 - val_loss: 0.5364 - val_acc: 0.7500\n",
      "Epoch 5/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.5323 - acc: 0.7560 - val_loss: 0.5121 - val_acc: 0.7565\n",
      "Epoch 6/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.5238 - acc: 0.7582 - val_loss: 0.5077 - val_acc: 0.7586\n",
      "Epoch 7/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.5096 - acc: 0.7609 - val_loss: 0.5024 - val_acc: 0.7716\n",
      "Epoch 8/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.5140 - acc: 0.7641 - val_loss: 0.4984 - val_acc: 0.7694\n",
      "Epoch 9/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.4998 - acc: 0.7636 - val_loss: 0.4961 - val_acc: 0.7608\n",
      "Epoch 10/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.5057 - acc: 0.7668 - val_loss: 0.4948 - val_acc: 0.7651\n",
      "Epoch 11/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4900 - acc: 0.7707 - val_loss: 0.4941 - val_acc: 0.7716\n",
      "Epoch 12/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4868 - acc: 0.7696 - val_loss: 0.4896 - val_acc: 0.7586\n",
      "Epoch 13/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4802 - acc: 0.7717 - val_loss: 0.4880 - val_acc: 0.7672\n",
      "Epoch 14/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.4885 - acc: 0.7696 - val_loss: 0.4865 - val_acc: 0.7716\n",
      "Epoch 15/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4728 - acc: 0.7804 - val_loss: 0.4853 - val_acc: 0.7694\n",
      "Epoch 16/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4632 - acc: 0.7810 - val_loss: 0.4839 - val_acc: 0.7802\n",
      "Epoch 17/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.4652 - acc: 0.7826 - val_loss: 0.4850 - val_acc: 0.7759\n",
      "Epoch 18/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4617 - acc: 0.7832 - val_loss: 0.4884 - val_acc: 0.7716\n",
      "Epoch 19/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4661 - acc: 0.7766 - val_loss: 0.4892 - val_acc: 0.7716\n",
      "Epoch 20/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4474 - acc: 0.7891 - val_loss: 0.4827 - val_acc: 0.7866\n",
      "Epoch 21/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.4509 - acc: 0.7897 - val_loss: 0.4834 - val_acc: 0.7845\n",
      "Epoch 22/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4318 - acc: 0.8027 - val_loss: 0.4904 - val_acc: 0.7672\n",
      "Epoch 23/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4304 - acc: 0.8043 - val_loss: 0.4923 - val_acc: 0.7629\n",
      "Epoch 24/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4297 - acc: 0.8043 - val_loss: 0.4846 - val_acc: 0.7672\n",
      "Epoch 25/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4295 - acc: 0.8049 - val_loss: 0.4823 - val_acc: 0.7694\n",
      "Epoch 26/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4117 - acc: 0.8141 - val_loss: 0.4791 - val_acc: 0.7716\n",
      "Epoch 27/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.4043 - acc: 0.8174 - val_loss: 0.4870 - val_acc: 0.7694\n",
      "Epoch 28/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.4025 - acc: 0.8158 - val_loss: 0.5006 - val_acc: 0.7629\n",
      "Epoch 29/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.4088 - acc: 0.8130 - val_loss: 0.4958 - val_acc: 0.7737\n",
      "Epoch 30/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3936 - acc: 0.8228 - val_loss: 0.4796 - val_acc: 0.7780\n",
      "Epoch 31/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3992 - acc: 0.8174 - val_loss: 0.4839 - val_acc: 0.7694\n",
      "Epoch 32/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3945 - acc: 0.8098 - val_loss: 0.4780 - val_acc: 0.7802\n",
      "Epoch 33/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3796 - acc: 0.8266 - val_loss: 0.4992 - val_acc: 0.7737\n",
      "Epoch 34/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3780 - acc: 0.8223 - val_loss: 0.5259 - val_acc: 0.7586\n",
      "Epoch 35/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3681 - acc: 0.8234 - val_loss: 0.5181 - val_acc: 0.7565\n",
      "Epoch 36/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3689 - acc: 0.8359 - val_loss: 0.5127 - val_acc: 0.7608\n",
      "Epoch 37/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3666 - acc: 0.8370 - val_loss: 0.5056 - val_acc: 0.7651\n",
      "Epoch 38/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3616 - acc: 0.8386 - val_loss: 0.5098 - val_acc: 0.7586\n",
      "Epoch 39/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3544 - acc: 0.8429 - val_loss: 0.5435 - val_acc: 0.7672\n",
      "Epoch 40/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3583 - acc: 0.8342 - val_loss: 0.5339 - val_acc: 0.7694\n",
      "Epoch 41/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3311 - acc: 0.8500 - val_loss: 0.5221 - val_acc: 0.7608\n",
      "Epoch 42/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3411 - acc: 0.8522 - val_loss: 0.5264 - val_acc: 0.7543\n",
      "Epoch 43/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3276 - acc: 0.8418 - val_loss: 0.5650 - val_acc: 0.7651\n",
      "Epoch 44/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3161 - acc: 0.8685 - val_loss: 0.5731 - val_acc: 0.7565\n",
      "Epoch 45/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3158 - acc: 0.8522 - val_loss: 0.5847 - val_acc: 0.7565\n",
      "Epoch 46/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3161 - acc: 0.8549 - val_loss: 0.5717 - val_acc: 0.7694\n",
      "Epoch 47/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3156 - acc: 0.8543 - val_loss: 0.5648 - val_acc: 0.7759\n",
      "Epoch 48/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.2827 - acc: 0.8717 - val_loss: 0.5912 - val_acc: 0.7414\n",
      "Epoch 49/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2963 - acc: 0.8674 - val_loss: 0.5976 - val_acc: 0.7586\n",
      "Epoch 50/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2788 - acc: 0.8772 - val_loss: 0.5897 - val_acc: 0.7629\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "# Set up the callback to save the best model based on validaion data\n",
    "best_weights_filepath = './best_weights_notebook_model5.hdf5'\n",
    "mcp = ModelCheckpoint(best_weights_filepath, monitor=\"val_acc\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model5.fit(X_train_rus, y_train_rus_wide,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.2,\n",
    "          shuffle=True,\n",
    "          callbacks=[mcp])\n",
    "\n",
    "end = time.time()\n",
    "timetaken=end - start\n",
    "model_train_time_comparisons['Model 5'] = timetaken\n",
    "#reload best weights\n",
    "model5.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evaluation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5133437990580848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.55      0.63       894\n",
      "           1       0.09      0.60      0.16        60\n",
      "           2       0.67      0.49      0.57       896\n",
      "           3       0.09      0.33      0.14        61\n",
      "\n",
      "   micro avg       0.51      0.51      0.51      1911\n",
      "   macro avg       0.40      0.49      0.37      1911\n",
      "weighted avg       0.67      0.51      0.57      1911\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>488</td>\n",
       "      <td>164</td>\n",
       "      <td>186</td>\n",
       "      <td>56</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142</td>\n",
       "      <td>168</td>\n",
       "      <td>437</td>\n",
       "      <td>149</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>648</td>\n",
       "      <td>387</td>\n",
       "      <td>650</td>\n",
       "      <td>226</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3   All\n",
       "True                               \n",
       "0          488  164  186   56   894\n",
       "1            9   36   14    1    60\n",
       "2          142  168  437  149   896\n",
       "3            9   19   13   20    61\n",
       "All        648  387  650  226  1911"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model5.predict_classes(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test_num, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Model 5\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test_num, y_pred))\n",
    "report = metrics.classification_report(y_test_num, y_pred)\n",
    "model_test_precision_comparisons[\"Model 5\"] = report.split('\\n')[-2].split('      ')[1]\n",
    "model_test_recall_comparisons[\"Model 5\"] = report.split('\\n')[-2].split('      ')[2]\n",
    "model_test_f1_comparisons[\"Model 5\"] = report.split('\\n')[-2].split('      ')[3]\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test_num), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Persist A Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model5_part1.mod\"\n",
    "model5.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "In Model 6, we are <b>changing the number of filters</b>. Usually, the number of filters in the convolutional layer grows after each layer. The first few layers with lower number of filter learns simple features of the images whereas the deeper layers learn more complex features. So we have created a model in which we are increasing the number of filters as we go deeper <br/>\n",
    "In Model 6, we have 4 convolutional layers. <br/> \n",
    "1) <b>First convolutional layer</b> consists consists of <b>32 filters</b> each of <b>size 3 x 3</b>. Followed by another max pooling layer of 2x2 window. <br/>\n",
    "2) <b>Second convolutional</b> layer consists consists of <b>32 filters</b> each of<b> size 3 x 3</b>. Followed by another max pooling layer of 2x2 window. <br/>\n",
    "3) <b>Third convolutional layer </b>consists consists of <b>64 filters </b>each of size 3 x 3. Followed by another max pooling layer of 2x2 window. <br/>\n",
    "4) <b>Fourth convolutional layer </b>consists consists of <b>128 filters </b>each of size 3 x 3. Followed by another max pooling layer of 2x2 window. <br/>\n",
    "5) Next layer is the <b>dense layer</b> (fully connected layer) that has <b>256 neurons</b>. This is followed by <b>dropout layer</b> with a dropout rate of 0.5.<br/>\n",
    "6) The final output layer is another <b>dense layer</b> which has number of neurons equal to the number of classes. Since it is a multi-class classification problem, the activation function is set to <b>softmax</b>\n",
    "\n",
    "<b> <font color=\"red\">In Model 6 we are handling class imbalance problem </font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom split to train and test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X_data, y_data, random_state=0, test_size = 0.30, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of clases\n",
    "num_classes = len(set(y_data))\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_train_num = y_train_encoder.fit_transform(y_train)\n",
    "y_train_wide = keras.utils.to_categorical(y_train_num, num_classes)\n",
    "\n",
    "y_test_num = y_train_encoder.fit_transform(y_test)\n",
    "y_test_wide = keras.utils.to_categorical(y_test_num, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply under sampling to balance the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_train_rus, y_train_rus, idx_resampled =rus.fit_sample(X_train.reshape(len(X_train), ROWS*COLS*CHANNELS), y_train)\n",
    "\n",
    "\n",
    "X_train_rus, y_train_rus = shuffle(X_train_rus, y_train_rus)\n",
    "\n",
    "X_train_rus = X_train_rus.reshape(len(X_train_rus), CHANNELS,ROWS, COLS)\n",
    "#print(X_train_rus.shape) #(2144, 1, 64, 64)\n",
    "\n",
    "# Plot a bar plot of the labels\n",
    "#seaborn.countplot - Show value counts for a single categorical variable:\n",
    "sns.countplot(y_train_rus) #class distribution is adjusted\n",
    "\n",
    "# convert to binary encoded labels\n",
    "y_train_rus_wide = keras.utils.to_categorical(y_train_rus, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 1, 84, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=4\n",
    "row_images = 2\n",
    "col_images = 4\n",
    "plt.figure(figsize=(col_images*pltsize, row_images*pltsize))\n",
    "\n",
    "print(X_train_rus.shape)\n",
    "\n",
    "for i in range(row_images * col_images):\n",
    "    i_rand = random.randint(0, X_train_rus.shape[0])\n",
    "    plt.subplot(row_images,col_images,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sp.misc.toimage(X_train_rus[i_rand][0]),cmap='gray')\n",
    "    plt.title((\"Action Number \",y_train_rus[i_rand]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Modelling </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 32, 82, 82)        320       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 32, 82, 82)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 32, 41, 41)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 39, 39)        9248      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 32, 39, 39)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 32, 19, 19)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 64, 17, 17)        18496     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 64, 17, 17)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 128, 6, 6)         73856     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 128, 6, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 128, 3, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 398,116\n",
      "Trainable params: 398,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model6.add(Conv2D(32, (3, 3)))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model6.add(Conv2D(64, (3, 3)))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model6.add(Conv2D(128, (3, 3)))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(256))\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(Dropout(0.5))\n",
    "model6.add(Dense(num_classes))\n",
    "model6.add(Activation('softmax'))\n",
    "\n",
    "model6.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. We allow the model to overfit but save the best set of weights based on validation loss as we go. Then at the end of training (assuming the model has overfit) we revert back to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 460 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.5616 - acc: 0.7500 - val_loss: 0.5605 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5566 - acc: 0.7500 - val_loss: 0.5562 - val_acc: 0.7500\n",
      "Epoch 3/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.5398 - acc: 0.7516 - val_loss: 0.5433 - val_acc: 0.7522\n",
      "Epoch 4/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5162 - acc: 0.7582 - val_loss: 0.5661 - val_acc: 0.7543\n",
      "Epoch 5/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.5242 - acc: 0.7592 - val_loss: 0.5364 - val_acc: 0.7522\n",
      "Epoch 6/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.5092 - acc: 0.7582 - val_loss: 0.5378 - val_acc: 0.7478\n",
      "Epoch 7/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.5088 - acc: 0.7625 - val_loss: 0.5311 - val_acc: 0.7543\n",
      "Epoch 8/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4923 - acc: 0.7679 - val_loss: 0.5401 - val_acc: 0.7500\n",
      "Epoch 9/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4882 - acc: 0.7679 - val_loss: 0.5370 - val_acc: 0.7457\n",
      "Epoch 10/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.4711 - acc: 0.7772 - val_loss: 0.5368 - val_acc: 0.7349\n",
      "Epoch 11/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4645 - acc: 0.7788 - val_loss: 0.5291 - val_acc: 0.7392\n",
      "Epoch 12/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4561 - acc: 0.7821 - val_loss: 0.5364 - val_acc: 0.7371\n",
      "Epoch 13/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4437 - acc: 0.7918 - val_loss: 0.5393 - val_acc: 0.7478\n",
      "Epoch 14/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4340 - acc: 0.7962 - val_loss: 0.5274 - val_acc: 0.7414\n",
      "Epoch 15/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4255 - acc: 0.8022 - val_loss: 0.5380 - val_acc: 0.7435\n",
      "Epoch 16/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.4166 - acc: 0.8141 - val_loss: 0.5157 - val_acc: 0.7392\n",
      "Epoch 17/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.4122 - acc: 0.8076 - val_loss: 0.5514 - val_acc: 0.7263\n",
      "Epoch 18/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3940 - acc: 0.8304 - val_loss: 0.5478 - val_acc: 0.7349\n",
      "Epoch 19/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3847 - acc: 0.8326 - val_loss: 0.5686 - val_acc: 0.7478\n",
      "Epoch 20/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.3789 - acc: 0.8272 - val_loss: 0.5444 - val_acc: 0.7349\n",
      "Epoch 21/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.3573 - acc: 0.8386 - val_loss: 0.5550 - val_acc: 0.7500\n",
      "Epoch 22/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3514 - acc: 0.8457 - val_loss: 0.5511 - val_acc: 0.7414\n",
      "Epoch 23/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3340 - acc: 0.8489 - val_loss: 0.5596 - val_acc: 0.7543\n",
      "Epoch 24/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3221 - acc: 0.8641 - val_loss: 0.6006 - val_acc: 0.7392\n",
      "Epoch 25/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3111 - acc: 0.8647 - val_loss: 0.5656 - val_acc: 0.7629\n",
      "Epoch 26/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.3067 - acc: 0.8717 - val_loss: 0.6255 - val_acc: 0.7349\n",
      "Epoch 27/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2913 - acc: 0.8707 - val_loss: 0.6345 - val_acc: 0.7349\n",
      "Epoch 28/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.2894 - acc: 0.8745 - val_loss: 0.5868 - val_acc: 0.7500\n",
      "Epoch 29/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2749 - acc: 0.8799 - val_loss: 0.6555 - val_acc: 0.7435\n",
      "Epoch 30/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2782 - acc: 0.8793 - val_loss: 0.6332 - val_acc: 0.7414\n",
      "Epoch 31/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2625 - acc: 0.8940 - val_loss: 0.6765 - val_acc: 0.7543\n",
      "Epoch 32/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2558 - acc: 0.8918 - val_loss: 0.6552 - val_acc: 0.7500\n",
      "Epoch 33/50\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2272 - acc: 0.9087 - val_loss: 0.7025 - val_acc: 0.7457\n",
      "Epoch 34/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2109 - acc: 0.9060 - val_loss: 0.7488 - val_acc: 0.7457\n",
      "Epoch 35/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1960 - acc: 0.9190 - val_loss: 0.7809 - val_acc: 0.7371\n",
      "Epoch 36/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1971 - acc: 0.9190 - val_loss: 0.8163 - val_acc: 0.7414\n",
      "Epoch 37/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1885 - acc: 0.9196 - val_loss: 0.7694 - val_acc: 0.7306\n",
      "Epoch 38/50\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.1752 - acc: 0.9272 - val_loss: 0.8386 - val_acc: 0.7435\n",
      "Epoch 39/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1677 - acc: 0.9293 - val_loss: 0.7963 - val_acc: 0.7522\n",
      "Epoch 40/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1509 - acc: 0.9402 - val_loss: 0.8596 - val_acc: 0.7392\n",
      "Epoch 41/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1495 - acc: 0.9429 - val_loss: 0.9008 - val_acc: 0.7478\n",
      "Epoch 42/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1385 - acc: 0.9484 - val_loss: 0.9967 - val_acc: 0.7543\n",
      "Epoch 43/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1418 - acc: 0.9397 - val_loss: 0.9853 - val_acc: 0.7457\n",
      "Epoch 44/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1295 - acc: 0.9467 - val_loss: 0.9886 - val_acc: 0.7478\n",
      "Epoch 45/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1070 - acc: 0.9609 - val_loss: 1.0271 - val_acc: 0.7500\n",
      "Epoch 46/50\n",
      "460/460 [==============================] - 6s 14ms/step - loss: 0.1142 - acc: 0.9549 - val_loss: 1.0149 - val_acc: 0.7284\n",
      "Epoch 47/50\n",
      "460/460 [==============================] - 7s 14ms/step - loss: 0.0940 - acc: 0.9663 - val_loss: 1.1632 - val_acc: 0.7651\n",
      "Epoch 48/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.0957 - acc: 0.9679 - val_loss: 1.2019 - val_acc: 0.7586\n",
      "Epoch 49/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1031 - acc: 0.9576 - val_loss: 1.1413 - val_acc: 0.7478\n",
      "Epoch 50/50\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.0984 - acc: 0.9609 - val_loss: 1.2395 - val_acc: 0.7392\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "# Set up the callback to save the best model based on validaion data\n",
    "best_weights_filepath = './best_weights_notebook_model6.hdf5'\n",
    "mcp = ModelCheckpoint(best_weights_filepath, monitor=\"val_acc\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model6.fit(X_train_rus, y_train_rus_wide,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.2,\n",
    "          shuffle=True,\n",
    "          callbacks=[mcp])\n",
    "end = time.time()\n",
    "timetaken=end - start\n",
    "model_train_time_comparisons['Model 6'] = timetaken\n",
    "\n",
    "#reload best weights\n",
    "model6.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evaluation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.456305599162742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.62      0.66       894\n",
      "           1       0.10      0.57      0.17        60\n",
      "           2       0.66      0.29      0.41       896\n",
      "           3       0.06      0.41      0.11        61\n",
      "\n",
      "   micro avg       0.46      0.46      0.46      1911\n",
      "   macro avg       0.38      0.47      0.34      1911\n",
      "weighted avg       0.64      0.46      0.51      1911\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>551</td>\n",
       "      <td>96</td>\n",
       "      <td>117</td>\n",
       "      <td>130</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201</td>\n",
       "      <td>193</td>\n",
       "      <td>262</td>\n",
       "      <td>240</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>782</td>\n",
       "      <td>330</td>\n",
       "      <td>397</td>\n",
       "      <td>402</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3   All\n",
       "True                               \n",
       "0          551   96  117  130   894\n",
       "1           11   34    8    7    60\n",
       "2          201  193  262  240   896\n",
       "3           19    7   10   25    61\n",
       "All        782  330  397  402  1911"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model6.predict_classes(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test_num, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Model 6\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test_num, y_pred))\n",
    "report = metrics.classification_report(y_test_num, y_pred)\n",
    "model_test_precision_comparisons[\"Model 6\"] = report.split('\\n')[-2].split('      ')[1]\n",
    "model_test_recall_comparisons[\"Model 6\"] = report.split('\\n')[-2].split('      ')[2]\n",
    "model_test_f1_comparisons[\"Model 6\"] = report.split('\\n')[-2].split('      ')[3]\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test_num), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Persist A Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model6_part1.mod\"\n",
    "model6.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Deploy each of the two models trained to the Lunar Lander Game play 200 episodes and analyse the reward achieved by the models trained using each approach <br/>\n",
    "\n",
    "1) The lunar_lander_ml_images_player.py and lunar_lander_rl_player.py python scripts contain the code to load a saved model and run iterations of the game using that model.<br/>\n",
    "2) Write a short document (no more that 350 words) in a Jupyter notebook to describe the results of the experiments.\n",
    "3) Reflect on the performance of each model. <br/>\n",
    "4) Reflect on the amount of computation required to train each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_model_reward_comparisons = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-04-23 00:09:30.608593: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "['-0.02', '+0.93', '-0.76', '-0.22', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.11\n",
      "['-0.17', '+0.78', '-0.76', '-0.75', '+0.19', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.77\n",
      "['-0.35', '+0.60', '-1.18', '-0.42', '+0.32', '+0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -59.93\n",
      "['-0.63', '+0.52', '-1.49', '-0.35', '+0.44', '+0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -115.32\n",
      "['-0.93', '+0.33', '-1.49', '-0.89', '+0.57', '+0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -165.10\n",
      "['-1.00', '+0.26', '-1.49', '-1.02', '+0.60', '+0.13', '+0.00', '+0.00']\n",
      "step 85 total_reward -277.29\n",
      "['+0.01', '+0.94', '+0.70', '-0.13', '-0.02', '-0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.93\n",
      "['+0.15', '+0.81', '+0.70', '-0.66', '-0.17', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.01\n",
      "['+0.31', '+0.62', '+0.96', '-0.53', '-0.35', '-0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -51.87\n",
      "['+0.55', '+0.51', '+1.46', '-0.24', '-0.55', '-0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -123.23\n",
      "['+0.90', '+0.44', '+1.90', '-0.36', '-0.78', '-0.23', '+0.00', '+0.00']\n",
      "step 80 total_reward -220.69\n",
      "['+1.01', '+0.40', '+1.90', '-0.52', '-0.85', '-0.23', '+0.00', '+0.00']\n",
      "step 86 total_reward -336.56\n",
      "['-0.01', '+0.95', '-0.39', '+0.25', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.68\n",
      "['-0.08', '+0.94', '-0.39', '-0.28', '+0.09', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -9.42\n",
      "['-0.16', '+0.77', '-0.40', '-0.75', '+0.18', '+0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.10\n",
      "['-0.27', '+0.61', '-0.75', '-0.37', '+0.27', '+0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -41.09\n",
      "['-0.46', '+0.55', '-1.14', '-0.06', '+0.31', '+0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -86.13\n",
      "['-0.71', '+0.53', '-1.29', '-0.27', '+0.36', '+0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -129.11\n",
      "['-0.97', '+0.36', '-1.29', '-0.80', '+0.41', '+0.05', '+0.00', '+0.00']\n",
      "step 120 total_reward -169.24\n",
      "['-1.01', '+0.33', '-1.29', '-0.88', '+0.42', '+0.05', '+0.00', '+0.00']\n",
      "step 123 total_reward -274.20\n",
      "['+0.01', '+0.94', '+0.28', '-0.08', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.73\n",
      "['+0.06', '+0.83', '+0.28', '-0.61', '-0.07', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.72\n",
      "['+0.12', '+0.64', '+0.32', '-0.53', '-0.14', '-0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -22.99\n",
      "['+0.21', '+0.55', '+0.61', '-0.07', '-0.26', '-0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -33.93\n",
      "['+0.37', '+0.58', '+0.98', '+0.19', '-0.39', '-0.14', '+0.00', '+0.00']\n",
      "step 80 total_reward -100.68\n",
      "['+0.63', '+0.68', '+1.65', '+0.49', '-0.48', '-0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward -212.80\n",
      "['+1.00', '+0.81', '+1.88', '+0.22', '-0.53', '-0.05', '+0.00', '+0.00']\n",
      "step 120 total_reward -272.30\n",
      "['+1.02', '+0.81', '+1.88', '+0.19', '-0.53', '-0.05', '+0.00', '+0.00']\n",
      "step 121 total_reward -372.30\n",
      "['+0.01', '+0.93', '+0.61', '-0.24', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.14\n",
      "['+0.13', '+0.78', '+0.60', '-0.77', '-0.12', '-0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.11\n",
      "['+0.27', '+0.58', '+0.83', '-0.53', '-0.23', '-0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -31.84\n",
      "['+0.47', '+0.47', '+1.22', '-0.21', '-0.33', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -75.68\n",
      "['+0.77', '+0.46', '+1.77', '+0.10', '-0.41', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -166.14\n",
      "['+1.01', '+0.44', '+1.77', '-0.27', '-0.43', '-0.03', '+0.00', '+0.00']\n",
      "step 94 total_reward -289.21\n",
      "['+0.00', '+0.94', '+0.23', '-0.10', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.00\n",
      "['+0.05', '+0.82', '+0.23', '-0.63', '-0.06', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.29\n",
      "['+0.10', '+0.63', '+0.30', '-0.48', '-0.10', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -16.34\n",
      "['+0.18', '+0.54', '+0.53', '-0.05', '-0.15', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -16.52\n",
      "['+0.32', '+0.60', '+0.75', '+0.40', '-0.16', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -66.65\n",
      "['+0.49', '+0.78', '+0.99', '+0.79', '-0.22', '-0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -144.36\n",
      "['+0.70', '+1.06', '+1.16', '+0.88', '-0.31', '-0.11', '+0.00', '+0.00']\n",
      "step 120 total_reward -212.76\n",
      "['+0.94', '+1.24', '+1.16', '+0.35', '-0.42', '-0.11', '+0.00', '+0.00']\n",
      "step 140 total_reward -227.11\n",
      "['+1.01', '+1.27', '+1.16', '+0.19', '-0.45', '-0.11', '+0.00', '+0.00']\n",
      "step 146 total_reward -331.79\n",
      "['-0.01', '+0.93', '-0.76', '-0.37', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.30\n",
      "['-0.16', '+0.74', '-0.77', '-0.83', '+0.19', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.27\n",
      "['-0.35', '+0.55', '-1.09', '-0.45', '+0.38', '+0.23', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.83\n",
      "['-0.61', '+0.42', '-1.38', '-0.59', '+0.66', '+0.32', '+0.00', '+0.00']\n",
      "step 60 total_reward -122.71\n",
      "['-0.88', '+0.17', '-1.38', '-1.12', '+0.98', '+0.32', '+0.00', '+0.00']\n",
      "step 80 total_reward -198.15\n",
      "['-1.00', '-0.00', '-1.38', '-1.36', '+1.13', '+0.32', '+0.00', '+0.00']\n",
      "step 89 total_reward -334.58\n",
      "['-0.01', '+0.95', '-0.31', '+0.16', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.72\n",
      "['-0.07', '+0.91', '-0.31', '-0.37', '+0.08', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.34\n",
      "['-0.13', '+0.72', '-0.31', '-0.78', '+0.14', '+0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.68\n",
      "['-0.22', '+0.53', '-0.59', '-0.49', '+0.20', '+0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -29.85\n",
      "['-0.37', '+0.43', '-0.90', '-0.19', '+0.21', '+0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward -51.45\n",
      "['-0.56', '+0.32', '-0.94', '-0.63', '+0.28', '+0.08', '+0.00', '+0.00']\n",
      "step 100 total_reward -88.23\n",
      "['-0.74', '+0.08', '-0.39', '-0.41', '+0.38', '+0.24', '+0.00', '+0.00']\n",
      "step 120 total_reward -51.82\n",
      "['-0.74', '+0.06', '-0.04', '-0.01', '+0.38', '-0.43', '+0.00', '+1.00']\n",
      "step 122 total_reward -145.30\n",
      "['-0.01', '+0.95', '-0.71', '+0.36', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.06\n",
      "['-0.15', '+0.98', '-0.71', '-0.17', '+0.18', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.98\n",
      "['-0.30', '+0.84', '-0.71', '-0.70', '+0.33', '+0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.17\n",
      "['-0.48', '+0.64', '-1.22', '-0.50', '+0.48', '+0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -89.08\n",
      "['-0.76', '+0.47', '-1.44', '-0.79', '+0.62', '+0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -146.03\n",
      "['-1.00', '+0.21', '-1.44', '-1.25', '+0.73', '+0.13', '+0.00', '+0.00']\n",
      "step 97 total_reward -292.87\n",
      "['+0.00', '+0.93', '+0.25', '-0.21', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.54\n",
      "['+0.05', '+0.79', '+0.25', '-0.75', '-0.06', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.43\n",
      "['+0.11', '+0.59', '+0.37', '-0.51', '-0.12', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -14.14\n",
      "['+0.21', '+0.48', '+0.62', '-0.24', '-0.10', '+0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -14.54\n",
      "['+0.35', '+0.47', '+0.71', '+0.15', '-0.05', '+0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward -27.85\n",
      "['+0.49', '+0.58', '+0.75', '+0.56', '-0.03', '+0.03', '+0.00', '+0.00']\n",
      "step 100 total_reward -70.56\n",
      "['+0.64', '+0.79', '+0.76', '+0.84', '-0.03', '+0.01', '+0.00', '+0.00']\n",
      "step 120 total_reward -120.90\n",
      "['+0.80', '+1.02', '+0.78', '+0.55', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 140 total_reward -130.06\n",
      "['+0.95', '+1.10', '+0.78', '+0.01', '+0.03', '+0.03', '+0.00', '+0.00']\n",
      "step 160 total_reward -131.70\n",
      "['+1.01', '+1.09', '+0.78', '-0.17', '+0.04', '+0.03', '+0.00', '+0.00']\n",
      "step 167 total_reward -236.45\n",
      "['+0.00', '+0.93', '+0.25', '-0.39', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.82\n",
      "['+0.05', '+0.73', '+0.27', '-0.85', '-0.06', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.72\n",
      "['+0.12', '+0.54', '+0.39', '-0.45', '-0.11', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward +5.06\n",
      "['+0.21', '+0.47', '+0.56', '-0.04', '-0.18', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -0.81\n",
      "['+0.35', '+0.52', '+0.90', '+0.42', '-0.26', '-0.06', '+0.00', '+0.00']\n",
      "step 80 total_reward -68.53\n",
      "['+0.57', '+0.71', '+1.31', '+0.84', '-0.33', '-0.07', '+0.00', '+0.00']\n",
      "step 100 total_reward -166.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.86', '+0.98', '+1.49', '+0.72', '-0.42', '-0.11', '+0.00', '+0.00']\n",
      "step 120 total_reward -227.68\n",
      "['+1.01', '+1.07', '+1.49', '+0.45', '-0.48', '-0.11', '+0.00', '+0.00']\n",
      "step 130 total_reward -338.59\n",
      "['+0.00', '+0.95', '+0.19', '+0.40', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.55\n",
      "['+0.04', '+0.99', '+0.19', '-0.14', '-0.05', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward +14.67\n",
      "['+0.08', '+0.86', '+0.19', '-0.67', '-0.09', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.77\n",
      "['+0.12', '+0.62', '+0.28', '-0.76', '-0.11', '-0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -17.58\n",
      "['+0.19', '+0.46', '+0.45', '-0.27', '-0.15', '-0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward +14.92\n",
      "['+0.31', '+0.45', '+0.71', '+0.16', '-0.20', '-0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward -21.85\n",
      "['+0.48', '+0.56', '+0.97', '+0.45', '-0.24', '-0.06', '+0.00', '+0.00']\n",
      "step 120 total_reward -84.33\n",
      "['+0.72', '+0.76', '+1.36', '+0.85', '-0.29', '-0.05', '+0.00', '+0.00']\n",
      "step 140 total_reward -179.68\n",
      "['+0.99', '+0.94', '+1.39', '+0.37', '-0.34', '-0.04', '+0.00', '+0.00']\n",
      "step 160 total_reward -200.51\n",
      "['+1.01', '+0.95', '+1.39', '+0.34', '-0.34', '-0.04', '+0.00', '+0.00']\n",
      "step 161 total_reward -300.51\n",
      "['-0.01', '+0.94', '-0.37', '-0.18', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.18\n",
      "['-0.08', '+0.80', '-0.37', '-0.72', '+0.09', '+0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.43\n",
      "['-0.16', '+0.58', '-0.49', '-0.56', '+0.19', '+0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.84\n",
      "['-0.29', '+0.47', '-0.85', '-0.17', '+0.32', '+0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -50.43\n",
      "['-0.49', '+0.42', '-1.10', '-0.36', '+0.56', '+0.43', '+0.00', '+0.00']\n",
      "step 80 total_reward -115.28\n",
      "['-0.70', '+0.23', '-1.10', '-0.89', '+0.99', '+0.43', '+0.00', '+0.00']\n",
      "step 100 total_reward -193.79\n",
      "['-0.80', '+0.13', '-1.00', '-0.08', '+1.82', '+4.64', '+0.00', '+1.00']\n",
      "step 109 total_reward -309.72\n",
      "['+0.01', '+0.95', '+0.62', '+0.29', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.07\n",
      "['+0.13', '+0.95', '+0.62', '-0.24', '-0.15', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.26\n",
      "['+0.26', '+0.80', '+0.62', '-0.78', '-0.29', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.39\n",
      "['+0.42', '+0.58', '+1.04', '-0.56', '-0.42', '-0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -72.37\n",
      "['+0.69', '+0.47', '+1.72', '-0.19', '-0.58', '-0.18', '+0.00', '+0.00']\n",
      "step 80 total_reward -160.96\n",
      "['+1.01', '+0.36', '+1.76', '-0.64', '-0.73', '-0.17', '+0.00', '+0.00']\n",
      "step 98 total_reward -311.89\n",
      "['+0.00', '+0.93', '+0.15', '-0.43', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.93\n",
      "['+0.03', '+0.72', '+0.15', '-0.80', '-0.04', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.97\n",
      "['+0.07', '+0.56', '+0.25', '-0.35', '-0.10', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward +20.47\n",
      "['+0.12', '+0.51', '+0.33', '-0.05', '-0.22', '-0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward +16.42\n",
      "['+0.22', '+0.56', '+0.70', '+0.33', '-0.37', '-0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -57.30\n",
      "['+0.42', '+0.71', '+1.33', '+0.67', '-0.55', '-0.19', '+0.00', '+0.00']\n",
      "step 100 total_reward -174.92\n",
      "['+0.77', '+0.94', '+2.06', '+0.79', '-0.72', '-0.17', '+0.00', '+0.00']\n",
      "step 120 total_reward -307.45\n",
      "['+1.01', '+1.05', '+2.06', '+0.47', '-0.82', '-0.17', '+0.00', '+0.00']\n",
      "step 132 total_reward -430.63\n",
      "['+0.01', '+0.94', '+0.33', '+0.01', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.19\n",
      "['+0.07', '+0.86', '+0.33', '-0.53', '-0.08', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.94\n",
      "['+0.13', '+0.66', '+0.38', '-0.56', '-0.19', '-0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -29.79\n",
      "['+0.25', '+0.55', '+0.76', '-0.20', '-0.29', '-0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -49.72\n",
      "['+0.45', '+0.55', '+1.24', '+0.16', '-0.35', '-0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -118.36\n",
      "['+0.75', '+0.64', '+1.70', '+0.35', '-0.35', '+0.02', '+0.00', '+0.00']\n",
      "step 100 total_reward -201.14\n",
      "['+1.01', '+0.68', '+1.71', '-0.03', '-0.34', '+0.01', '+0.00', '+0.00']\n",
      "step 115 total_reward -319.18\n",
      "['-0.01', '+0.93', '-0.40', '-0.34', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.51\n",
      "['-0.09', '+0.75', '-0.40', '-0.87', '+0.10', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.93\n",
      "['-0.18', '+0.53', '-0.53', '-0.61', '+0.22', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -19.94\n",
      "['-0.31', '+0.39', '-0.83', '-0.30', '+0.40', '+0.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -45.17\n",
      "['-0.51', '+0.31', '-1.08', '-0.44', '+0.66', '+0.26', '+0.00', '+0.00']\n",
      "step 80 total_reward -111.94\n",
      "['-0.72', '+0.10', '-1.08', '-0.97', '+0.93', '+0.26', '+0.00', '+0.00']\n",
      "step 100 total_reward -180.38\n",
      "['-0.80', '+0.01', '-1.05', '-0.19', '+0.69', '-3.48', '+0.00', '+1.00']\n",
      "step 107 total_reward -266.98\n",
      "['+0.01', '+0.93', '+0.37', '-0.43', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.66\n",
      "['+0.08', '+0.72', '+0.36', '-0.74', '-0.09', '-0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.89\n",
      "['+0.17', '+0.54', '+0.48', '-0.47', '-0.22', '-0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -4.86\n",
      "['+0.30', '+0.46', '+0.90', '-0.09', '-0.43', '-0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -51.97\n",
      "['+0.54', '+0.47', '+1.53', '+0.16', '-0.63', '-0.24', '+0.00', '+0.00']\n",
      "step 80 total_reward -159.78\n",
      "['+0.92', '+0.53', '+2.09', '+0.06', '-0.89', '-0.25', '+0.00', '+0.00']\n",
      "step 100 total_reward -278.29\n",
      "['+1.01', '+0.53', '+2.09', '-0.05', '-0.94', '-0.25', '+0.00', '+0.00']\n",
      "step 104 total_reward -387.40\n",
      "['-0.01', '+0.93', '-0.33', '-0.51', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.71\n",
      "['-0.07', '+0.69', '-0.37', '-0.94', '+0.08', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.41\n",
      "['-0.16', '+0.46', '-0.57', '-0.61', '+0.11', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward +1.76\n",
      "['-0.29', '+0.35', '-0.68', '-0.23', '+0.15', '+0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward +7.04\n",
      "['-0.44', '+0.36', '-0.83', '+0.05', '+0.26', '+0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -30.72\n",
      "['-0.60', '+0.29', '-0.83', '-0.48', '+0.38', '+0.13', '+0.00', '+0.00']\n",
      "step 100 total_reward -66.41\n",
      "['-0.77', '+0.06', '-0.83', '-1.02', '+0.51', '+0.13', '+0.00', '+0.00']\n",
      "step 120 total_reward -124.26\n",
      "['-0.80', '-0.00', '-1.22', '-0.54', '+0.50', '-1.48', '+0.00', '+1.00']\n",
      "step 124 total_reward -224.67\n",
      "['+0.01', '+0.93', '+0.31', '-0.23', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['+0.07', '+0.78', '+0.31', '-0.76', '-0.08', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.12\n",
      "['+0.15', '+0.59', '+0.47', '-0.42', '-0.13', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -10.87\n",
      "['+0.25', '+0.53', '+0.63', '-0.09', '-0.24', '-0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -25.88\n",
      "['+0.43', '+0.56', '+1.09', '+0.22', '-0.30', '-0.02', '+0.00', '+0.00']\n",
      "step 80 total_reward -96.92\n",
      "['+0.69', '+0.68', '+1.49', '+0.58', '-0.32', '-0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward -180.30\n",
      "['+0.99', '+0.78', '+1.53', '+0.10', '-0.35', '-0.03', '+0.00', '+0.00']\n",
      "step 120 total_reward -206.09\n",
      "['+1.01', '+0.78', '+1.53', '+0.07', '-0.35', '-0.03', '+0.00', '+0.00']\n",
      "step 121 total_reward -306.09\n",
      "['-0.00', '+0.95', '-0.03', '+0.46', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.85\n",
      "['-0.01', '+1.01', '-0.03', '-0.08', '+0.01', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward +33.44\n",
      "['-0.01', '+0.90', '-0.03', '-0.61', '+0.01', '+0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -9.45\n",
      "['-0.01', '+0.65', '+0.01', '-0.79', '+0.03', '+0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -6.30\n",
      "['-0.02', '+0.49', '-0.05', '-0.34', '+0.08', '+0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward +43.69\n",
      "['-0.05', '+0.43', '-0.24', '-0.02', '+0.12', '+0.01', '+0.00', '+0.00']\n",
      "step 100 total_reward +49.23\n",
      "['-0.11', '+0.48', '-0.39', '+0.31', '+0.16', '+0.04', '+0.00', '+0.00']\n",
      "step 120 total_reward +8.00\n",
      "['-0.21', '+0.64', '-0.62', '+0.71', '+0.23', '+0.07', '+0.00', '+0.00']\n",
      "step 140 total_reward -67.71\n",
      "['-0.37', '+0.90', '-0.90', '+0.81', '+0.26', '+0.03', '+0.00', '+0.00']\n",
      "step 160 total_reward -131.78\n",
      "['-0.55', '+1.06', '-0.90', '+0.28', '+0.29', '+0.03', '+0.00', '+0.00']\n",
      "step 180 total_reward -129.85\n",
      "['-0.73', '+1.06', '-0.90', '-0.25', '+0.32', '+0.03', '+0.00', '+0.00']\n",
      "step 200 total_reward -141.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.91', '+0.90', '-0.90', '-0.79', '+0.35', '+0.03', '+0.00', '+0.00']\n",
      "step 220 total_reward -169.67\n",
      "['-1.01', '+0.74', '-0.90', '-1.08', '+0.36', '+0.03', '+0.00', '+0.00']\n",
      "step 231 total_reward -287.80\n",
      "['-0.01', '+0.95', '-0.43', '+0.35', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.70\n",
      "['-0.09', '+0.97', '-0.43', '-0.18', '+0.11', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.66\n",
      "['-0.18', '+0.83', '-0.43', '-0.72', '+0.20', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -36.67\n",
      "['-0.29', '+0.62', '-0.77', '-0.60', '+0.26', '+0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -43.48\n",
      "['-0.48', '+0.49', '-1.14', '-0.31', '+0.27', '+0.02', '+0.00', '+0.00']\n",
      "step 80 total_reward -72.29\n",
      "['-0.71', '+0.32', '-1.16', '-0.84', '+0.36', '+0.09', '+0.00', '+0.00']\n",
      "step 100 total_reward -115.57\n",
      "['-0.86', '+0.12', '-0.99', '-0.45', '+0.28', '-4.00', '+0.00', '+1.00']\n",
      "step 113 total_reward -231.06\n",
      "['-0.00', '+0.93', '-0.24', '-0.48', '+0.01', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.82\n",
      "['-0.05', '+0.71', '-0.27', '-0.84', '+0.06', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.63\n",
      "['-0.12', '+0.52', '-0.42', '-0.44', '+0.07', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward +17.02\n",
      "['-0.21', '+0.48', '-0.51', '+0.12', '+0.05', '-0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward +21.66\n",
      "['-0.33', '+0.59', '-0.62', '+0.59', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -28.30\n",
      "['-0.45', '+0.83', '-0.55', '+0.94', '-0.09', '-0.08', '+0.00', '+0.00']\n",
      "step 100 total_reward -92.62\n",
      "['-0.56', '+1.05', '-0.54', '+0.47', '-0.16', '-0.07', '+0.00', '+0.00']\n",
      "step 120 total_reward -87.06\n",
      "['-0.66', '+1.10', '-0.54', '-0.06', '-0.24', '-0.07', '+0.00', '+0.00']\n",
      "step 140 total_reward -87.62\n",
      "['-0.77', '+1.00', '-0.54', '-0.60', '-0.31', '-0.07', '+0.00', '+0.00']\n",
      "step 160 total_reward -118.92\n",
      "['-0.88', '+0.74', '-0.54', '-1.13', '-0.39', '-0.07', '+0.00', '+0.00']\n",
      "step 180 total_reward -159.67\n",
      "['-0.99', '+0.32', '-0.54', '-1.66', '-0.46', '-0.07', '+0.00', '+0.00']\n",
      "step 200 total_reward -205.69\n",
      "['-1.01', '+0.24', '-0.54', '-1.74', '-0.47', '-0.07', '+0.00', '+0.00']\n",
      "step 203 total_reward -311.13\n",
      "['-0.01', '+0.93', '-0.49', '-0.35', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.43\n",
      "['-0.11', '+0.74', '-0.50', '-0.82', '+0.12', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.64\n",
      "['-0.24', '+0.56', '-0.77', '-0.49', '+0.19', '+0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.30\n",
      "['-0.42', '+0.46', '-1.09', '-0.15', '+0.22', '-0.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -54.52\n",
      "['-0.66', '+0.38', '-1.20', '-0.52', '+0.29', '+0.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -95.63\n",
      "['-0.90', '+0.15', '-1.22', '-0.97', '+0.39', '+0.11', '+0.00', '+0.00']\n",
      "step 100 total_reward -146.93\n",
      "['-0.94', '+0.10', '-0.42', '-0.01', '+0.27', '-1.98', '+0.00', '+0.00']\n",
      "step 105 total_reward -222.47\n",
      "['+0.01', '+0.93', '+0.29', '-0.43', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.77\n",
      "['+0.06', '+0.72', '+0.33', '-0.83', '-0.07', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.46\n",
      "['+0.14', '+0.54', '+0.49', '-0.42', '-0.11', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward +5.18\n",
      "['+0.25', '+0.48', '+0.64', '-0.00', '-0.14', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -1.86\n",
      "['+0.40', '+0.54', '+0.83', '+0.35', '-0.17', '-0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward -49.01\n",
      "['+0.60', '+0.71', '+1.08', '+0.73', '-0.22', '-0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward -125.73\n",
      "['+0.83', '+0.95', '+1.22', '+0.68', '-0.32', '-0.10', '+0.00', '+0.00']\n",
      "step 120 total_reward -181.22\n",
      "['+1.01', '+1.06', '+1.22', '+0.28', '-0.39', '-0.10', '+0.00', '+0.00']\n",
      "step 135 total_reward -293.22\n",
      "['+0.00', '+0.95', '+0.10', '+0.35', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.86\n",
      "['+0.02', '+0.97', '+0.10', '-0.19', '-0.02', '-0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward +12.66\n",
      "['+0.04', '+0.83', '+0.10', '-0.72', '-0.05', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -27.27\n",
      "['+0.06', '+0.60', '+0.11', '-0.60', '-0.09', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -2.24\n",
      "['+0.10', '+0.48', '+0.32', '-0.21', '-0.14', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward +21.83\n",
      "['+0.18', '+0.49', '+0.53', '+0.17', '-0.19', '-0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward -9.67\n",
      "['+0.31', '+0.59', '+0.78', '+0.50', '-0.23', '-0.06', '+0.00', '+0.00']\n",
      "step 120 total_reward -71.81\n",
      "['+0.51', '+0.81', '+1.09', '+0.93', '-0.29', '-0.11', '+0.00', '+0.00']\n",
      "step 140 total_reward -163.48\n",
      "['+0.76', '+1.09', '+1.27', '+0.71', '-0.38', '-0.09', '+0.00', '+0.00']\n",
      "step 160 total_reward -213.66\n",
      "['+1.01', '+1.22', '+1.27', '+0.18', '-0.47', '-0.09', '+0.00', '+0.00']\n",
      "step 180 total_reward -329.63\n",
      "['+0.02', '+0.94', '+0.77', '+0.06', '-0.02', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.61\n",
      "['+0.17', '+0.88', '+0.77', '-0.47', '-0.19', '-0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.94\n",
      "['+0.33', '+0.69', '+0.96', '-0.54', '-0.38', '-0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.20\n",
      "['+0.58', '+0.58', '+1.55', '-0.26', '-0.60', '-0.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -134.67\n",
      "['+0.97', '+0.52', '+2.09', '-0.33', '-0.84', '-0.25', '+0.00', '+0.00']\n",
      "step 80 total_reward -244.75\n",
      "['+1.01', '+0.50', '+2.09', '-0.38', '-0.86', '-0.25', '+0.00', '+0.00']\n",
      "step 82 total_reward -348.02\n",
      "['+0.01', '+0.95', '+0.60', '+0.44', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.28\n",
      "['+0.13', '+1.00', '+0.60', '-0.09', '-0.15', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -5.04\n",
      "['+0.25', '+0.89', '+0.60', '-0.63', '-0.28', '-0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.87\n",
      "['+0.40', '+0.68', '+0.97', '-0.58', '-0.40', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -64.54\n",
      "['+0.65', '+0.57', '+1.60', '-0.25', '-0.49', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -135.71\n",
      "['+1.00', '+0.48', '+1.87', '-0.51', '-0.52', '-0.02', '+0.00', '+0.00']\n",
      "step 99 total_reward -295.13\n",
      "['+0.02', '+0.95', '+0.80', '+0.42', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.21\n",
      "['+0.17', '+1.00', '+0.80', '-0.11', '-0.20', '-0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.23\n",
      "['+0.33', '+0.88', '+0.80', '-0.65', '-0.38', '-0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.99\n",
      "['+0.55', '+0.72', '+1.40', '-0.42', '-0.52', '-0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -107.43\n",
      "['+0.89', '+0.62', '+1.87', '-0.42', '-0.69', '-0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -192.36\n",
      "['+1.01', '+0.58', '+1.87', '-0.58', '-0.74', '-0.17', '+0.00', '+0.00']\n",
      "step 86 total_reward -305.72\n",
      "['-0.01', '+0.95', '-0.48', '+0.35', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.54\n",
      "['-0.10', '+0.97', '-0.48', '-0.18', '+0.12', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.74\n",
      "['-0.20', '+0.83', '-0.48', '-0.72', '+0.22', '+0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.29\n",
      "['-0.32', '+0.61', '-0.79', '-0.63', '+0.31', '+0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -49.46\n",
      "['-0.53', '+0.47', '-1.22', '-0.38', '+0.35', '+0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -88.28\n",
      "['-0.77', '+0.28', '-1.23', '-0.91', '+0.44', '+0.09', '+0.00', '+0.00']\n",
      "step 100 total_reward -133.71\n",
      "['-1.01', '-0.06', '-1.23', '-1.42', '+0.52', '+0.09', '+0.00', '+0.00']\n",
      "step 119 total_reward -291.57\n",
      "['+0.00', '+0.94', '+0.20', '+0.01', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.13\n",
      "['+0.04', '+0.86', '+0.20', '-0.52', '-0.05', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.15\n",
      "['+0.09', '+0.65', '+0.29', '-0.68', '-0.09', '-0.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.77\n",
      "['+0.15', '+0.49', '+0.40', '-0.40', '-0.11', '-0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -12.07\n",
      "['+0.24', '+0.42', '+0.55', '+0.00', '-0.14', '-0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -17.17\n",
      "['+0.38', '+0.48', '+0.80', '+0.42', '-0.15', '+0.01', '+0.00', '+0.00']\n",
      "step 100 total_reward -72.21\n",
      "['+0.55', '+0.66', '+0.99', '+0.76', '-0.16', '-0.01', '+0.00', '+0.00']\n",
      "step 120 total_reward -139.46\n",
      "['+0.77', '+0.94', '+1.13', '+0.88', '-0.19', '-0.02', '+0.00', '+0.00']\n",
      "step 140 total_reward -200.31\n",
      "['+1.00', '+1.12', '+1.13', '+0.34', '-0.22', '-0.03', '+0.00', '+0.00']\n",
      "step 160 total_reward -206.29\n",
      "['+1.01', '+1.13', '+1.13', '+0.32', '-0.22', '-0.03', '+0.00', '+0.00']\n",
      "step 161 total_reward -306.29\n",
      "['+0.01', '+0.96', '+0.54', '+0.47', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.12', '+1.01', '+0.54', '-0.06', '-0.13', '-0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -0.64\n",
      "['+0.22', '+0.91', '+0.54', '-0.59', '-0.25', '-0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -30.50\n",
      "['+0.35', '+0.70', '+0.79', '-0.61', '-0.38', '-0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -50.24\n",
      "['+0.56', '+0.55', '+1.34', '-0.38', '-0.51', '-0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -109.21\n",
      "['+0.90', '+0.47', '+1.90', '-0.34', '-0.63', '-0.11', '+0.00', '+0.00']\n",
      "step 100 total_reward -200.86\n",
      "['+1.02', '+0.43', '+1.90', '-0.50', '-0.66', '-0.11', '+0.00', '+0.00']\n",
      "step 106 total_reward -313.68\n",
      "['-0.02', '+0.94', '-0.78', '+0.04', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.65\n",
      "['-0.17', '+0.87', '-0.78', '-0.49', '+0.19', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.61\n",
      "['-0.34', '+0.69', '-1.05', '-0.46', '+0.36', '+0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -58.26\n",
      "['-0.61', '+0.59', '-1.64', '-0.29', '+0.49', '+0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -136.80\n",
      "['-0.94', '+0.42', '-1.64', '-0.83', '+0.56', '+0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -178.38\n",
      "['-1.00', '+0.37', '-1.64', '-0.93', '+0.57', '+0.07', '+0.00', '+0.00']\n",
      "step 84 total_reward -286.15\n",
      "['+0.00', '+0.96', '+0.22', '+0.48', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.40\n",
      "['+0.05', '+1.02', '+0.22', '-0.05', '-0.05', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward +20.68\n",
      "['+0.09', '+0.92', '+0.22', '-0.59', '-0.10', '-0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -14.69\n",
      "['+0.14', '+0.67', '+0.28', '-0.82', '-0.15', '-0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -22.87\n",
      "['+0.21', '+0.48', '+0.50', '-0.47', '-0.21', '-0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward +0.40\n",
      "['+0.33', '+0.40', '+0.78', '-0.05', '-0.35', '-0.19', '+0.00', '+0.00']\n",
      "step 100 total_reward -29.37\n",
      "['+0.55', '+0.43', '+1.40', '+0.24', '-0.52', '-0.15', '+0.00', '+0.00']\n",
      "step 120 total_reward -133.39\n",
      "['+0.89', '+0.54', '+1.89', '+0.29', '-0.71', '-0.21', '+0.00', '+0.00']\n",
      "step 140 total_reward -240.55\n",
      "['+1.00', '+0.56', '+1.89', '+0.13', '-0.78', '-0.21', '+0.00', '+0.00']\n",
      "step 146 total_reward -353.15\n",
      "['-0.01', '+0.94', '-0.73', '+0.02', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.64\n",
      "['-0.16', '+0.86', '-0.73', '-0.51', '+0.18', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.91\n",
      "['-0.32', '+0.67', '-0.94', '-0.56', '+0.35', '+0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -53.67\n",
      "['-0.55', '+0.54', '-1.41', '-0.40', '+0.51', '+0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -115.19\n",
      "['-0.84', '+0.34', '-1.41', '-0.93', '+0.68', '+0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -167.15\n",
      "['-1.00', '+0.14', '-1.41', '-1.25', '+0.78', '+0.17', '+0.00', '+0.00']\n",
      "step 92 total_reward -304.31\n",
      "['+0.01', '+0.93', '+0.44', '-0.44', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.58\n",
      "['+0.10', '+0.71', '+0.49', '-0.83', '-0.11', '-0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.08\n",
      "['+0.21', '+0.52', '+0.70', '-0.47', '-0.18', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -10.70\n",
      "['+0.38', '+0.44', '+0.95', '-0.08', '-0.25', '-0.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -37.30\n",
      "['+0.61', '+0.46', '+1.32', '+0.19', '-0.32', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -105.72\n",
      "['+0.91', '+0.53', '+1.59', '+0.09', '-0.38', '-0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward -169.12\n",
      "['+1.01', '+0.53', '+1.59', '-0.07', '-0.39', '-0.04', '+0.00', '+0.00']\n",
      "step 106 total_reward -276.93\n",
      "['+0.01', '+0.94', '+0.42', '+0.07', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.01\n",
      "['+0.09', '+0.88', '+0.42', '-0.46', '-0.10', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -23.52\n",
      "['+0.17', '+0.67', '+0.45', '-0.70', '-0.20', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.63\n",
      "['+0.29', '+0.52', '+0.77', '-0.32', '-0.37', '-0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -50.88\n",
      "['+0.50', '+0.47', '+1.37', '-0.07', '-0.58', '-0.20', '+0.00', '+0.00']\n",
      "step 80 total_reward -140.88\n",
      "['+0.85', '+0.48', '+1.96', '-0.07', '-0.82', '-0.22', '+0.00', '+0.00']\n",
      "step 100 total_reward -256.42\n",
      "['+1.00', '+0.46', '+1.96', '-0.28', '-0.90', '-0.22', '+0.00', '+0.00']\n",
      "step 108 total_reward -377.09\n",
      "['+0.00', '+0.92', '+0.03', '-0.56', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.75\n",
      "['+0.01', '+0.68', '+0.03', '-0.94', '-0.01', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.48\n",
      "['+0.01', '+0.45', '-0.02', '-0.56', '-0.05', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward +33.76\n",
      "['+0.02', '+0.34', '+0.12', '-0.23', '-0.12', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward +62.39\n",
      "['+0.06', '+0.32', '+0.28', '+0.06', '-0.17', '-0.06', '+0.00', '+0.00']\n",
      "step 80 total_reward +49.64\n",
      "['+0.14', '+0.38', '+0.51', '+0.32', '-0.23', '-0.07', '+0.00', '+0.00']\n",
      "step 100 total_reward -0.23\n",
      "['+0.28', '+0.52', '+0.89', '+0.62', '-0.27', '-0.01', '+0.00', '+0.00']\n",
      "step 120 total_reward -77.14\n",
      "['+0.49', '+0.75', '+1.25', '+0.92', '-0.28', '-0.01', '+0.00', '+0.00']\n",
      "step 140 total_reward -162.09\n",
      "['+0.77', '+1.04', '+1.45', '+0.79', '-0.26', '+0.03', '+0.00', '+0.00']\n",
      "step 160 total_reward -213.09\n",
      "['+1.01', '+1.18', '+1.45', '+0.36', '-0.24', '+0.03', '+0.00', '+0.00']\n",
      "step 176 total_reward -319.88\n",
      "['+0.01', '+0.93', '+0.55', '-0.46', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.49\n",
      "['+0.12', '+0.72', '+0.59', '-0.70', '-0.14', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.34\n",
      "['+0.27', '+0.57', '+0.87', '-0.31', '-0.27', '-0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -25.44\n",
      "['+0.48', '+0.54', '+1.31', '+0.03', '-0.39', '-0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -91.47\n",
      "['+0.81', '+0.61', '+1.93', '+0.37', '-0.56', '-0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -208.83\n",
      "['+1.02', '+0.64', '+1.93', '+0.08', '-0.65', '-0.17', '+0.00', '+0.00']\n",
      "step 91 total_reward -331.74\n",
      "['+0.00', '+0.93', '+0.12', '-0.43', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.95\n",
      "['+0.03', '+0.72', '+0.13', '-0.83', '-0.03', '-0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -23.74\n",
      "['+0.06', '+0.52', '+0.17', '-0.48', '-0.06', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward +19.00\n",
      "['+0.10', '+0.44', '+0.31', '-0.05', '-0.17', '-0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward +29.64\n",
      "['+0.18', '+0.48', '+0.60', '+0.30', '-0.32', '-0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -33.27\n",
      "['+0.36', '+0.61', '+1.17', '+0.59', '-0.46', '-0.12', '+0.00', '+0.00']\n",
      "step 100 total_reward -135.93\n",
      "['+0.66', '+0.82', '+1.86', '+0.81', '-0.56', '-0.07', '+0.00', '+0.00']\n",
      "step 120 total_reward -259.28\n",
      "['+1.01', '+1.00', '+1.97', '+0.43', '-0.62', '-0.06', '+0.00', '+0.00']\n",
      "step 138 total_reward -400.16\n",
      "['-0.01', '+0.94', '-0.73', '-0.10', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.87\n",
      "['-0.16', '+0.83', '-0.73', '-0.63', '+0.18', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.21\n",
      "['-0.32', '+0.63', '-0.99', '-0.53', '+0.36', '+0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.96\n",
      "['-0.57', '+0.52', '-1.52', '-0.28', '+0.55', '+0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -129.21\n",
      "['-0.87', '+0.36', '-1.52', '-0.81', '+0.76', '+0.21', '+0.00', '+0.00']\n",
      "step 80 total_reward -184.38\n",
      "['-1.01', '+0.23', '-1.52', '-1.05', '+0.85', '+0.21', '+0.00', '+0.00']\n",
      "step 89 total_reward -311.67\n",
      "['-0.00', '+0.94', '-0.11', '+0.01', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.34\n",
      "['-0.02', '+0.86', '-0.11', '-0.52', '+0.03', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.35\n",
      "['-0.05', '+0.64', '-0.15', '-0.69', '+0.05', '+0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -36.60\n",
      "['-0.08', '+0.48', '-0.20', '-0.40', '+0.11', '+0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -7.00\n",
      "['-0.12', '+0.41', '-0.32', '-0.11', '+0.24', '+0.14', '+0.00', '+0.00']\n",
      "step 80 total_reward -8.84\n",
      "['-0.23', '+0.43', '-0.71', '+0.26', '+0.38', '+0.17', '+0.00', '+0.00']\n",
      "step 100 total_reward -77.04\n",
      "['-0.44', '+0.56', '-1.32', '+0.56', '+0.52', '+0.16', '+0.00', '+0.00']\n",
      "step 120 total_reward -187.27\n",
      "['-0.76', '+0.74', '-1.69', '+0.44', '+0.68', '+0.17', '+0.00', '+0.00']\n",
      "step 140 total_reward -271.35\n",
      "['-1.01', '+0.79', '-1.69', '+0.04', '+0.81', '+0.17', '+0.00', '+0.00']\n",
      "step 155 total_reward -399.06\n",
      "['+0.00', '+0.94', '+0.09', '-0.00', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.02', '+0.86', '+0.09', '-0.54', '-0.02', '-0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.69\n",
      "['+0.04', '+0.65', '+0.12', '-0.63', '-0.04', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -32.68\n",
      "['+0.07', '+0.52', '+0.20', '-0.26', '-0.08', '-0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward +1.63\n",
      "['+0.12', '+0.50', '+0.33', '+0.06', '-0.14', '-0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -9.32\n",
      "['+0.21', '+0.58', '+0.55', '+0.47', '-0.21', '-0.08', '+0.00', '+0.00']\n",
      "step 100 total_reward -70.98\n",
      "['+0.35', '+0.79', '+0.95', '+0.95', '-0.29', '-0.08', '+0.00', '+0.00']\n",
      "step 120 total_reward -173.47\n",
      "['+0.58', '+1.11', '+1.17', '+0.87', '-0.37', '-0.07', '+0.00', '+0.00']\n",
      "step 140 total_reward -233.21\n",
      "['+0.81', '+1.28', '+1.17', '+0.34', '-0.43', '-0.07', '+0.00', '+0.00']\n",
      "step 160 total_reward -242.79\n",
      "['+1.01', '+1.31', '+1.17', '-0.12', '-0.49', '-0.07', '+0.00', '+0.00']\n",
      "step 177 total_reward -356.52\n",
      "['+0.01', '+0.93', '+0.27', '-0.27', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.65\n",
      "['+0.06', '+0.77', '+0.27', '-0.81', '-0.07', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.97\n",
      "['+0.12', '+0.59', '+0.38', '-0.42', '-0.15', '-0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -7.19\n",
      "['+0.22', '+0.51', '+0.64', '-0.05', '-0.23', '-0.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -23.87\n",
      "['+0.39', '+0.56', '+1.09', '+0.29', '-0.30', '-0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward -98.00\n",
      "['+0.66', '+0.72', '+1.56', '+0.74', '-0.30', '-0.00', '+0.00', '+0.00']\n",
      "step 100 total_reward -192.95\n",
      "['+0.99', '+0.91', '+1.64', '+0.42', '-0.32', '-0.02', '+0.00', '+0.00']\n",
      "step 120 total_reward -229.33\n",
      "['+1.00', '+0.92', '+1.64', '+0.39', '-0.32', '-0.02', '+0.00', '+0.00']\n",
      "step 121 total_reward -329.33\n",
      "['+0.00', '+0.95', '+0.13', '+0.30', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.79\n",
      "['+0.03', '+0.96', '+0.13', '-0.23', '-0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward +4.30\n",
      "['+0.05', '+0.80', '+0.13', '-0.77', '-0.06', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -34.48\n",
      "['+0.09', '+0.60', '+0.22', '-0.45', '-0.08', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward +5.88\n",
      "['+0.14', '+0.51', '+0.30', '-0.03', '-0.14', '-0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward +21.16\n",
      "['+0.23', '+0.56', '+0.57', '+0.32', '-0.19', '-0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -31.89\n",
      "['+0.38', '+0.73', '+0.87', '+0.74', '-0.25', '-0.07', '+0.00', '+0.00']\n",
      "step 120 total_reward -114.38\n",
      "['+0.59', '+1.00', '+1.20', '+1.00', '-0.30', '-0.07', '+0.00', '+0.00']\n",
      "step 140 total_reward -200.45\n",
      "['+0.82', '+1.21', '+1.20', '+0.46', '-0.37', '-0.07', '+0.00', '+0.00']\n",
      "step 160 total_reward -210.60\n",
      "['+1.00', '+1.27', '+1.20', '+0.06', '-0.42', '-0.07', '+0.00', '+0.00']\n",
      "step 175 total_reward -321.23\n",
      "['+0.01', '+0.94', '+0.29', '+0.09', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.42\n",
      "['+0.06', '+0.89', '+0.23', '-0.44', '+0.13', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.93\n",
      "['+0.10', '+0.70', '+0.12', '-0.62', '+0.30', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -39.89\n",
      "['+0.07', '+0.56', '-0.46', '-0.27', '+0.45', '+0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -37.96\n",
      "['-0.09', '+0.54', '-1.14', '+0.07', '+0.61', '+0.20', '+0.00', '+0.00']\n",
      "step 80 total_reward -118.64\n",
      "['-0.41', '+0.59', '-1.96', '+0.22', '+0.81', '+0.23', '+0.00', '+0.00']\n",
      "step 100 total_reward -243.95\n",
      "['-0.86', '+0.62', '-2.32', '-0.10', '+1.04', '+0.23', '+0.00', '+0.00']\n",
      "step 120 total_reward -339.14\n",
      "['-1.02', '+0.60', '-2.32', '-0.29', '+1.12', '+0.23', '+0.00', '+0.00']\n",
      "step 127 total_reward -457.89\n",
      "['-0.01', '+0.96', '-0.75', '+0.48', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.08\n",
      "['-0.16', '+1.02', '-0.75', '-0.05', '+0.19', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.53\n",
      "['-0.31', '+0.92', '-0.75', '-0.59', '+0.35', '+0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.32\n",
      "['-0.49', '+0.71', '-1.15', '-0.61', '+0.51', '+0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -84.45\n",
      "['-0.76', '+0.53', '-1.41', '-0.79', '+0.71', '+0.21', '+0.00', '+0.00']\n",
      "step 80 total_reward -145.14\n",
      "['-1.01', '+0.25', '-1.41', '-1.27', '+0.90', '+0.21', '+0.00', '+0.00']\n",
      "step 98 total_reward -299.75\n",
      "['-0.00', '+0.92', '-0.04', '-0.57', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.74\n",
      "['-0.01', '+0.68', '-0.03', '-0.88', '+0.01', '+0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.28\n",
      "['-0.02', '+0.46', '-0.09', '-0.59', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward +34.65\n",
      "['-0.03', '+0.34', '-0.02', '-0.24', '-0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward +76.95\n",
      "['-0.04', '+0.34', '-0.03', '+0.21', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 80 total_reward +72.85\n",
      "['-0.05', '+0.43', '-0.05', '+0.38', '+0.04', '+0.02', '+0.00', '+0.00']\n",
      "step 100 total_reward +38.78\n",
      "['-0.06', '+0.62', '-0.06', '+0.85', '+0.08', '+0.08', '+0.00', '+0.00']\n",
      "step 120 total_reward -37.59\n",
      "['-0.07', '+0.90', '-0.09', '+0.76', '+0.19', '+0.12', '+0.00', '+0.00']\n",
      "step 140 total_reward -70.75\n",
      "['-0.09', '+1.04', '-0.09', '+0.23', '+0.30', '+0.12', '+0.00', '+0.00']\n",
      "step 160 total_reward -44.74\n",
      "['-0.11', '+1.03', '-0.09', '-0.31', '+0.42', '+0.12', '+0.00', '+0.00']\n",
      "step 180 total_reward -62.49\n",
      "['-0.13', '+0.85', '-0.09', '-0.84', '+0.53', '+0.12', '+0.00', '+0.00']\n",
      "step 200 total_reward -109.28\n",
      "['-0.18', '+0.57', '-0.55', '-0.92', '+0.66', '+0.13', '+0.00', '+0.00']\n",
      "step 220 total_reward -121.34\n",
      "['-0.37', '+0.29', '-1.28', '-0.94', '+0.76', '+0.08', '+0.00', '+0.00']\n",
      "step 240 total_reward -175.31\n",
      "['-0.66', '+0.12', '-1.40', '-0.33', '+2.24', '+3.21', '+0.00', '+0.00']\n",
      "step 260 total_reward -331.70\n",
      "['-0.67', '+0.12', '-1.31', '+0.05', '+2.42', '+3.62', '+0.00', '+0.00']\n",
      "step 261 total_reward -431.70\n",
      "['-0.01', '+0.94', '-0.55', '-0.07', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.69\n",
      "['-0.12', '+0.84', '-0.55', '-0.60', '+0.14', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.46\n",
      "['-0.24', '+0.64', '-0.69', '-0.52', '+0.28', '+0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.46\n",
      "['-0.42', '+0.53', '-1.16', '-0.17', '+0.44', '+0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -89.82\n",
      "['-0.69', '+0.48', '-1.37', '-0.39', '+0.67', '+0.23', '+0.00', '+0.00']\n",
      "step 80 total_reward -156.06\n",
      "['-0.96', '+0.28', '-1.37', '-0.92', '+0.90', '+0.23', '+0.00', '+0.00']\n",
      "step 100 total_reward -218.32\n",
      "['-1.00', '+0.24', '-1.37', '-1.00', '+0.94', '+0.23', '+0.00', '+0.00']\n",
      "step 103 total_reward -325.56\n",
      "['-0.00', '+0.95', '-0.18', '+0.16', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.39\n",
      "['-0.04', '+0.91', '-0.18', '-0.37', '+0.04', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.08\n",
      "['-0.07', '+0.72', '-0.19', '-0.83', '+0.08', '+0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.07\n",
      "['-0.12', '+0.53', '-0.34', '-0.39', '+0.12', '+0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -5.30\n",
      "['-0.21', '+0.47', '-0.51', '+0.04', '+0.19', '+0.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -14.30\n",
      "['-0.34', '+0.55', '-0.82', '+0.43', '+0.30', '+0.12', '+0.00', '+0.00']\n",
      "step 100 total_reward -85.05\n",
      "['-0.55', '+0.72', '-1.33', '+0.70', '+0.37', '+0.04', '+0.00', '+0.00']\n",
      "step 120 total_reward -183.00\n",
      "['-0.85', '+0.93', '-1.50', '+0.47', '+0.42', '+0.05', '+0.00', '+0.00']\n",
      "step 140 total_reward -231.36\n",
      "['-1.01', '+0.98', '-1.50', '+0.18', '+0.45', '+0.05', '+0.00', '+0.00']\n",
      "step 151 total_reward -341.83\n",
      "['-0.00', '+0.95', '-0.02', '+0.34', '+0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.04\n",
      "['-0.00', '+0.97', '-0.02', '-0.19', '+0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 20 total_reward +14.51\n",
      "['-0.01', '+0.83', '-0.02', '-0.73', '+0.01', '+0.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -25.07\n",
      "['-0.01', '+0.58', '-0.01', '-0.70', '+0.01', '+0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -2.09\n",
      "['-0.02', '+0.43', '-0.05', '-0.31', '+0.02', '+0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward +43.56\n",
      "['-0.03', '+0.39', '-0.08', '-0.01', '+0.05', '+0.03', '+0.00', '+0.00']\n",
      "step 100 total_reward +64.17\n",
      "['-0.05', '+0.43', '-0.20', '+0.24', '+0.08', '-0.02', '+0.00', '+0.00']\n",
      "step 120 total_reward +28.36\n",
      "['-0.10', '+0.57', '-0.31', '+0.63', '+0.07', '-0.01', '+0.00', '+0.00']\n",
      "step 140 total_reward -29.82\n",
      "['-0.18', '+0.81', '-0.40', '+0.90', '+0.04', '-0.03', '+0.00', '+0.00']\n",
      "step 160 total_reward -84.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.26', '+0.99', '-0.40', '+0.36', '+0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 180 total_reward -57.22\n",
      "['-0.34', '+1.02', '-0.40', '-0.17', '-0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 200 total_reward -53.85\n",
      "['-0.42', '+0.88', '-0.40', '-0.70', '-0.06', '-0.03', '+0.00', '+0.00']\n",
      "step 220 total_reward -85.13\n",
      "['-0.48', '+0.67', '-0.28', '-0.57', '-0.07', '+0.01', '+0.00', '+0.00']\n",
      "step 240 total_reward -57.89\n",
      "['-0.53', '+0.54', '-0.21', '-0.42', '-0.05', '+0.02', '+0.00', '+0.00']\n",
      "step 260 total_reward -38.20\n",
      "['-0.57', '+0.33', '-0.22', '-0.95', '-0.00', '+0.07', '+0.00', '+0.00']\n",
      "step 280 total_reward -74.46\n",
      "['-0.61', '+0.03', '-0.08', '-0.76', '+0.01', '-4.45', '+1.00', '+1.00']\n",
      "step 297 total_reward -202.94\n",
      "['-0.01', '+0.93', '-0.47', '-0.52', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.58\n",
      "['-0.10', '+0.69', '-0.48', '-0.90', '+0.12', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.38\n",
      "['-0.22', '+0.48', '-0.75', '-0.54', '+0.22', '+0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -12.03\n",
      "['-0.41', '+0.38', '-1.15', '-0.19', '+0.30', '+0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -53.05\n",
      "['-0.64', '+0.24', '-1.15', '-0.73', '+0.34', '+0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -88.54\n",
      "['-0.83', '+0.00', '-0.96', '-0.43', '+0.13', '-3.96', '+1.00', '+1.00']\n",
      "step 97 total_reward -196.78\n",
      "['+0.01', '+0.93', '+0.40', '-0.25', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.32\n",
      "['+0.09', '+0.78', '+0.40', '-0.78', '-0.10', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.57\n",
      "['+0.18', '+0.59', '+0.63', '-0.42', '-0.20', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -22.02\n",
      "['+0.34', '+0.52', '+0.96', '-0.07', '-0.27', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -56.93\n",
      "['+0.58', '+0.58', '+1.39', '+0.34', '-0.34', '-0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -135.82\n",
      "['+0.90', '+0.71', '+1.72', '+0.40', '-0.41', '-0.07', '+0.00', '+0.00']\n",
      "step 100 total_reward -214.43\n",
      "['+1.00', '+0.74', '+1.72', '+0.24', '-0.43', '-0.07', '+0.00', '+0.00']\n",
      "step 106 total_reward -322.01\n",
      "['-0.01', '+0.93', '-0.63', '-0.48', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.45\n",
      "['-0.14', '+0.70', '-0.68', '-0.85', '+0.15', '+0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.25\n",
      "['-0.30', '+0.52', '-0.95', '-0.38', '+0.30', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -26.97\n",
      "['-0.53', '+0.43', '-1.24', '-0.47', '+0.53', '+0.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -92.66\n",
      "['-0.77', '+0.20', '-1.24', '-1.00', '+0.90', '+0.37', '+0.00', '+0.00']\n",
      "step 80 total_reward -168.58\n",
      "['-0.96', '-0.03', '-1.27', '+0.10', '+1.83', '+1.51', '+0.00', '+1.00']\n",
      "step 95 total_reward -323.94\n",
      "['-0.02', '+0.92', '-0.77', '-0.58', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['-0.17', '+0.68', '-0.85', '-0.82', '+0.19', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.39\n",
      "['-0.38', '+0.50', '-1.21', '-0.45', '+0.33', '+0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.36\n",
      "['-0.65', '+0.34', '-1.42', '-0.73', '+0.48', '+0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -101.94\n",
      "['-0.93', '+0.04', '-1.42', '-1.26', '+0.65', '+0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -168.38\n",
      "['-1.01', '-0.05', '-1.43', '-1.32', '+0.61', '-0.44', '+0.00', '+0.00']\n",
      "step 85 total_reward -275.87\n",
      "['+0.01', '+0.93', '+0.72', '-0.46', '-0.02', '-0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.40\n",
      "['+0.16', '+0.71', '+0.78', '-0.78', '-0.17', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -23.38\n",
      "['+0.34', '+0.54', '+1.10', '-0.45', '-0.37', '-0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.12\n",
      "['+0.61', '+0.44', '+1.70', '-0.20', '-0.56', '-0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -137.31\n",
      "['+1.00', '+0.37', '+2.02', '-0.44', '-0.72', '-0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -319.23\n",
      "['-0.00', '+0.93', '-0.07', '-0.53', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.82\n",
      "['-0.02', '+0.69', '-0.06', '-0.89', '+0.02', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.74\n",
      "['-0.04', '+0.49', '-0.08', '-0.46', '+0.03', '+0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward +38.95\n",
      "['-0.06', '+0.41', '-0.11', '-0.11', '+0.06', '+0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward +68.69\n",
      "['-0.09', '+0.40', '-0.26', '+0.03', '+0.14', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward +45.06\n",
      "['-0.15', '+0.46', '-0.46', '+0.41', '+0.20', '+0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward -10.29\n",
      "['-0.28', '+0.65', '-0.78', '+0.80', '+0.25', '+0.07', '+0.00', '+0.00']\n",
      "step 120 total_reward -93.07\n",
      "['-0.46', '+0.92', '-1.00', '+0.90', '+0.37', '+0.13', '+0.00', '+0.00']\n",
      "step 140 total_reward -164.52\n",
      "['-0.66', '+1.11', '-1.00', '+0.37', '+0.50', '+0.13', '+0.00', '+0.00']\n",
      "step 160 total_reward -175.22\n",
      "['-0.86', '+1.13', '-1.00', '-0.17', '+0.63', '+0.13', '+0.00', '+0.00']\n",
      "step 180 total_reward -196.24\n",
      "['-1.00', '+1.05', '-1.00', '-0.57', '+0.73', '+0.13', '+0.00', '+0.00']\n",
      "step 195 total_reward -320.67\n",
      "['+0.01', '+0.95', '+0.42', '+0.41', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.79\n",
      "['+0.09', '+0.99', '+0.42', '-0.12', '-0.10', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward +2.07\n",
      "['+0.17', '+0.87', '+0.42', '-0.66', '-0.20', '-0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -30.82\n",
      "['+0.27', '+0.62', '+0.66', '-0.77', '-0.27', '-0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -43.88\n",
      "['+0.44', '+0.44', '+1.06', '-0.40', '-0.27', '-0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward -56.53\n",
      "['+0.70', '+0.39', '+1.48', '+0.04', '-0.28', '+0.00', '+0.00', '+0.00']\n",
      "step 100 total_reward -115.64\n",
      "['+1.01', '+0.36', '+1.55', '-0.32', '-0.28', '-0.01', '+0.00', '+0.00']\n",
      "step 120 total_reward -252.02\n",
      "['+0.01', '+0.93', '+0.27', '-0.45', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.79\n",
      "['+0.06', '+0.71', '+0.33', '-0.84', '-0.06', '-0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.91\n",
      "['+0.13', '+0.52', '+0.47', '-0.44', '-0.09', '+0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward +9.88\n",
      "['+0.24', '+0.45', '+0.59', '-0.03', '-0.07', '+0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward +13.64\n",
      "['+0.37', '+0.49', '+0.68', '+0.32', '-0.03', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -14.85\n",
      "['+0.52', '+0.65', '+0.78', '+0.72', '+0.04', '+0.13', '+0.00', '+0.00']\n",
      "step 100 total_reward -74.91\n",
      "['+0.67', '+0.93', '+0.69', '+0.97', '+0.19', '+0.15', '+0.00', '+0.00']\n",
      "step 120 total_reward -137.63\n",
      "['+0.81', '+1.13', '+0.69', '+0.43', '+0.33', '+0.15', '+0.00', '+0.00']\n",
      "step 140 total_reward -139.99\n",
      "['+0.95', '+1.18', '+0.69', '-0.10', '+0.48', '+0.15', '+0.00', '+0.00']\n",
      "step 160 total_reward -155.16\n",
      "['+1.00', '+1.15', '+0.69', '-0.31', '+0.54', '+0.15', '+0.00', '+0.00']\n",
      "step 168 total_reward -266.85\n",
      "['+0.01', '+0.93', '+0.32', '-0.35', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.67\n",
      "['+0.07', '+0.74', '+0.31', '-0.83', '-0.08', '-0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.41\n",
      "['+0.15', '+0.57', '+0.48', '-0.36', '-0.16', '-0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -1.87\n",
      "['+0.27', '+0.53', '+0.77', '+0.04', '-0.24', '-0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -34.72\n",
      "['+0.46', '+0.62', '+1.11', '+0.44', '-0.36', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -111.60\n",
      "['+0.73', '+0.78', '+1.59', '+0.59', '-0.50', '-0.13', '+0.00', '+0.00']\n",
      "step 100 total_reward -211.01\n",
      "['+1.01', '+0.87', '+1.63', '+0.15', '-0.60', '-0.12', '+0.00', '+0.00']\n",
      "step 117 total_reward -339.70\n",
      "['-0.00', '+0.94', '-0.15', '-0.08', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.10\n",
      "['-0.03', '+0.83', '-0.15', '-0.62', '+0.04', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.83\n",
      "['-0.07', '+0.63', '-0.22', '-0.53', '+0.06', '+0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -20.34\n",
      "['-0.12', '+0.52', '-0.32', '-0.14', '+0.08', '+0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward +4.03\n",
      "['-0.20', '+0.55', '-0.45', '+0.34', '+0.08', '+0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward -28.67\n",
      "['-0.30', '+0.71', '-0.59', '+0.70', '+0.13', '+0.02', '+0.00', '+0.00']\n",
      "step 100 total_reward -92.70\n",
      "['-0.43', '+0.94', '-0.71', '+0.63', '+0.12', '-0.01', '+0.00', '+0.00']\n",
      "step 120 total_reward -124.61\n",
      "['-0.57', '+1.04', '-0.71', '+0.09', '+0.11', '-0.01', '+0.00', '+0.00']\n",
      "step 140 total_reward -115.90\n",
      "['-0.72', '+0.99', '-0.71', '-0.44', '+0.10', '-0.01', '+0.00', '+0.00']\n",
      "step 160 total_reward -129.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.86', '+0.77', '-0.71', '-0.97', '+0.08', '-0.01', '+0.00', '+0.00']\n",
      "step 180 total_reward -158.79\n",
      "['-1.00', '+0.40', '-0.71', '-1.51', '+0.07', '-0.01', '+0.00', '+0.00']\n",
      "step 200 total_reward -195.85\n",
      "['-1.01', '+0.37', '-0.71', '-1.53', '+0.07', '-0.01', '+0.00', '+0.00']\n",
      "step 201 total_reward -295.85\n",
      "['+0.02', '+0.93', '+0.80', '-0.33', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.26\n",
      "['+0.18', '+0.75', '+0.82', '-0.83', '-0.20', '-0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.86\n",
      "['+0.37', '+0.55', '+1.15', '-0.53', '-0.39', '-0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -57.64\n",
      "['+0.65', '+0.43', '+1.79', '-0.29', '-0.61', '-0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -151.78\n",
      "['+1.01', '+0.30', '+1.90', '-0.69', '-0.81', '-0.21', '+0.00', '+0.00']\n",
      "step 79 total_reward -317.53\n",
      "['+0.01', '+0.93', '+0.36', '-0.41', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.66\n",
      "['+0.08', '+0.72', '+0.38', '-0.82', '-0.09', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.48\n",
      "['+0.18', '+0.55', '+0.60', '-0.39', '-0.13', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -2.47\n",
      "['+0.31', '+0.47', '+0.70', '-0.16', '-0.18', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -13.11\n",
      "['+0.48', '+0.50', '+1.01', '+0.23', '-0.28', '-0.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -72.52\n",
      "['+0.73', '+0.62', '+1.38', '+0.46', '-0.36', '-0.09', '+0.00', '+0.00']\n",
      "step 100 total_reward -153.45\n",
      "['+1.00', '+0.67', '+1.38', '-0.08', '-0.45', '-0.09', '+0.00', '+0.00']\n",
      "step 120 total_reward -279.08\n",
      "['-0.00', '+0.93', '-0.11', '-0.29', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.07\n",
      "['-0.02', '+0.76', '-0.11', '-0.83', '+0.03', '+0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.46\n",
      "['-0.05', '+0.58', '-0.16', '-0.41', '+0.06', '+0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward +7.59\n",
      "['-0.10', '+0.53', '-0.32', '+0.01', '+0.07', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward +18.03\n",
      "['-0.16', '+0.60', '-0.34', '+0.39', '+0.09', '+0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -17.85\n",
      "['-0.24', '+0.78', '-0.47', '+0.74', '+0.14', '+0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward -84.47\n",
      "['-0.33', '+0.96', '-0.46', '+0.36', '+0.23', '+0.09', '+0.00', '+0.00']\n",
      "step 120 total_reward -85.24\n",
      "['-0.42', '+0.98', '-0.46', '-0.17', '+0.32', '+0.09', '+0.00', '+0.00']\n",
      "step 140 total_reward -91.19\n",
      "['-0.52', '+0.85', '-0.46', '-0.71', '+0.42', '+0.09', '+0.00', '+0.00']\n",
      "step 160 total_reward -127.87\n",
      "['-0.65', '+0.64', '-0.88', '-0.67', '+0.51', '+0.10', '+0.00', '+0.00']\n",
      "step 180 total_reward -160.09\n",
      "['-0.83', '+0.36', '-0.88', '-1.21', '+0.61', '+0.10', '+0.00', '+0.00']\n",
      "step 200 total_reward -206.82\n",
      "['-1.00', '-0.09', '-0.88', '-1.74', '+0.70', '+0.10', '+0.00', '+0.00']\n",
      "step 220 total_reward -368.70\n",
      "['-0.01', '+0.94', '-0.30', '-0.05', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.56\n",
      "['-0.07', '+0.84', '-0.30', '-0.59', '+0.08', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.84\n",
      "['-0.13', '+0.63', '-0.38', '-0.54', '+0.15', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.74\n",
      "['-0.23', '+0.54', '-0.69', '-0.10', '+0.27', '+0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -38.88\n",
      "['-0.41', '+0.57', '-1.14', '+0.29', '+0.36', '+0.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -114.09\n",
      "['-0.68', '+0.68', '-1.42', '+0.25', '+0.47', '+0.11', '+0.00', '+0.00']\n",
      "step 100 total_reward -180.60\n",
      "['-0.96', '+0.67', '-1.42', '-0.29', '+0.57', '+0.11', '+0.00', '+0.00']\n",
      "step 120 total_reward -213.28\n",
      "['-1.00', '+0.66', '-1.42', '-0.37', '+0.59', '+0.11', '+0.00', '+0.00']\n",
      "step 123 total_reward -317.30\n",
      "['+0.01', '+0.94', '+0.69', '-0.07', '-0.02', '-0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.79\n",
      "['+0.15', '+0.83', '+0.69', '-0.60', '-0.17', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.39\n",
      "['+0.30', '+0.63', '+0.84', '-0.59', '-0.35', '-0.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.17\n",
      "['+0.53', '+0.50', '+1.47', '-0.33', '-0.54', '-0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -123.54\n",
      "['+0.90', '+0.43', '+1.97', '-0.37', '-0.71', '-0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -220.00\n",
      "['+1.01', '+0.38', '+1.97', '-0.53', '-0.76', '-0.17', '+0.00', '+0.00']\n",
      "step 86 total_reward -334.77\n",
      "['-0.00', '+0.95', '-0.23', '+0.17', '+0.01', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.09\n",
      "['-0.05', '+0.91', '-0.23', '-0.37', '+0.06', '+0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.83\n",
      "['-0.10', '+0.72', '-0.25', '-0.80', '+0.11', '+0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.53\n",
      "['-0.17', '+0.54', '-0.45', '-0.42', '+0.11', '+0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -10.65\n",
      "['-0.28', '+0.49', '-0.60', '+0.05', '+0.14', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -18.74\n",
      "['-0.42', '+0.58', '-0.81', '+0.51', '+0.16', '+0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -77.34\n",
      "['-0.62', '+0.81', '-1.11', '+0.90', '+0.17', '-0.02', '+0.00', '+0.00']\n",
      "step 120 total_reward -161.24\n",
      "['-0.84', '+1.00', '-1.11', '+0.36', '+0.15', '-0.02', '+0.00', '+0.00']\n",
      "step 140 total_reward -161.97\n",
      "['-1.01', '+1.03', '-1.11', '-0.04', '+0.14', '-0.02', '+0.00', '+0.00']\n",
      "step 155 total_reward -267.96\n",
      "['+0.00', '+0.95', '+0.02', '+0.24', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.17\n",
      "['+0.01', '+0.94', '+0.02', '-0.30', '-0.01', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.81\n",
      "['+0.01', '+0.76', '+0.02', '-0.83', '-0.01', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -39.43\n",
      "['+0.01', '+0.57', '-0.01', '-0.44', '-0.05', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward +9.76\n",
      "['+0.02', '+0.50', '+0.13', '-0.08', '-0.08', '-0.02', '+0.00', '+0.00']\n",
      "step 80 total_reward +36.04\n",
      "['+0.07', '+0.54', '+0.29', '+0.33', '-0.04', '+0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward +1.06\n",
      "['+0.14', '+0.69', '+0.37', '+0.66', '+0.03', '+0.10', '+0.00', '+0.00']\n",
      "step 120 total_reward -51.33\n",
      "['+0.22', '+0.89', '+0.41', '+0.48', '+0.19', '+0.17', '+0.00', '+0.00']\n",
      "step 140 total_reward -79.31\n",
      "['+0.31', '+0.95', '+0.42', '-0.05', '+0.36', '+0.17', '+0.00', '+0.00']\n",
      "step 160 total_reward -82.55\n",
      "['+0.39', '+0.87', '+0.22', '-0.35', '+0.53', '+0.17', '+0.00', '+0.00']\n",
      "step 180 total_reward -95.48\n",
      "['+0.36', '+0.79', '-0.57', '-0.14', '+0.67', '+0.15', '+0.00', '+0.00']\n",
      "step 200 total_reward -124.35\n",
      "['+0.15', '+0.77', '-1.38', '-0.06', '+0.84', '+0.17', '+0.00', '+0.00']\n",
      "step 220 total_reward -218.14\n",
      "['-0.21', '+0.73', '-2.34', '-0.15', '+0.99', '+0.13', '+0.00', '+0.00']\n",
      "step 240 total_reward -331.65\n",
      "['-0.79', '+0.68', '-3.26', '-0.28', '+1.15', '+0.18', '+0.00', '+0.00']\n",
      "step 260 total_reward -473.70\n",
      "['-1.02', '+0.64', '-3.26', '-0.47', '+1.21', '+0.18', '+0.00', '+0.00']\n",
      "step 267 total_reward -594.42\n",
      "['+0.01', '+0.93', '+0.68', '-0.37', '-0.02', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.32\n",
      "['+0.15', '+0.74', '+0.66', '-0.81', '-0.14', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -23.95\n",
      "['+0.30', '+0.55', '+0.96', '-0.41', '-0.31', '-0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -33.85\n",
      "['+0.54', '+0.50', '+1.41', '-0.06', '-0.52', '-0.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -108.55\n",
      "['+0.90', '+0.52', '+2.05', '+0.01', '-0.75', '-0.23', '+0.00', '+0.00']\n",
      "step 80 total_reward -230.41\n",
      "['+1.01', '+0.52', '+2.05', '-0.12', '-0.81', '-0.23', '+0.00', '+0.00']\n",
      "step 85 total_reward -342.16\n",
      "['-0.01', '+0.95', '-0.65', '+0.29', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.01\n",
      "['-0.14', '+0.95', '-0.65', '-0.24', '+0.16', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.22\n",
      "['-0.27', '+0.80', '-0.65', '-0.78', '+0.31', '+0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.44\n",
      "['-0.45', '+0.61', '-1.12', '-0.52', '+0.45', '+0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -82.73\n",
      "['-0.73', '+0.49', '-1.46', '-0.58', '+0.59', '+0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -144.97\n",
      "['-1.00', '+0.25', '-1.46', '-1.09', '+0.74', '+0.16', '+0.00', '+0.00']\n",
      "step 99 total_reward -297.24\n",
      "['-0.01', '+0.94', '-0.47', '+0.05', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.19\n",
      "['-0.10', '+0.87', '-0.47', '-0.48', '+0.11', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.53\n",
      "['-0.20', '+0.68', '-0.60', '-0.56', '+0.22', '+0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.47\n",
      "['-0.35', '+0.57', '-0.95', '-0.18', '+0.33', '+0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -63.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.59', '+0.58', '-1.35', '+0.03', '+0.47', '+0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -135.87\n",
      "['-0.86', '+0.50', '-1.35', '-0.50', '+0.63', '+0.16', '+0.00', '+0.00']\n",
      "step 100 total_reward -178.15\n",
      "['-1.01', '+0.40', '-1.35', '-0.79', '+0.72', '+0.16', '+0.00', '+0.00']\n",
      "step 111 total_reward -305.39\n",
      "['+0.01', '+0.93', '+0.65', '-0.29', '-0.01', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.22\n",
      "['+0.14', '+0.76', '+0.65', '-0.82', '-0.16', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.70\n",
      "['+0.30', '+0.57', '+0.96', '-0.50', '-0.32', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.89\n",
      "['+0.54', '+0.48', '+1.50', '-0.13', '-0.47', '-0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -117.25\n",
      "['+0.90', '+0.46', '+1.90', '-0.18', '-0.63', '-0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -206.50\n",
      "['+1.01', '+0.44', '+1.90', '-0.34', '-0.68', '-0.17', '+0.00', '+0.00']\n",
      "step 86 total_reward -320.10\n",
      "['+0.01', '+0.95', '+0.46', '+0.23', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.38\n",
      "['+0.10', '+0.93', '+0.46', '-0.30', '-0.11', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.56\n",
      "['+0.19', '+0.76', '+0.46', '-0.84', '-0.22', '-0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.58\n",
      "['+0.33', '+0.55', '+0.86', '-0.49', '-0.28', '-0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -49.01\n",
      "['+0.54', '+0.47', '+1.30', '-0.08', '-0.32', '-0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -97.81\n",
      "['+0.85', '+0.47', '+1.64', '-0.08', '-0.32', '+0.02', '+0.00', '+0.00']\n",
      "step 100 total_reward -161.09\n",
      "['+1.02', '+0.44', '+1.64', '-0.35', '-0.31', '+0.02', '+0.00', '+0.00']\n",
      "step 110 total_reward -274.91\n",
      "['+0.01', '+0.93', '+0.68', '-0.43', '-0.02', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.38\n",
      "['+0.15', '+0.72', '+0.70', '-0.85', '-0.17', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.53\n",
      "['+0.32', '+0.52', '+1.04', '-0.50', '-0.35', '-0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.72\n",
      "['+0.58', '+0.42', '+1.67', '-0.13', '-0.53', '-0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -131.77\n",
      "['+0.96', '+0.39', '+1.93', '-0.33', '-0.75', '-0.22', '+0.00', '+0.00']\n",
      "step 80 total_reward -215.16\n",
      "['+1.01', '+0.37', '+1.93', '-0.41', '-0.78', '-0.22', '+0.00', '+0.00']\n",
      "step 83 total_reward -321.51\n",
      "['-0.02', '+0.94', '-0.78', '+0.02', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.71\n",
      "['-0.17', '+0.86', '-0.79', '-0.51', '+0.19', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.48\n",
      "['-0.35', '+0.68', '-1.07', '-0.51', '+0.35', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -59.99\n",
      "['-0.61', '+0.60', '-1.58', '-0.19', '+0.54', '+0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -134.21\n",
      "['-0.93', '+0.46', '-1.58', '-0.72', '+0.76', '+0.21', '+0.00', '+0.00']\n",
      "step 80 total_reward -187.82\n",
      "['-1.00', '+0.40', '-1.58', '-0.85', '+0.81', '+0.21', '+0.00', '+0.00']\n",
      "step 85 total_reward -300.54\n",
      "['-0.01', '+0.95', '-0.38', '+0.18', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.48\n",
      "['-0.08', '+0.92', '-0.38', '-0.36', '+0.09', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.50\n",
      "['-0.16', '+0.73', '-0.43', '-0.78', '+0.18', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.87\n",
      "['-0.28', '+0.56', '-0.78', '-0.39', '+0.22', '+0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -39.77\n",
      "['-0.47', '+0.48', '-1.11', '-0.33', '+0.23', '+0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -78.70\n",
      "['-0.70', '+0.29', '-1.12', '-0.87', '+0.46', '+0.24', '+0.00', '+0.00']\n",
      "step 100 total_reward -136.98\n",
      "['-0.87', '+0.06', '-1.51', '-0.69', '+0.59', '-0.14', '+1.00', '+1.00']\n",
      "step 115 total_reward -255.43\n",
      "['-0.01', '+0.93', '-0.47', '-0.51', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.59\n",
      "['-0.10', '+0.70', '-0.47', '-0.85', '+0.12', '+0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.72\n",
      "['-0.22', '+0.50', '-0.69', '-0.52', '+0.22', '+0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -8.67\n",
      "['-0.39', '+0.38', '-1.03', '-0.25', '+0.36', '+0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -47.54\n",
      "['-0.60', '+0.24', '-1.10', '-0.65', '+0.50', '+0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -95.57\n",
      "['-0.82', '-0.04', '-1.12', '-1.16', '+0.62', '-0.23', '+0.00', '+1.00']\n",
      "step 100 total_reward -148.64\n",
      "['-0.86', '-0.08', '-1.49', '-0.76', '+0.62', '+1.36', '+0.00', '+1.00']\n",
      "step 103 total_reward -237.42\n",
      "['-0.01', '+0.95', '-0.47', '+0.32', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.52\n",
      "['-0.10', '+0.96', '-0.47', '-0.21', '+0.12', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -6.63\n",
      "['-0.20', '+0.81', '-0.47', '-0.75', '+0.22', '+0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.74\n",
      "['-0.32', '+0.63', '-0.80', '-0.47', '+0.34', '+0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -49.25\n",
      "['-0.53', '+0.55', '-1.31', '-0.13', '+0.49', '+0.18', '+0.00', '+0.00']\n",
      "step 80 total_reward -114.29\n",
      "['-0.79', '+0.42', '-1.32', '-0.66', '+0.71', '+0.23', '+0.00', '+0.00']\n",
      "step 100 total_reward -167.00\n",
      "['-1.00', '+0.21', '-1.32', '-1.09', '+0.90', '+0.23', '+0.00', '+0.00']\n",
      "step 116 total_reward -317.67\n",
      "['-0.01', '+0.94', '-0.30', '-0.07', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.65\n",
      "['-0.06', '+0.84', '-0.30', '-0.60', '+0.07', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.65\n",
      "['-0.13', '+0.62', '-0.40', '-0.58', '+0.14', '+0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -26.67\n",
      "['-0.23', '+0.49', '-0.63', '-0.25', '+0.19', '+0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -25.57\n",
      "['-0.38', '+0.47', '-0.87', '-0.06', '+0.29', '+0.24', '+0.00', '+0.00']\n",
      "step 80 total_reward -66.80\n",
      "['-0.57', '+0.37', '-0.96', '-0.62', '+0.84', '+0.63', '+0.00', '+0.00']\n",
      "step 100 total_reward -155.02\n",
      "['-0.76', '+0.14', '-1.28', '-0.41', '+1.75', '+2.62', '+0.00', '+0.00']\n",
      "step 120 total_reward -277.78\n",
      "['-0.86', '+0.09', '-1.20', '-0.30', '+2.68', '+4.23', '+0.00', '+0.00']\n",
      "step 127 total_reward -467.92\n",
      "['-0.02', '+0.95', '-0.79', '+0.18', '+0.02', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.49\n",
      "['-0.17', '+0.92', '-0.77', '-0.36', '+0.09', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -9.44\n",
      "['-0.33', '+0.74', '-0.82', '-0.61', '+0.16', '+0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.70\n",
      "['-0.52', '+0.63', '-1.11', '-0.17', '+0.25', '+0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -49.85\n",
      "['-0.76', '+0.56', '-1.21', '-0.46', '+0.35', '+0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward -90.04\n",
      "['-1.00', '+0.34', '-1.21', '-0.99', '+0.45', '+0.10', '+0.00', '+0.00']\n",
      "step 100 total_reward -138.16\n",
      "['-1.01', '+0.32', '-1.21', '-1.02', '+0.45', '+0.10', '+0.00', '+0.00']\n",
      "step 101 total_reward -238.16\n",
      "['-0.00', '+0.94', '-0.20', '-0.06', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.67\n",
      "['-0.04', '+0.84', '-0.20', '-0.59', '+0.05', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.75\n",
      "['-0.09', '+0.63', '-0.25', '-0.60', '+0.09', '+0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -26.45\n",
      "['-0.14', '+0.51', '-0.40', '-0.18', '+0.19', '+0.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -10.36\n",
      "['-0.25', '+0.53', '-0.74', '+0.31', '+0.31', '+0.14', '+0.00', '+0.00']\n",
      "step 80 total_reward -70.90\n",
      "['-0.45', '+0.69', '-1.27', '+0.68', '+0.46', '+0.15', '+0.00', '+0.00']\n",
      "step 100 total_reward -178.89\n",
      "['-0.76', '+0.92', '-1.67', '+0.68', '+0.62', '+0.18', '+0.00', '+0.00']\n",
      "step 120 total_reward -272.40\n",
      "['-1.01', '+1.03', '-1.67', '+0.28', '+0.76', '+0.18', '+0.00', '+0.00']\n",
      "step 135 total_reward -397.34\n",
      "['-0.01', '+0.93', '-0.29', '-0.52', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.75\n",
      "['-0.06', '+0.69', '-0.32', '-0.82', '+0.07', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.05\n",
      "['-0.14', '+0.49', '-0.41', '-0.57', '+0.15', '+0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward +7.62\n",
      "['-0.24', '+0.38', '-0.64', '-0.24', '+0.24', '+0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward +0.96\n",
      "['-0.41', '+0.36', '-1.04', '-0.01', '+0.34', '+0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward -60.21\n",
      "['-0.62', '+0.27', '-1.04', '-0.54', '+0.39', '+0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -91.64\n",
      "['-0.83', '+0.03', '-1.09', '-1.03', '+0.43', '+0.03', '+0.00', '+0.00']\n",
      "step 120 total_reward -144.04\n",
      "['-0.88', '-0.02', '-0.93', '-0.45', '+0.28', '-4.00', '+0.00', '+1.00']\n",
      "step 124 total_reward -227.70\n",
      "['+0.00', '+0.94', '+0.01', '+0.04', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.00', '+0.87', '+0.01', '-0.49', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.76\n",
      "['+0.01', '+0.66', '+0.02', '-0.61', '-0.01', '+0.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -29.45\n",
      "['+0.00', '+0.56', '-0.02', '-0.18', '-0.04', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward +15.06\n",
      "['+0.01', '+0.55', '+0.09', '+0.20', '-0.07', '-0.02', '+0.00', '+0.00']\n",
      "step 80 total_reward +2.60\n",
      "['+0.04', '+0.65', '+0.13', '+0.52', '-0.11', '-0.09', '+0.00', '+0.00']\n",
      "step 100 total_reward -48.86\n",
      "['+0.08', '+0.86', '+0.28', '+0.58', '-0.16', '-0.04', '+0.00', '+0.00']\n",
      "step 120 total_reward -89.91\n",
      "['+0.14', '+0.95', '+0.28', '+0.05', '-0.20', '-0.04', '+0.00', '+0.00']\n",
      "step 140 total_reward -67.85\n",
      "['+0.20', '+0.88', '+0.28', '-0.48', '-0.25', '-0.04', '+0.00', '+0.00']\n",
      "step 160 total_reward -93.85\n",
      "['+0.26', '+0.68', '+0.46', '-0.63', '-0.29', '-0.04', '+0.00', '+0.00']\n",
      "step 180 total_reward -104.76\n",
      "['+0.40', '+0.55', '+0.91', '-0.26', '-0.29', '+0.01', '+0.00', '+0.00']\n",
      "step 200 total_reward -122.94\n",
      "['+0.62', '+0.52', '+1.28', '-0.01', '-0.26', '+0.04', '+0.00', '+0.00']\n",
      "step 220 total_reward -172.51\n",
      "['+0.91', '+0.54', '+1.47', '-0.09', '-0.22', '+0.05', '+0.00', '+0.00']\n",
      "step 240 total_reward -215.41\n",
      "['+1.01', '+0.51', '+1.47', '-0.27', '-0.21', '+0.05', '+0.00', '+0.00']\n",
      "step 247 total_reward -322.67\n",
      "['-0.00', '+0.94', '-0.16', '-0.13', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.49\n",
      "['-0.03', '+0.82', '-0.16', '-0.66', '+0.04', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.60\n",
      "['-0.07', '+0.61', '-0.17', '-0.50', '+0.08', '+0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -13.33\n",
      "['-0.11', '+0.52', '-0.31', '-0.05', '+0.17', '+0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward +0.33\n",
      "['-0.21', '+0.58', '-0.60', '+0.37', '+0.27', '+0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -62.68\n",
      "['-0.38', '+0.77', '-1.08', '+0.81', '+0.39', '+0.14', '+0.00', '+0.00']\n",
      "step 100 total_reward -169.80\n",
      "['-0.63', '+1.01', '-1.30', '+0.59', '+0.53', '+0.13', '+0.00', '+0.00']\n",
      "step 120 total_reward -226.15\n",
      "['-0.89', '+1.10', '-1.30', '+0.05', '+0.65', '+0.13', '+0.00', '+0.00']\n",
      "step 140 total_reward -248.79\n",
      "['-1.01', '+1.09', '-1.30', '-0.19', '+0.71', '+0.13', '+0.00', '+0.00']\n",
      "step 149 total_reward -360.83\n",
      "['+0.01', '+0.93', '+0.47', '-0.53', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.59\n",
      "['+0.10', '+0.70', '+0.53', '-0.79', '-0.11', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.21\n",
      "['+0.22', '+0.53', '+0.68', '-0.40', '-0.26', '-0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -6.98\n",
      "['+0.39', '+0.47', '+1.07', '+0.00', '-0.47', '-0.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -65.77\n",
      "['+0.69', '+0.52', '+1.91', '+0.27', '-0.71', '-0.24', '+0.00', '+0.00']\n",
      "step 80 total_reward -207.28\n",
      "['+1.02', '+0.56', '+2.09', '-0.02', '-0.91', '-0.25', '+0.00', '+0.00']\n",
      "step 96 total_reward -371.46\n",
      "['-0.00', '+0.93', '-0.03', '-0.33', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.10\n",
      "['-0.01', '+0.75', '-0.03', '-0.86', '+0.01', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.92\n",
      "['-0.02', '+0.55', '-0.08', '-0.47', '+0.01', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward +14.61\n",
      "['-0.03', '+0.48', '-0.08', '-0.08', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward +52.79\n",
      "['-0.05', '+0.51', '-0.05', '+0.30', '-0.02', '+0.00', '+0.00', '+0.00']\n",
      "step 80 total_reward +22.74\n",
      "['-0.05', '+0.63', '+0.03', '+0.56', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward -19.55\n",
      "['-0.05', '+0.83', '-0.02', '+0.55', '+0.02', '+0.02', '+0.00', '+0.00']\n",
      "step 120 total_reward -44.08\n",
      "['-0.06', '+0.91', '-0.02', '+0.02', '+0.04', '+0.02', '+0.00', '+0.00']\n",
      "step 140 total_reward -1.60\n",
      "['-0.06', '+0.83', '-0.02', '-0.52', '+0.06', '+0.02', '+0.00', '+0.00']\n",
      "step 160 total_reward -44.28\n",
      "['-0.07', '+0.63', '-0.08', '-0.56', '+0.07', '+0.01', '+0.00', '+0.00']\n",
      "step 180 total_reward -33.94\n",
      "['-0.09', '+0.54', '-0.20', '-0.15', '+0.10', '+0.00', '+0.00', '+0.00']\n",
      "step 200 total_reward -2.72\n",
      "['-0.15', '+0.57', '-0.42', '+0.29', '+0.09', '-0.06', '+0.00', '+0.00']\n",
      "step 220 total_reward -37.86\n",
      "['-0.24', '+0.73', '-0.54', '+0.71', '+0.07', '-0.07', '+0.00', '+0.00']\n",
      "step 240 total_reward -97.04\n",
      "['-0.35', '+0.94', '-0.54', '+0.52', '+0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 260 total_reward -105.46\n",
      "['-0.45', '+1.01', '-0.54', '-0.01', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 280 total_reward -92.67\n",
      "['-0.56', '+0.93', '-0.54', '-0.54', '-0.04', '-0.03', '+0.00', '+0.00']\n",
      "step 300 total_reward -116.19\n",
      "['-0.67', '+0.70', '-0.53', '-0.88', '-0.08', '-0.05', '+0.00', '+0.00']\n",
      "step 320 total_reward -136.82\n",
      "['-0.78', '+0.35', '-0.53', '-1.42', '-0.13', '-0.05', '+0.00', '+0.00']\n",
      "step 340 total_reward -178.27\n",
      "['-0.85', '+0.04', '-0.26', '-1.06', '-0.21', '-5.90', '+1.00', '+1.00']\n",
      "step 353 total_reward -290.61\n",
      "['+0.01', '+0.93', '+0.39', '-0.33', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.52\n",
      "['+0.09', '+0.75', '+0.41', '-0.82', '-0.10', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.39\n",
      "['+0.18', '+0.56', '+0.53', '-0.51', '-0.18', '-0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -12.58\n",
      "['+0.31', '+0.45', '+0.78', '-0.16', '-0.32', '-0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -35.20\n",
      "['+0.53', '+0.45', '+1.42', '+0.12', '-0.46', '-0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -132.15\n",
      "['+0.87', '+0.52', '+1.87', '+0.18', '-0.53', '-0.09', '+0.00', '+0.00']\n",
      "step 100 total_reward -220.38\n",
      "['+1.00', '+0.53', '+1.87', '-0.01', '-0.56', '-0.09', '+0.00', '+0.00']\n",
      "step 107 total_reward -332.22\n",
      "['-0.00', '+0.95', '-0.09', '+0.20', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.98\n",
      "['-0.02', '+0.92', '-0.09', '-0.33', '+0.02', '+0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -9.66\n",
      "['-0.04', '+0.74', '-0.09', '-0.86', '+0.04', '+0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.10\n",
      "['-0.06', '+0.53', '-0.19', '-0.53', '+0.05', '-0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -1.03\n",
      "['-0.11', '+0.43', '-0.23', '-0.22', '+0.04', '+0.00', '+0.00', '+0.00']\n",
      "step 80 total_reward +27.65\n",
      "['-0.16', '+0.42', '-0.35', '+0.17', '+0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 100 total_reward +16.78\n",
      "['-0.23', '+0.51', '-0.36', '+0.45', '-0.06', '-0.11', '+0.00', '+0.00']\n",
      "step 120 total_reward -24.76\n",
      "['-0.30', '+0.71', '-0.24', '+0.85', '-0.21', '-0.18', '+0.00', '+0.00']\n",
      "step 140 total_reward -97.66\n",
      "['-0.33', '+0.97', '-0.12', '+0.65', '-0.39', '-0.18', '+0.00', '+0.00']\n",
      "step 160 total_reward -120.26\n",
      "['-0.35', '+1.08', '-0.12', '+0.12', '-0.57', '-0.18', '+0.00', '+0.00']\n",
      "step 180 total_reward -100.56\n",
      "['-0.38', '+1.03', '-0.12', '-0.41', '-0.75', '-0.18', '+0.00', '+0.00']\n",
      "step 200 total_reward -140.94\n",
      "['-0.41', '+0.83', '-0.05', '-0.91', '-0.93', '-0.18', '+0.00', '+0.00']\n",
      "step 220 total_reward -189.14\n",
      "['-0.29', '+0.56', '+1.15', '-0.89', '-1.14', '-0.25', '+0.00', '+0.00']\n",
      "step 240 total_reward -241.32\n",
      "['+0.08', '+0.28', '+2.47', '-1.10', '-1.43', '-0.32', '+0.00', '+0.00']\n",
      "step 260 total_reward -366.82\n",
      "['+0.41', '+0.07', '+2.54', '-0.17', '-1.78', '-5.31', '+1.00', '+0.00']\n",
      "step 272 total_reward -531.56\n",
      "['-0.00', '+0.94', '-0.06', '-0.18', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.19\n",
      "['-0.01', '+0.80', '-0.06', '-0.71', '+0.02', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.36\n",
      "['-0.03', '+0.58', '-0.12', '-0.55', '+0.03', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -11.80\n",
      "['-0.05', '+0.47', '-0.08', '-0.18', '+0.05', '+0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward +28.59\n",
      "['-0.08', '+0.47', '-0.17', '+0.16', '+0.12', '+0.09', '+0.00', '+0.00']\n",
      "step 80 total_reward +11.23\n",
      "['-0.13', '+0.58', '-0.38', '+0.57', '+0.22', '+0.11', '+0.00', '+0.00']\n",
      "step 100 total_reward -61.89\n",
      "['-0.24', '+0.81', '-0.77', '+0.93', '+0.33', '+0.10', '+0.00', '+0.00']\n",
      "step 120 total_reward -156.38\n",
      "['-0.39', '+1.01', '-0.77', '+0.40', '+0.42', '+0.10', '+0.00', '+0.00']\n",
      "step 140 total_reward -155.24\n",
      "['-0.55', '+1.05', '-0.77', '-0.13', '+0.52', '+0.10', '+0.00', '+0.00']\n",
      "step 160 total_reward -165.78\n",
      "['-0.70', '+0.92', '-0.77', '-0.67', '+0.61', '+0.10', '+0.00', '+0.00']\n",
      "step 180 total_reward -196.77\n",
      "['-0.85', '+0.64', '-0.77', '-1.20', '+0.71', '+0.10', '+0.00', '+0.00']\n",
      "step 200 total_reward -237.82\n",
      "['-1.01', '+0.20', '-0.77', '-1.73', '+0.80', '+0.10', '+0.00', '+0.00']\n",
      "step 220 total_reward -387.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.00', '+0.94', '+0.23', '+0.12', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.86\n",
      "['+0.05', '+0.90', '+0.23', '-0.42', '-0.06', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.30\n",
      "['+0.10', '+0.70', '+0.22', '-0.74', '-0.11', '-0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.73\n",
      "['+0.16', '+0.54', '+0.44', '-0.36', '-0.23', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -22.38\n",
      "['+0.28', '+0.49', '+0.81', '-0.05', '-0.33', '-0.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -62.59\n",
      "['+0.49', '+0.54', '+1.40', '+0.35', '-0.42', '-0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward -157.71\n",
      "['+0.83', '+0.69', '+1.85', '+0.50', '-0.50', '-0.10', '+0.00', '+0.00']\n",
      "step 120 total_reward -252.55\n",
      "['+1.02', '+0.75', '+1.85', '+0.24', '-0.55', '-0.10', '+0.00', '+0.00']\n",
      "step 130 total_reward -368.17\n",
      "['+0.01', '+0.95', '+0.74', '+0.27', '-0.02', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.23\n",
      "['+0.16', '+0.95', '+0.74', '-0.26', '-0.18', '-0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.62\n",
      "['+0.31', '+0.79', '+0.79', '-0.72', '-0.35', '-0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -51.55\n",
      "['+0.52', '+0.61', '+1.33', '-0.45', '-0.49', '-0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -99.81\n",
      "['+0.86', '+0.52', '+1.90', '-0.34', '-0.65', '-0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -193.52\n",
      "['+1.01', '+0.46', '+1.90', '-0.55', '-0.71', '-0.16', '+0.00', '+0.00']\n",
      "step 88 total_reward -312.78\n",
      "['+0.01', '+0.95', '+0.56', '+0.15', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.07\n",
      "['+0.12', '+0.91', '+0.56', '-0.38', '-0.14', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.44\n",
      "['+0.23', '+0.71', '+0.62', '-0.79', '-0.26', '-0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.73\n",
      "['+0.41', '+0.52', '+1.07', '-0.51', '-0.33', '-0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -70.82\n",
      "['+0.68', '+0.43', '+1.62', '-0.16', '-0.36', '+0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward -138.06\n",
      "['+1.00', '+0.35', '+1.71', '-0.49', '-0.36', '+0.01', '+0.00', '+0.00']\n",
      "step 99 total_reward -277.90\n",
      "['-0.01', '+0.95', '-0.57', '+0.41', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.33\n",
      "['-0.12', '+0.99', '-0.57', '-0.13', '+0.14', '+0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -5.40\n",
      "['-0.24', '+0.87', '-0.57', '-0.66', '+0.27', '+0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.26\n",
      "['-0.37', '+0.66', '-0.87', '-0.50', '+0.42', '+0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -55.49\n",
      "['-0.62', '+0.56', '-1.48', '-0.33', '+0.57', '+0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -134.74\n",
      "['-0.92', '+0.38', '-1.48', '-0.87', '+0.72', '+0.15', '+0.00', '+0.00']\n",
      "step 100 total_reward -184.43\n",
      "['-1.00', '+0.29', '-1.48', '-1.03', '+0.76', '+0.15', '+0.00', '+0.00']\n",
      "step 106 total_reward -299.70\n",
      "['+0.01', '+0.94', '+0.45', '+0.00', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.36\n",
      "['+0.10', '+0.86', '+0.45', '-0.53', '-0.11', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.40\n",
      "['+0.19', '+0.65', '+0.58', '-0.64', '-0.20', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.60\n",
      "['+0.33', '+0.51', '+0.85', '-0.30', '-0.32', '-0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -53.25\n",
      "['+0.57', '+0.48', '+1.44', '+0.05', '-0.41', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -135.40\n",
      "['+0.89', '+0.51', '+1.73', '+0.00', '-0.53', '-0.14', '+0.00', '+0.00']\n",
      "step 100 total_reward -208.17\n",
      "['+1.01', '+0.50', '+1.73', '-0.18', '-0.58', '-0.14', '+0.00', '+0.00']\n",
      "step 107 total_reward -321.66\n",
      "['-0.01', '+0.93', '-0.71', '-0.40', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.34\n",
      "['-0.16', '+0.73', '-0.71', '-0.87', '+0.18', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.58\n",
      "['-0.34', '+0.53', '-1.13', '-0.45', '+0.30', '+0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.00\n",
      "['-0.60', '+0.40', '-1.36', '-0.62', '+0.49', '+0.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -105.22\n",
      "['-0.87', '+0.13', '-1.36', '-1.16', '+0.78', '+0.28', '+0.00', '+0.00']\n",
      "step 80 total_reward -178.34\n",
      "['-1.01', '+0.08', '-1.08', '-0.32', '+2.32', '+3.07', '+0.00', '+0.00']\n",
      "step 92 total_reward -362.89\n",
      "['+0.01', '+0.95', '+0.26', '+0.24', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.15\n",
      "['+0.06', '+0.94', '+0.26', '-0.29', '-0.06', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.65\n",
      "['+0.11', '+0.76', '+0.26', '-0.83', '-0.12', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.33\n",
      "['+0.18', '+0.59', '+0.49', '-0.38', '-0.19', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -15.63\n",
      "['+0.30', '+0.54', '+0.72', '+0.04', '-0.28', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -41.46\n",
      "['+0.50', '+0.61', '+1.22', '+0.38', '-0.43', '-0.14', '+0.00', '+0.00']\n",
      "step 100 total_reward -133.89\n",
      "['+0.80', '+0.77', '+1.73', '+0.56', '-0.57', '-0.18', '+0.00', '+0.00']\n",
      "step 120 total_reward -240.13\n",
      "['+1.01', '+0.84', '+1.73', '+0.24', '-0.68', '-0.18', '+0.00', '+0.00']\n",
      "step 132 total_reward -361.61\n",
      "['+0.01', '+0.93', '+0.41', '-0.41', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.59\n",
      "['+0.09', '+0.72', '+0.41', '-0.86', '-0.10', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.58\n",
      "['+0.19', '+0.52', '+0.63', '-0.51', '-0.19', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -12.12\n",
      "['+0.36', '+0.43', '+0.99', '-0.13', '-0.22', '-0.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -40.25\n",
      "['+0.58', '+0.44', '+1.23', '+0.17', '-0.24', '-0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward -89.86\n",
      "['+0.87', '+0.56', '+1.56', '+0.43', '-0.29', '-0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward -166.16\n",
      "['+1.01', '+0.60', '+1.56', '+0.19', '-0.30', '-0.04', '+0.00', '+0.00']\n",
      "step 109 total_reward -275.82\n",
      "['-0.01', '+0.94', '-0.28', '-0.12', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.01\n",
      "['-0.06', '+0.82', '-0.28', '-0.65', '+0.07', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.99\n",
      "['-0.12', '+0.62', '-0.37', '-0.48', '+0.15', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -18.90\n",
      "['-0.22', '+0.54', '-0.65', '-0.12', '+0.26', '+0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -36.34\n",
      "['-0.38', '+0.55', '-1.03', '+0.26', '+0.38', '+0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -103.59\n",
      "['-0.64', '+0.67', '-1.48', '+0.36', '+0.53', '+0.15', '+0.00', '+0.00']\n",
      "step 100 total_reward -195.26\n",
      "['-0.94', '+0.70', '-1.48', '-0.17', '+0.69', '+0.15', '+0.00', '+0.00']\n",
      "step 120 total_reward -231.01\n",
      "['-1.01', '+0.68', '-1.48', '-0.30', '+0.72', '+0.15', '+0.00', '+0.00']\n",
      "step 125 total_reward -339.64\n",
      "['-0.01', '+0.94', '-0.41', '+0.10', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.15\n",
      "['-0.09', '+0.89', '-0.41', '-0.43', '+0.10', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.23\n",
      "['-0.17', '+0.70', '-0.43', '-0.67', '+0.20', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.13\n",
      "['-0.29', '+0.55', '-0.81', '-0.34', '+0.32', '+0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -52.81\n",
      "['-0.50', '+0.50', '-1.26', '-0.01', '+0.48', '+0.18', '+0.00', '+0.00']\n",
      "step 80 total_reward -121.15\n",
      "['-0.76', '+0.44', '-1.31', '-0.49', '+0.66', '+0.18', '+0.00', '+0.00']\n",
      "step 100 total_reward -169.68\n",
      "['-1.00', '+0.22', '-1.31', '-0.99', '+0.83', '+0.18', '+0.00', '+0.00']\n",
      "step 119 total_reward -323.65\n",
      "['+0.02', '+0.93', '+0.80', '-0.32', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.24\n",
      "['+0.17', '+0.75', '+0.80', '-0.85', '-0.20', '-0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.95\n",
      "['+0.38', '+0.56', '+1.25', '-0.47', '-0.34', '-0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -62.02\n",
      "['+0.68', '+0.48', '+1.85', '-0.14', '-0.45', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -145.79\n",
      "['+1.01', '+0.40', '+1.93', '-0.50', '-0.50', '-0.05', '+0.00', '+0.00']\n",
      "step 77 total_reward -287.87\n",
      "['+0.01', '+0.95', '+0.67', '+0.46', '-0.02', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.12\n",
      "['+0.15', '+1.01', '+0.67', '-0.07', '-0.16', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.42\n",
      "['+0.28', '+0.90', '+0.67', '-0.61', '-0.31', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.03\n",
      "['+0.44', '+0.69', '+1.05', '-0.68', '-0.43', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -75.63\n",
      "['+0.72', '+0.54', '+1.77', '-0.26', '-0.51', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -151.21\n",
      "['+1.01', '+0.46', '+1.87', '-0.55', '-0.58', '-0.09', '+0.00', '+0.00']\n",
      "step 96 total_reward -294.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.00', '+0.94', '+0.14', '+0.00', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.13\n",
      "['+0.03', '+0.86', '+0.14', '-0.53', '-0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.72\n",
      "['+0.06', '+0.65', '+0.20', '-0.63', '-0.06', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -31.37\n",
      "['+0.11', '+0.52', '+0.26', '-0.25', '-0.08', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward +2.18\n",
      "['+0.17', '+0.49', '+0.39', '+0.06', '-0.11', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -9.22\n",
      "['+0.27', '+0.57', '+0.58', '+0.45', '-0.14', '-0.01', '+0.00', '+0.00']\n",
      "step 100 total_reward -62.03\n",
      "['+0.40', '+0.76', '+0.79', '+0.87', '-0.14', '-0.01', '+0.00', '+0.00']\n",
      "step 120 total_reward -136.42\n",
      "['+0.58', '+1.05', '+0.94', '+0.81', '-0.11', '+0.04', '+0.00', '+0.00']\n",
      "step 140 total_reward -176.92\n",
      "['+0.77', '+1.21', '+0.94', '+0.28', '-0.07', '+0.04', '+0.00', '+0.00']\n",
      "step 160 total_reward -170.04\n",
      "['+0.96', '+1.21', '+0.94', '-0.26', '-0.03', '+0.04', '+0.00', '+0.00']\n",
      "step 180 total_reward -176.35\n",
      "['+1.01', '+1.18', '+0.94', '-0.39', '-0.02', '+0.04', '+0.00', '+0.00']\n",
      "step 185 total_reward -279.77\n",
      "['-0.01', '+0.94', '-0.32', '-0.16', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.16\n",
      "['-0.07', '+0.80', '-0.32', '-0.70', '+0.08', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.04\n",
      "['-0.14', '+0.60', '-0.45', '-0.52', '+0.15', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -20.36\n",
      "['-0.26', '+0.50', '-0.73', '-0.14', '+0.21', '+0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -33.15\n",
      "['-0.44', '+0.51', '-1.09', '+0.18', '+0.25', '+0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -89.91\n",
      "['-0.68', '+0.56', '-1.23', '-0.03', '+0.34', '+0.08', '+0.00', '+0.00']\n",
      "step 100 total_reward -134.00\n",
      "['-0.93', '+0.46', '-1.23', '-0.57', '+0.42', '+0.08', '+0.00', '+0.00']\n",
      "step 120 total_reward -170.30\n",
      "['-1.01', '+0.39', '-1.23', '-0.75', '+0.45', '+0.08', '+0.00', '+0.00']\n",
      "step 127 total_reward -284.51\n",
      "['+0.01', '+0.92', '+0.46', '-0.57', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.60\n",
      "['+0.10', '+0.68', '+0.50', '-0.88', '-0.11', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.97\n",
      "['+0.22', '+0.47', '+0.73', '-0.48', '-0.19', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -0.95\n",
      "['+0.40', '+0.38', '+1.06', '-0.13', '-0.27', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -37.37\n",
      "['+0.65', '+0.40', '+1.45', '+0.19', '-0.35', '-0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -112.49\n",
      "['+0.96', '+0.42', '+1.58', '-0.18', '-0.39', '-0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward -159.02\n",
      "['+1.01', '+0.41', '+1.58', '-0.26', '-0.40', '-0.04', '+0.00', '+0.00']\n",
      "step 103 total_reward -262.72\n",
      "['+0.00', '+0.93', '+0.22', '-0.33', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.84\n",
      "['+0.05', '+0.75', '+0.21', '-0.81', '-0.05', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.84\n",
      "['+0.11', '+0.59', '+0.42', '-0.28', '-0.10', '+0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward +3.62\n",
      "['+0.20', '+0.58', '+0.49', '+0.14', '-0.14', '-0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -6.65\n",
      "['+0.32', '+0.67', '+0.74', '+0.49', '-0.20', '-0.06', '+0.00', '+0.00']\n",
      "step 80 total_reward -69.50\n",
      "['+0.50', '+0.87', '+1.03', '+0.83', '-0.27', '-0.07', '+0.00', '+0.00']\n",
      "step 100 total_reward -152.50\n",
      "['+0.73', '+1.10', '+1.20', '+0.55', '-0.29', '-0.02', '+0.00', '+0.00']\n",
      "step 120 total_reward -187.32\n",
      "['+0.97', '+1.18', '+1.20', '+0.01', '-0.30', '-0.02', '+0.00', '+0.00']\n",
      "step 140 total_reward -197.72\n",
      "['+1.01', '+1.18', '+1.20', '-0.07', '-0.30', '-0.02', '+0.00', '+0.00']\n",
      "step 143 total_reward -299.41\n",
      "['-0.01', '+0.94', '-0.30', '-0.18', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.25\n",
      "['-0.06', '+0.80', '-0.24', '-0.72', '-0.16', '-0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.80\n",
      "['-0.08', '+0.62', '+0.10', '-0.36', '-0.37', '-0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -13.84\n",
      "['+0.00', '+0.56', '+0.80', '-0.06', '-0.54', '-0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -72.54\n",
      "['+0.24', '+0.57', '+1.57', '+0.15', '-0.64', '-0.11', '+0.00', '+0.00']\n",
      "step 80 total_reward -173.13\n",
      "['+0.63', '+0.64', '+2.37', '+0.27', '-0.77', '-0.13', '+0.00', '+0.00']\n",
      "step 100 total_reward -299.97\n",
      "['+1.02', '+0.69', '+2.64', '+0.09', '-0.90', '-0.18', '+0.00', '+0.00']\n",
      "step 115 total_reward -470.91\n",
      "['-0.00', '+0.94', '-0.09', '-0.20', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.12\n",
      "['-0.02', '+0.79', '-0.09', '-0.73', '+0.02', '+0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.83\n",
      "['-0.04', '+0.56', '-0.17', '-0.64', '+0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -14.70\n",
      "['-0.08', '+0.43', '-0.14', '-0.27', '-0.04', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward +24.56\n",
      "['-0.11', '+0.39', '-0.12', '+0.08', '-0.12', '-0.12', '+0.00', '+0.00']\n",
      "step 80 total_reward +28.66\n",
      "['-0.10', '+0.50', '+0.19', '+0.56', '-0.19', '-0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -37.67\n",
      "['-0.03', '+0.71', '+0.44', '+0.86', '-0.24', '-0.06', '+0.00', '+0.00']\n",
      "step 120 total_reward -107.05\n",
      "['+0.06', '+0.93', '+0.49', '+0.50', '-0.32', '-0.08', '+0.00', '+0.00']\n",
      "step 140 total_reward -111.58\n",
      "['+0.16', '+0.99', '+0.49', '-0.03', '-0.41', '-0.08', '+0.00', '+0.00']\n",
      "step 160 total_reward -106.74\n",
      "['+0.26', '+0.90', '+0.49', '-0.57', '-0.49', '-0.08', '+0.00', '+0.00']\n",
      "step 180 total_reward -133.91\n",
      "['+0.39', '+0.70', '+0.99', '-0.57', '-0.56', '-0.06', '+0.00', '+0.00']\n",
      "step 200 total_reward -170.39\n",
      "['+0.68', '+0.58', '+1.86', '-0.27', '-0.58', '-0.01', '+0.00', '+0.00']\n",
      "step 220 total_reward -261.63\n",
      "['+1.01', '+0.47', '+1.96', '-0.63', '-0.59', '-0.00', '+0.00', '+0.00']\n",
      "step 237 total_reward -400.71\n",
      "['-0.01', '+0.95', '-0.70', '+0.46', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.03\n",
      "['-0.15', '+1.01', '-0.70', '-0.07', '+0.17', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -8.91\n",
      "['-0.29', '+0.90', '-0.70', '-0.61', '+0.33', '+0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -39.71\n",
      "['-0.45', '+0.69', '-1.08', '-0.60', '+0.50', '+0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -77.86\n",
      "['-0.72', '+0.53', '-1.44', '-0.70', '+0.69', '+0.19', '+0.00', '+0.00']\n",
      "step 80 total_reward -143.51\n",
      "['-1.01', '+0.23', '-1.44', '-1.23', '+0.88', '+0.19', '+0.00', '+0.00']\n",
      "step 100 total_reward -302.35\n",
      "['+0.01', '+0.94', '+0.57', '+0.03', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.49\n",
      "['+0.12', '+0.87', '+0.54', '-0.50', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -9.12\n",
      "['+0.23', '+0.65', '+0.50', '-0.71', '+0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -6.76\n",
      "['+0.32', '+0.51', '+0.47', '-0.26', '-0.03', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward +26.16\n",
      "['+0.43', '+0.49', '+0.54', '+0.14', '-0.07', '-0.06', '+0.00', '+0.00']\n",
      "step 80 total_reward +9.46\n",
      "['+0.55', '+0.60', '+0.73', '+0.56', '-0.10', '-0.01', '+0.00', '+0.00']\n",
      "step 100 total_reward -51.20\n",
      "['+0.71', '+0.80', '+0.79', '+0.79', '-0.13', '-0.06', '+0.00', '+0.00']\n",
      "step 120 total_reward -105.30\n",
      "['+0.87', '+0.98', '+0.80', '+0.33', '-0.19', '-0.07', '+0.00', '+0.00']\n",
      "step 140 total_reward -110.95\n",
      "['+1.00', '+1.00', '+0.80', '-0.12', '-0.25', '-0.07', '+0.00', '+0.00']\n",
      "step 157 total_reward -220.94\n",
      "['+0.01', '+0.94', '+0.61', '-0.12', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.87\n",
      "['+0.13', '+0.82', '+0.61', '-0.65', '-0.15', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.81\n",
      "['+0.27', '+0.60', '+0.84', '-0.60', '-0.28', '-0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.88\n",
      "['+0.48', '+0.48', '+1.28', '-0.26', '-0.42', '-0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -94.45\n",
      "['+0.79', '+0.42', '+1.72', '-0.27', '-0.54', '-0.11', '+0.00', '+0.00']\n",
      "step 80 total_reward -175.89\n",
      "['+1.02', '+0.33', '+1.72', '-0.61', '-0.61', '-0.11', '+0.00', '+0.00']\n",
      "step 93 total_reward -305.99\n",
      "['-0.01', '+0.94', '-0.43', '+0.05', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.10\n",
      "['-0.09', '+0.88', '-0.43', '-0.48', '+0.10', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.52\n",
      "['-0.18', '+0.67', '-0.57', '-0.69', '+0.19', '+0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.48\n",
      "['-0.33', '+0.51', '-0.89', '-0.34', '+0.22', '+0.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -47.91\n",
      "['-0.52', '+0.38', '-1.02', '-0.68', '+0.42', '+0.33', '+0.00', '+0.00']\n",
      "step 80 total_reward -100.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.70', '+0.14', '-1.10', '-0.50', '+0.55', '-3.20', '+0.00', '+1.00']\n",
      "step 98 total_reward -241.35\n",
      "['-0.01', '+0.93', '-0.63', '-0.34', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.31\n",
      "['-0.14', '+0.75', '-0.63', '-0.80', '+0.16', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.23\n",
      "['-0.29', '+0.55', '-0.90', '-0.55', '+0.32', '+0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.30\n",
      "['-0.51', '+0.43', '-1.25', '-0.48', '+0.52', '+0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -98.25\n",
      "['-0.76', '+0.20', '-1.25', '-1.01', '+0.78', '+0.26', '+0.00', '+0.00']\n",
      "step 80 total_reward -162.80\n",
      "['-0.87', '+0.06', '-0.07', '+0.10', '+1.22', '+0.48', '+0.00', '+1.00']\n",
      "step 90 total_reward -255.18\n",
      "['-0.00', '+0.94', '-0.10', '-0.09', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.51\n",
      "['-0.02', '+0.83', '-0.10', '-0.62', '+0.03', '+0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.30\n",
      "['-0.05', '+0.63', '-0.17', '-0.51', '+0.03', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -18.17\n",
      "['-0.09', '+0.56', '-0.26', '-0.06', '+0.00', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward +12.94\n",
      "['-0.14', '+0.61', '-0.25', '+0.38', '-0.07', '-0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward -25.18\n",
      "['-0.18', '+0.79', '-0.11', '+0.81', '-0.15', '-0.11', '+0.00', '+0.00']\n",
      "step 100 total_reward -93.91\n",
      "['-0.20', '+0.95', '-0.11', '+0.28', '-0.26', '-0.11', '+0.00', '+0.00']\n",
      "step 120 total_reward -68.57\n",
      "['-0.22', '+0.95', '-0.11', '-0.26', '-0.37', '-0.11', '+0.00', '+0.00']\n",
      "step 140 total_reward -77.88\n",
      "['-0.24', '+0.79', '-0.09', '-0.77', '-0.48', '-0.11', '+0.00', '+0.00']\n",
      "step 160 total_reward -123.52\n",
      "['-0.18', '+0.61', '+0.69', '-0.47', '-0.55', '-0.05', '+0.00', '+0.00']\n",
      "step 180 total_reward -124.44\n",
      "['+0.03', '+0.50', '+1.41', '-0.30', '-0.60', '-0.02', '+0.00', '+0.00']\n",
      "step 200 total_reward -182.54\n",
      "['+0.40', '+0.46', '+2.29', '+0.02', '-0.64', '-0.05', '+0.00', '+0.00']\n",
      "step 220 total_reward -287.82\n",
      "['+0.94', '+0.47', '+2.84', '-0.06', '-0.67', '-0.03', '+0.00', '+0.00']\n",
      "step 240 total_reward -394.15\n",
      "['+1.02', '+0.47', '+2.84', '-0.14', '-0.68', '-0.03', '+0.00', '+0.00']\n",
      "step 243 total_reward -499.54\n",
      "['-0.01', '+0.93', '-0.66', '-0.49', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['-0.14', '+0.70', '-0.69', '-0.83', '+0.16', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.72\n",
      "['-0.31', '+0.51', '-1.02', '-0.51', '+0.32', '+0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.01\n",
      "['-0.54', '+0.34', '-1.20', '-0.79', '+0.59', '+0.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -101.37\n",
      "['-0.77', '+0.13', '-1.06', '-0.14', '+0.94', '-0.55', '+0.00', '+0.00']\n",
      "step 80 total_reward -112.97\n",
      "['-0.98', '-0.00', '-1.05', '-0.67', '+0.39', '-0.55', '+0.00', '+0.00']\n",
      "step 100 total_reward -97.04\n",
      "['-1.01', '-0.02', '-1.05', '-0.73', '+0.33', '-0.55', '+0.00', '+0.00']\n",
      "step 102 total_reward -196.83\n",
      "['+0.01', '+0.95', '+0.75', '+0.31', '-0.02', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.22\n",
      "['+0.16', '+0.96', '+0.75', '-0.23', '-0.19', '-0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.64\n",
      "['+0.31', '+0.81', '+0.79', '-0.68', '-0.35', '-0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.14\n",
      "['+0.53', '+0.68', '+1.42', '-0.26', '-0.54', '-0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -113.02\n",
      "['+0.88', '+0.61', '+1.91', '-0.36', '-0.71', '-0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -204.77\n",
      "['+1.02', '+0.56', '+1.91', '-0.55', '-0.76', '-0.16', '+0.00', '+0.00']\n",
      "step 87 total_reward -320.74\n",
      "['-0.01', '+0.95', '-0.66', '+0.34', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.05\n",
      "['-0.14', '+0.97', '-0.66', '-0.19', '+0.16', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.84\n",
      "['-0.27', '+0.83', '-0.66', '-0.72', '+0.31', '+0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -45.15\n",
      "['-0.43', '+0.63', '-1.06', '-0.56', '+0.48', '+0.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -79.64\n",
      "['-0.70', '+0.49', '-1.45', '-0.61', '+0.69', '+0.21', '+0.00', '+0.00']\n",
      "step 80 total_reward -148.89\n",
      "['-0.99', '+0.22', '-1.45', '-1.15', '+0.90', '+0.21', '+0.00', '+1.00']\n",
      "step 100 total_reward -203.36\n",
      "['-1.01', '+0.20', '-1.47', '-1.12', '+0.89', '-0.13', '+0.00', '+1.00']\n",
      "step 101 total_reward -303.36\n",
      "['+0.00', '+0.95', '+0.25', '+0.26', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.24\n",
      "['+0.05', '+0.94', '+0.25', '-0.27', '-0.06', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.92\n",
      "['+0.10', '+0.78', '+0.25', '-0.81', '-0.12', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.89\n",
      "['+0.16', '+0.54', '+0.38', '-0.69', '-0.19', '-0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -26.62\n",
      "['+0.27', '+0.38', '+0.68', '-0.33', '-0.26', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -27.52\n",
      "['+0.45', '+0.34', '+1.11', '-0.06', '-0.33', '-0.03', '+0.00', '+0.00']\n",
      "step 100 total_reward -83.82\n",
      "['+0.72', '+0.38', '+1.60', '+0.36', '-0.36', '-0.05', '+0.00', '+0.00']\n",
      "step 120 total_reward -171.37\n",
      "['+1.01', '+0.45', '+1.68', '+0.07', '-0.41', '-0.06', '+0.00', '+0.00']\n",
      "step 137 total_reward -307.80\n",
      "['+0.01', '+0.95', '+0.67', '+0.46', '-0.02', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.12\n",
      "['+0.15', '+1.01', '+0.67', '-0.08', '-0.16', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.44\n",
      "['+0.28', '+0.90', '+0.67', '-0.61', '-0.31', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.05\n",
      "['+0.44', '+0.70', '+1.07', '-0.56', '-0.46', '-0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -75.11\n",
      "['+0.72', '+0.58', '+1.69', '-0.34', '-0.60', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -156.83\n",
      "['+1.01', '+0.44', '+1.70', '-0.77', '-0.74', '-0.16', '+0.00', '+0.00']\n",
      "step 97 total_reward -299.34\n",
      "['+0.01', '+0.94', '+0.57', '+0.09', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.26\n",
      "['+0.12', '+0.89', '+0.57', '-0.45', '-0.14', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.88\n",
      "['+0.24', '+0.69', '+0.65', '-0.70', '-0.28', '-0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.24\n",
      "['+0.43', '+0.56', '+1.22', '-0.19', '-0.43', '-0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -91.98\n",
      "['+0.74', '+0.55', '+1.87', '+0.08', '-0.58', '-0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -198.13\n",
      "['+1.00', '+0.53', '+1.87', '-0.29', '-0.70', '-0.17', '+0.00', '+0.00']\n",
      "step 94 total_reward -330.24\n",
      "['-0.02', '+0.92', '-0.79', '-0.54', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.42\n",
      "['-0.17', '+0.69', '-0.84', '-0.84', '+0.19', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.85\n",
      "['-0.37', '+0.50', '-1.11', '-0.45', '+0.42', '+0.31', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.56\n",
      "['-0.61', '+0.31', '-1.24', '-0.89', '+0.94', '+0.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -135.05\n",
      "['-0.85', '-0.03', '-1.23', '-1.43', '+1.54', '+0.59', '+0.00', '+0.00']\n",
      "step 80 total_reward -247.70\n",
      "['-0.89', '-0.11', '+0.27', '-0.16', '+1.62', '-1.08', '+0.00', '+0.00']\n",
      "step 84 total_reward -366.91\n",
      "['-0.00', '+0.93', '-0.03', '-0.39', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.00\n",
      "['-0.01', '+0.73', '-0.04', '-0.83', '+0.08', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.40\n",
      "['-0.04', '+0.54', '-0.23', '-0.43', '+0.13', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward +8.26\n",
      "['-0.11', '+0.47', '-0.47', '-0.06', '+0.18', '+0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward +4.05\n",
      "['-0.23', '+0.52', '-0.74', '+0.34', '+0.25', '+0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -51.41\n",
      "['-0.42', '+0.70', '-1.16', '+0.71', '+0.34', '+0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward -145.66\n",
      "['-0.69', '+0.96', '-1.46', '+0.80', '+0.44', '+0.12', '+0.00', '+0.00']\n",
      "step 120 total_reward -226.55\n",
      "['-0.98', '+1.11', '-1.46', '+0.27', '+0.56', '+0.12', '+0.00', '+0.00']\n",
      "step 140 total_reward -250.49\n",
      "['-1.01', '+1.12', '-1.46', '+0.21', '+0.57', '+0.12', '+0.00', '+0.00']\n",
      "step 142 total_reward -351.86\n",
      "['+0.00', '+0.94', '+0.21', '-0.06', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.66\n",
      "['+0.05', '+0.84', '+0.21', '-0.59', '-0.05', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.37\n",
      "['+0.09', '+0.62', '+0.29', '-0.59', '-0.10', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -27.02\n",
      "['+0.16', '+0.50', '+0.43', '-0.26', '-0.18', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -15.17\n",
      "['+0.28', '+0.48', '+0.79', '+0.14', '-0.20', '-0.00', '+0.00', '+0.00']\n",
      "step 80 total_reward -56.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.48', '+0.60', '+1.06', '+0.60', '-0.17', '-0.01', '+0.00', '+0.00']\n",
      "step 100 total_reward -121.96\n",
      "['+0.71', '+0.84', '+1.28', '+0.97', '-0.20', '-0.04', '+0.00', '+0.00']\n",
      "step 120 total_reward -202.55\n",
      "['+0.96', '+1.05', '+1.28', '+0.44', '-0.24', '-0.04', '+0.00', '+0.00']\n",
      "step 140 total_reward -213.34\n",
      "['+1.00', '+1.07', '+1.28', '+0.36', '-0.24', '-0.04', '+0.00', '+0.00']\n",
      "step 143 total_reward -314.69\n",
      "['-0.01', '+0.93', '-0.29', '-0.49', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.76\n",
      "['-0.06', '+0.70', '-0.27', '-0.87', '+0.08', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.99\n",
      "['-0.13', '+0.48', '-0.40', '-0.58', '+0.19', '+0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward +1.67\n",
      "['-0.24', '+0.35', '-0.72', '-0.29', '+0.33', '+0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -17.30\n",
      "['-0.42', '+0.30', '-1.17', '-0.07', '+0.47', '+0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -86.21\n",
      "['-0.68', '+0.24', '-1.34', '-0.42', '+0.62', '+0.14', '+0.00', '+0.00']\n",
      "step 100 total_reward -146.41\n",
      "['-0.95', '+0.04', '-1.38', '-0.90', '+0.78', '+0.15', '+0.00', '+0.00']\n",
      "step 120 total_reward -210.01\n",
      "['-1.01', '-0.02', '-1.38', '-1.01', '+0.81', '+0.15', '+0.00', '+0.00']\n",
      "step 124 total_reward -320.89\n",
      "['+0.01', '+0.92', '+0.29', '-0.57', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.72\n",
      "['+0.06', '+0.68', '+0.33', '-0.85', '-0.07', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.90\n",
      "['+0.15', '+0.50', '+0.50', '-0.38', '-0.11', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward +22.08\n",
      "['+0.26', '+0.44', '+0.65', '+0.01', '-0.12', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward +13.49\n",
      "['+0.41', '+0.51', '+0.84', '+0.48', '-0.16', '-0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward -42.29\n",
      "['+0.61', '+0.69', '+1.08', '+0.76', '-0.18', '-0.02', '+0.00', '+0.00']\n",
      "step 100 total_reward -112.43\n",
      "['+0.84', '+0.94', '+1.18', '+0.63', '-0.21', '-0.04', '+0.00', '+0.00']\n",
      "step 120 total_reward -153.19\n",
      "['+1.00', '+1.03', '+1.18', '+0.26', '-0.24', '-0.04', '+0.00', '+0.00']\n",
      "step 134 total_reward -260.11\n",
      "['-0.00', '+0.93', '-0.06', '-0.32', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.10\n",
      "['-0.01', '+0.75', '-0.06', '-0.86', '+0.02', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.39\n",
      "['-0.03', '+0.54', '-0.07', '-0.52', '+0.03', '+0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward +7.78\n",
      "['-0.05', '+0.44', '-0.17', '-0.22', '+0.05', '-0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward +35.65\n",
      "['-0.09', '+0.42', '-0.25', '+0.05', '+0.04', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward +34.29\n",
      "['-0.15', '+0.49', '-0.30', '+0.40', '-0.02', '-0.07', '+0.00', '+0.00']\n",
      "step 100 total_reward -3.15\n",
      "['-0.21', '+0.69', '-0.21', '+0.86', '-0.09', '-0.05', '+0.00', '+0.00']\n",
      "step 120 total_reward -75.61\n",
      "['-0.24', '+0.97', '-0.15', '+0.74', '-0.13', '-0.04', '+0.00', '+0.00']\n",
      "step 140 total_reward -96.67\n",
      "['-0.27', '+1.11', '-0.15', '+0.21', '-0.18', '-0.04', '+0.00', '+0.00']\n",
      "step 160 total_reward -64.96\n",
      "['-0.30', '+1.08', '-0.15', '-0.33', '-0.22', '-0.04', '+0.00', '+0.00']\n",
      "step 180 total_reward -78.21\n",
      "['-0.33', '+0.90', '-0.15', '-0.86', '-0.26', '-0.04', '+0.00', '+0.00']\n",
      "step 200 total_reward -117.57\n",
      "['-0.33', '+0.63', '+0.18', '-0.75', '-0.28', '+0.01', '+0.00', '+0.00']\n",
      "step 220 total_reward -89.34\n",
      "['-0.26', '+0.46', '+0.51', '-0.43', '-0.30', '-0.04', '+0.00', '+0.00']\n",
      "step 240 total_reward -68.27\n",
      "['-0.11', '+0.38', '+0.97', '-0.05', '-0.29', '+0.04', '+0.00', '+0.00']\n",
      "step 260 total_reward -90.51\n",
      "['+0.12', '+0.43', '+1.40', '+0.37', '-0.25', '+0.07', '+0.00', '+0.00']\n",
      "step 280 total_reward -143.95\n",
      "['+0.44', '+0.60', '+1.71', '+0.75', '-0.16', '+0.09', '+0.00', '+0.00']\n",
      "step 300 total_reward -213.16\n",
      "['+0.79', '+0.90', '+1.80', '+1.01', '-0.12', '+0.01', '+0.00', '+0.00']\n",
      "step 320 total_reward -279.42\n",
      "['+1.01', '+1.05', '+1.80', '+0.69', '-0.12', '+0.01', '+0.00', '+0.00']\n",
      "step 332 total_reward -389.95\n",
      "['+0.00', '+0.93', '+0.01', '-0.40', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.97\n",
      "['+0.00', '+0.73', '+0.00', '-0.83', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.46\n",
      "['+0.00', '+0.55', '-0.01', '-0.42', '-0.02', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward +25.62\n",
      "['+0.00', '+0.49', '+0.01', '-0.03', '-0.08', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward +58.82\n",
      "['+0.03', '+0.55', '+0.21', '+0.45', '-0.12', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -4.05\n",
      "['+0.10', '+0.74', '+0.42', '+0.77', '-0.12', '+0.01', '+0.00', '+0.00']\n",
      "step 100 total_reward -68.02\n",
      "['+0.18', '+0.92', '+0.44', '+0.35', '-0.11', '+0.01', '+0.00', '+0.00']\n",
      "step 120 total_reward -55.44\n",
      "['+0.27', '+0.94', '+0.44', '-0.19', '-0.10', '+0.01', '+0.00', '+0.00']\n",
      "step 140 total_reward -50.25\n",
      "['+0.36', '+0.80', '+0.44', '-0.55', '-0.10', '-0.02', '+0.00', '+0.00']\n",
      "step 160 total_reward -63.13\n",
      "['+0.47', '+0.71', '+0.64', '-0.11', '-0.07', '+0.05', '+0.00', '+0.00']\n",
      "step 180 total_reward -58.40\n",
      "['+0.61', '+0.75', '+0.70', '+0.36', '-0.04', '+0.03', '+0.00', '+0.00']\n",
      "step 200 total_reward -86.65\n",
      "['+0.75', '+0.90', '+0.71', '+0.43', '-0.01', '+0.02', '+0.00', '+0.00']\n",
      "step 220 total_reward -113.06\n",
      "['+0.89', '+0.95', '+0.71', '-0.10', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 240 total_reward -113.66\n",
      "['+1.01', '+0.87', '+0.71', '-0.53', '+0.02', '+0.02', '+0.00', '+0.00']\n",
      "step 256 total_reward -232.76\n",
      "['-0.00', '+0.95', '-0.20', '+0.23', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.42\n",
      "['-0.04', '+0.93', '-0.20', '-0.31', '+0.05', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.95\n",
      "['-0.08', '+0.76', '-0.20', '-0.84', '+0.09', '+0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.97\n",
      "['-0.13', '+0.57', '-0.38', '-0.41', '+0.17', '+0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -9.29\n",
      "['-0.24', '+0.52', '-0.70', '+0.08', '+0.21', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -33.24\n",
      "['-0.40', '+0.59', '-0.97', '+0.36', '+0.27', '+0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -92.55\n",
      "['-0.64', '+0.75', '-1.36', '+0.70', '+0.29', '+0.03', '+0.00', '+0.00']\n",
      "step 120 total_reward -178.22\n",
      "['-0.92', '+0.89', '-1.39', '+0.20', '+0.32', '+0.02', '+0.00', '+0.00']\n",
      "step 140 total_reward -196.79\n",
      "['-1.00', '+0.90', '-1.39', '+0.04', '+0.33', '+0.02', '+0.00', '+0.00']\n",
      "step 146 total_reward -301.79\n",
      "['+0.01', '+0.93', '+0.42', '-0.30', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.43\n",
      "['+0.09', '+0.76', '+0.42', '-0.83', '-0.10', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.75\n",
      "['+0.19', '+0.59', '+0.59', '-0.32', '-0.21', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -11.92\n",
      "['+0.35', '+0.57', '+1.04', '+0.10', '-0.33', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -72.30\n",
      "['+0.61', '+0.65', '+1.50', '+0.44', '-0.45', '-0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -164.43\n",
      "['+0.95', '+0.79', '+1.80', '+0.28', '-0.60', '-0.15', '+0.00', '+0.00']\n",
      "step 100 total_reward -243.08\n",
      "['+1.01', '+0.80', '+1.80', '+0.20', '-0.62', '-0.15', '+0.00', '+0.00']\n",
      "step 103 total_reward -347.03\n",
      "['+0.00', '+0.95', '+0.14', '+0.44', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.65\n",
      "['+0.03', '+1.00', '+0.14', '-0.09', '-0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward +22.94\n",
      "['+0.06', '+0.89', '+0.14', '-0.63', '-0.07', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -16.59\n",
      "['+0.09', '+0.65', '+0.15', '-0.73', '-0.11', '-0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -10.78\n",
      "['+0.15', '+0.48', '+0.44', '-0.41', '-0.14', '-0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward +10.82\n",
      "['+0.26', '+0.42', '+0.66', '-0.07', '-0.14', '+0.02', '+0.00', '+0.00']\n",
      "step 100 total_reward -0.69\n",
      "['+0.42', '+0.45', '+0.86', '+0.26', '-0.08', '+0.07', '+0.00', '+0.00']\n",
      "step 120 total_reward -36.92\n",
      "['+0.60', '+0.59', '+0.93', '+0.66', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 140 total_reward -82.26\n",
      "['+0.78', '+0.85', '+0.87', '+0.90', '+0.06', '+0.06', '+0.00', '+0.00']\n",
      "step 160 total_reward -134.29\n",
      "['+0.95', '+1.04', '+0.87', '+0.36', '+0.12', '+0.06', '+0.00', '+0.00']\n",
      "step 180 total_reward -134.47\n",
      "['+1.00', '+1.06', '+0.87', '+0.20', '+0.13', '+0.06', '+0.00', '+0.00']\n",
      "step 186 total_reward -236.06\n",
      "['-0.01', '+0.94', '-0.54', '-0.09', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.77\n",
      "['-0.12', '+0.83', '-0.54', '-0.63', '+0.13', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.24', '+0.62', '-0.77', '-0.57', '+0.24', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.06\n",
      "['-0.43', '+0.52', '-1.11', '-0.15', '+0.31', '+0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -72.78\n",
      "['-0.68', '+0.49', '-1.31', '-0.31', '+0.49', '+0.22', '+0.00', '+0.00']\n",
      "step 80 total_reward -130.63\n",
      "['-0.94', '+0.31', '-1.31', '-0.84', '+0.71', '+0.22', '+0.00', '+0.00']\n",
      "step 100 total_reward -189.28\n",
      "['-1.01', '+0.24', '-1.31', '-0.98', '+0.76', '+0.22', '+0.00', '+0.00']\n",
      "step 105 total_reward -303.17\n",
      "['+0.01', '+0.93', '+0.36', '-0.38', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.63\n",
      "['+0.08', '+0.73', '+0.35', '-0.84', '-0.09', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.98\n",
      "['+0.16', '+0.54', '+0.54', '-0.47', '-0.23', '-0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -12.70\n",
      "['+0.31', '+0.44', '+0.97', '-0.16', '-0.33', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -53.27\n",
      "['+0.56', '+0.45', '+1.44', '+0.20', '-0.40', '-0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward -130.73\n",
      "['+0.89', '+0.53', '+1.72', '+0.07', '-0.50', '-0.10', '+0.00', '+0.00']\n",
      "step 100 total_reward -202.19\n",
      "['+1.01', '+0.52', '+1.72', '-0.11', '-0.54', '-0.10', '+0.00', '+0.00']\n",
      "step 107 total_reward -314.15\n",
      "['-0.02', '+0.94', '-0.80', '-0.09', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.91\n",
      "['-0.17', '+0.83', '-0.80', '-0.63', '+0.20', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.52\n",
      "['-0.35', '+0.65', '-1.11', '-0.47', '+0.39', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -63.53\n",
      "['-0.64', '+0.54', '-1.61', '-0.38', '+0.57', '+0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -140.21\n",
      "['-0.96', '+0.35', '-1.61', '-0.91', '+0.73', '+0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -194.72\n",
      "['-1.01', '+0.30', '-1.61', '-0.99', '+0.76', '+0.17', '+0.00', '+0.00']\n",
      "step 83 total_reward -301.19\n",
      "['-0.01', '+0.94', '-0.73', '+0.06', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.56\n",
      "['-0.16', '+0.88', '-0.73', '-0.47', '+0.18', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.42\n",
      "['-0.31', '+0.69', '-0.92', '-0.55', '+0.34', '+0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -52.99\n",
      "['-0.55', '+0.58', '-1.49', '-0.23', '+0.54', '+0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -125.52\n",
      "['-0.85', '+0.43', '-1.49', '-0.76', '+0.73', '+0.20', '+0.00', '+0.00']\n",
      "step 80 total_reward -176.58\n",
      "['-1.01', '+0.28', '-1.49', '-1.06', '+0.84', '+0.20', '+0.00', '+0.00']\n",
      "step 91 total_reward -308.86\n",
      "['-0.01', '+0.95', '-0.70', '+0.16', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.32\n",
      "['-0.15', '+0.91', '-0.70', '-0.37', '+0.17', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.25\n",
      "['-0.29', '+0.73', '-0.82', '-0.61', '+0.33', '+0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.26\n",
      "['-0.52', '+0.60', '-1.47', '-0.22', '+0.47', '+0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -116.60\n",
      "['-0.83', '+0.47', '-1.54', '-0.69', '+0.58', '+0.11', '+0.00', '+0.00']\n",
      "step 80 total_reward -163.46\n",
      "['-1.01', '+0.32', '-1.54', '-1.01', '+0.65', '+0.11', '+0.00', '+0.00']\n",
      "step 92 total_reward -293.14\n",
      "['-0.01', '+0.94', '-0.53', '+0.06', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.27\n",
      "['-0.11', '+0.88', '-0.53', '-0.48', '+0.13', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.19\n",
      "['-0.23', '+0.68', '-0.68', '-0.59', '+0.24', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.23\n",
      "['-0.40', '+0.56', '-1.03', '-0.31', '+0.35', '+0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -71.27\n",
      "['-0.65', '+0.50', '-1.36', '-0.28', '+0.46', '+0.12', '+0.00', '+0.00']\n",
      "step 80 total_reward -131.59\n",
      "['-0.92', '+0.34', '-1.36', '-0.81', '+0.58', '+0.12', '+0.00', '+0.00']\n",
      "step 100 total_reward -178.70\n",
      "['-1.00', '+0.25', '-1.36', '-0.97', '+0.62', '+0.12', '+0.00', '+0.00']\n",
      "step 106 total_reward -293.30\n",
      "['+0.00', '+0.95', '+0.10', '+0.22', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.92\n",
      "['+0.02', '+0.93', '+0.10', '-0.31', '-0.02', '-0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -6.91\n",
      "['+0.04', '+0.75', '+0.10', '-0.85', '-0.05', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.93\n",
      "['+0.08', '+0.56', '+0.21', '-0.45', '-0.03', '+0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward +6.58\n",
      "['+0.12', '+0.47', '+0.16', '-0.15', '-0.03', '-0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward +36.40\n",
      "['+0.15', '+0.48', '+0.25', '+0.21', '-0.07', '-0.03', '+0.00', '+0.00']\n",
      "step 100 total_reward +14.11\n",
      "['+0.23', '+0.60', '+0.41', '+0.61', '-0.03', '+0.06', '+0.00', '+0.00']\n",
      "step 120 total_reward -43.99\n",
      "['+0.30', '+0.85', '+0.38', '+1.04', '-0.01', '+0.00', '+0.00', '+0.00']\n",
      "step 140 total_reward -110.29\n",
      "['+0.38', '+1.11', '+0.38', '+0.62', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 160 total_reward -100.05\n",
      "['+0.46', '+1.21', '+0.38', '+0.09', '+0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 180 total_reward -78.36\n",
      "['+0.53', '+1.16', '+0.38', '-0.44', '+0.01', '+0.00', '+0.00', '+0.00']\n",
      "step 200 total_reward -95.78\n",
      "['+0.61', '+0.94', '+0.38', '-0.98', '+0.01', '+0.00', '+0.00', '+0.00']\n",
      "step 220 total_reward -127.27\n",
      "['+0.69', '+0.68', '+0.37', '-0.68', '+0.02', '+0.01', '+0.00', '+0.00']\n",
      "step 240 total_reward -92.39\n",
      "['+0.77', '+0.56', '+0.36', '-0.20', '+0.08', '+0.07', '+0.00', '+0.00']\n",
      "step 260 total_reward -66.18\n",
      "['+0.84', '+0.53', '+0.33', '-0.23', '+0.20', '+0.12', '+0.00', '+0.00']\n",
      "step 280 total_reward -83.13\n",
      "['+0.90', '+0.37', '+0.33', '-0.76', '+0.32', '+0.12', '+0.00', '+0.00']\n",
      "step 300 total_reward -137.17\n",
      "['+0.97', '+0.06', '+0.33', '-1.30', '+0.44', '+0.12', '+0.00', '+0.00']\n",
      "step 320 total_reward -199.34\n",
      "['+1.00', '-0.09', '+0.43', '-0.56', '+0.25', '-4.96', '+0.00', '+1.00']\n",
      "step 328 total_reward -307.20\n",
      "['+0.01', '+0.94', '+0.51', '-0.14', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.91\n",
      "['+0.11', '+0.81', '+0.51', '-0.67', '-0.13', '-0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.04\n",
      "['+0.22', '+0.60', '+0.70', '-0.54', '-0.25', '-0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -34.59\n",
      "['+0.41', '+0.49', '+1.17', '-0.16', '-0.36', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -81.38\n",
      "['+0.69', '+0.47', '+1.73', '+0.07', '-0.43', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -169.57\n",
      "['+1.00', '+0.43', '+1.73', '-0.39', '-0.47', '-0.04', '+0.00', '+0.00']\n",
      "step 98 total_reward -301.17\n",
      "['-0.00', '+0.94', '-0.15', '+0.03', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.47\n",
      "['-0.03', '+0.87', '-0.15', '-0.51', '+0.04', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.19\n",
      "['-0.06', '+0.66', '-0.21', '-0.62', '+0.06', '+0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -31.32\n",
      "['-0.12', '+0.54', '-0.35', '-0.21', '+0.04', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward +1.37\n",
      "['-0.20', '+0.53', '-0.40', '+0.13', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -4.74\n",
      "['-0.27', '+0.63', '-0.32', '+0.50', '-0.08', '-0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -46.90\n",
      "['-0.32', '+0.83', '-0.12', '+0.80', '-0.10', '+0.01', '+0.00', '+0.00']\n",
      "step 120 total_reward -96.82\n",
      "['-0.34', '+0.99', '-0.12', '+0.27', '-0.09', '+0.01', '+0.00', '+0.00']\n",
      "step 140 total_reward -59.40\n",
      "['-0.37', '+0.98', '-0.12', '-0.27', '-0.08', '+0.01', '+0.00', '+0.00']\n",
      "step 160 total_reward -58.52\n",
      "['-0.39', '+0.82', '-0.12', '-0.80', '-0.06', '+0.01', '+0.00', '+0.00']\n",
      "step 180 total_reward -94.50\n",
      "['-0.41', '+0.64', '-0.09', '-0.39', '-0.08', '-0.04', '+0.00', '+0.00']\n",
      "step 200 total_reward -46.07\n",
      "['-0.41', '+0.59', '+0.03', '+0.05', '-0.10', '-0.05', '+0.00', '+0.00']\n",
      "step 220 total_reward -17.16\n",
      "['-0.40', '+0.66', '+0.14', '+0.36', '-0.17', '-0.10', '+0.00', '+0.00']\n",
      "step 240 total_reward -67.36\n",
      "['-0.34', '+0.83', '+0.43', '+0.75', '-0.28', '-0.12', '+0.00', '+0.00']\n",
      "step 260 total_reward -144.56\n",
      "['-0.25', '+0.98', '+0.44', '+0.26', '-0.41', '-0.13', '+0.00', '+0.00']\n",
      "step 280 total_reward -134.22\n",
      "['-0.17', '+0.98', '+0.44', '-0.27', '-0.54', '-0.13', '+0.00', '+0.00']\n",
      "step 300 total_reward -145.45\n",
      "['-0.08', '+0.81', '+0.44', '-0.80', '-0.67', '-0.13', '+0.00', '+0.00']\n",
      "step 320 total_reward -181.09\n",
      "['+0.05', '+0.54', '+1.04', '-0.92', '-0.79', '-0.11', '+0.00', '+0.00']\n",
      "step 340 total_reward -217.93\n",
      "['+0.35', '+0.26', '+1.93', '-0.90', '-0.89', '-0.12', '+0.00', '+0.00']\n",
      "step 360 total_reward -296.99\n",
      "['+0.58', '+0.10', '+1.33', '+0.24', '-1.81', '-3.97', '+1.00', '+0.00']\n",
      "step 373 total_reward -439.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.00', '+0.95', '+0.02', '+0.15', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.29\n",
      "['+0.01', '+0.91', '+0.02', '-0.38', '-0.01', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.46\n",
      "['+0.01', '+0.71', '+0.04', '-0.81', '-0.01', '+0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.28\n",
      "['+0.02', '+0.55', '-0.01', '-0.28', '-0.02', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward +20.47\n",
      "['+0.03', '+0.54', '+0.12', '+0.22', '-0.04', '+0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward +16.43\n",
      "['+0.06', '+0.66', '+0.25', '+0.61', '-0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 100 total_reward -39.35\n",
      "['+0.11', '+0.86', '+0.22', '+0.53', '+0.06', '+0.07', '+0.00', '+0.00']\n",
      "step 120 total_reward -59.81\n",
      "['+0.15', '+0.94', '+0.22', '-0.01', '+0.13', '+0.07', '+0.00', '+0.00']\n",
      "step 140 total_reward -39.61\n",
      "['+0.20', '+0.85', '+0.22', '-0.54', '+0.19', '+0.07', '+0.00', '+0.00']\n",
      "step 160 total_reward -74.77\n",
      "['+0.24', '+0.65', '+0.04', '-0.54', '+0.26', '+0.08', '+0.00', '+0.00']\n",
      "step 180 total_reward -62.77\n",
      "['+0.19', '+0.56', '-0.48', '-0.07', '+0.32', '+0.03', '+0.00', '+0.00']\n",
      "step 200 total_reward -58.43\n",
      "['+0.05', '+0.58', '-0.96', '+0.20', '+0.30', '-0.05', '+0.00', '+0.00']\n",
      "step 220 total_reward -110.71\n",
      "['-0.20', '+0.69', '-1.38', '+0.53', '+0.19', '-0.13', '+0.00', '+0.00']\n",
      "step 240 total_reward -169.21\n",
      "['-0.50', '+0.88', '-1.54', '+0.63', '+0.04', '-0.16', '+0.00', '+0.00']\n",
      "step 260 total_reward -207.12\n",
      "['-0.81', '+0.99', '-1.54', '+0.10', '-0.12', '-0.16', '+0.00', '+0.00']\n",
      "step 280 total_reward -229.56\n",
      "['-1.01', '+0.97', '-1.54', '-0.25', '-0.22', '-0.16', '+0.00', '+0.00']\n",
      "step 293 total_reward -351.89\n",
      "['+0.01', '+0.95', '+0.46', '+0.32', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.55\n",
      "['+0.10', '+0.96', '+0.46', '-0.21', '-0.11', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -6.07\n",
      "['+0.19', '+0.82', '+0.46', '-0.75', '-0.22', '-0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.23\n",
      "['+0.31', '+0.59', '+0.82', '-0.59', '-0.28', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -47.63\n",
      "['+0.53', '+0.47', '+1.23', '-0.23', '-0.28', '-0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward -81.15\n",
      "['+0.82', '+0.46', '+1.58', '-0.01', '-0.26', '+0.02', '+0.00', '+0.00']\n",
      "step 100 total_reward -140.68\n",
      "['+1.01', '+0.42', '+1.58', '-0.33', '-0.25', '+0.02', '+0.00', '+0.00']\n",
      "step 112 total_reward -257.04\n",
      "['-0.00', '+0.94', '-0.19', '-0.18', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.62\n",
      "['-0.04', '+0.80', '-0.19', '-0.71', '+0.05', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.92\n",
      "['-0.08', '+0.61', '-0.27', '-0.43', '+0.08', '+0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -7.38\n",
      "['-0.16', '+0.55', '-0.43', '+0.01', '+0.09', '+0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -2.47\n",
      "['-0.27', '+0.63', '-0.58', '+0.49', '+0.06', '-0.02', '+0.00', '+0.00']\n",
      "step 80 total_reward -48.87\n",
      "['-0.39', '+0.84', '-0.61', '+0.84', '+0.04', '+0.01', '+0.00', '+0.00']\n",
      "step 100 total_reward -105.31\n",
      "['-0.51', '+1.01', '-0.61', '+0.31', '+0.05', '+0.01', '+0.00', '+0.00']\n",
      "step 120 total_reward -91.05\n",
      "['-0.64', '+1.02', '-0.61', '-0.22', '+0.05', '+0.01', '+0.00', '+0.00']\n",
      "step 140 total_reward -95.14\n",
      "['-0.76', '+0.86', '-0.61', '-0.76', '+0.06', '+0.01', '+0.00', '+0.00']\n",
      "step 160 total_reward -123.20\n",
      "['-0.88', '+0.55', '-0.61', '-1.29', '+0.07', '+0.01', '+0.00', '+0.00']\n",
      "step 180 total_reward -158.42\n",
      "['-1.00', '+0.08', '-0.61', '-1.82', '+0.08', '+0.01', '+0.00', '+0.00']\n",
      "step 200 total_reward -302.41\n",
      "['+0.00', '+0.94', '+0.19', '-0.03', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.33\n",
      "['+0.04', '+0.85', '+0.20', '-0.56', '-0.05', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.19\n",
      "['+0.08', '+0.64', '+0.25', '-0.60', '-0.09', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -28.34\n",
      "['+0.15', '+0.50', '+0.36', '-0.27', '-0.14', '-0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -7.46\n",
      "['+0.24', '+0.49', '+0.62', '+0.18', '-0.26', '-0.11', '+0.00', '+0.00']\n",
      "step 80 total_reward -46.62\n",
      "['+0.40', '+0.61', '+1.03', '+0.53', '-0.40', '-0.14', '+0.00', '+0.00']\n",
      "step 100 total_reward -135.76\n",
      "['+0.66', '+0.82', '+1.64', '+0.90', '-0.58', '-0.21', '+0.00', '+0.00']\n",
      "step 120 total_reward -263.99\n",
      "['+1.00', '+1.03', '+1.69', '+0.43', '-0.81', '-0.23', '+0.00', '+0.00']\n",
      "step 140 total_reward -312.28\n",
      "['+1.01', '+1.03', '+1.69', '+0.40', '-0.82', '-0.23', '+0.00', '+0.00']\n",
      "step 141 total_reward -412.28\n",
      "['-0.01', '+0.92', '-0.26', '-0.54', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.77\n",
      "['-0.06', '+0.69', '-0.33', '-0.87', '+0.06', '+0.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.66\n",
      "['-0.13', '+0.49', '-0.44', '-0.48', '+0.05', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward +22.82\n",
      "['-0.23', '+0.40', '-0.46', '-0.19', '+0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward +41.53\n",
      "['-0.32', '+0.42', '-0.48', '+0.28', '-0.02', '-0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward +20.95\n",
      "['-0.41', '+0.54', '-0.51', '+0.54', '-0.01', '-0.01', '+0.00', '+0.00']\n",
      "step 100 total_reward -17.18\n",
      "['-0.52', '+0.78', '-0.52', '+0.98', '-0.02', '-0.03', '+0.00', '+0.00']\n",
      "step 120 total_reward -86.36\n",
      "['-0.62', '+1.05', '-0.53', '+0.65', '-0.07', '-0.05', '+0.00', '+0.00']\n",
      "step 140 total_reward -93.76\n",
      "['-0.73', '+1.16', '-0.53', '+0.12', '-0.12', '-0.05', '+0.00', '+0.00']\n",
      "step 160 total_reward -83.81\n",
      "['-0.83', '+1.11', '-0.53', '-0.41', '-0.17', '-0.05', '+0.00', '+0.00']\n",
      "step 180 total_reward -103.73\n",
      "['-0.94', '+0.90', '-0.53', '-0.95', '-0.22', '-0.05', '+0.00', '+0.00']\n",
      "step 200 total_reward -141.40\n",
      "['-1.00', '+0.70', '-0.53', '-1.27', '-0.25', '-0.05', '+0.00', '+0.00']\n",
      "step 212 total_reward -263.20\n",
      "['-0.02', '+0.93', '-0.77', '-0.45', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.37\n",
      "['-0.17', '+0.71', '-0.80', '-0.82', '+0.19', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.01\n",
      "['-0.37', '+0.54', '-1.18', '-0.41', '+0.37', '+0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -51.24\n",
      "['-0.64', '+0.44', '-1.44', '-0.50', '+0.60', '+0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -118.08\n",
      "['-0.93', '+0.20', '-1.44', '-1.04', '+0.86', '+0.26', '+0.00', '+0.00']\n",
      "step 80 total_reward -186.35\n",
      "['-1.00', '+0.12', '-1.44', '-1.17', '+0.92', '+0.26', '+0.00', '+0.00']\n",
      "step 85 total_reward -302.46\n",
      "['+0.01', '+0.95', '+0.61', '+0.31', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.11\n",
      "['+0.13', '+0.96', '+0.61', '-0.23', '-0.15', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.08\n",
      "['+0.25', '+0.81', '+0.61', '-0.76', '-0.29', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -45.95\n",
      "['+0.41', '+0.59', '+1.01', '-0.57', '-0.42', '-0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -70.12\n",
      "['+0.69', '+0.48', '+1.74', '-0.16', '-0.55', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -160.40\n",
      "['+1.00', '+0.38', '+1.76', '-0.61', '-0.70', '-0.16', '+0.00', '+0.00']\n",
      "step 98 total_reward -306.65\n",
      "['-0.01', '+0.95', '-0.29', '+0.38', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.20\n",
      "['-0.06', '+0.98', '-0.29', '-0.15', '+0.07', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward +6.79\n",
      "['-0.12', '+0.85', '-0.29', '-0.68', '+0.14', '+0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -29.07\n",
      "['-0.19', '+0.61', '-0.45', '-0.73', '+0.19', '+0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -27.49\n",
      "['-0.32', '+0.45', '-0.83', '-0.38', '+0.17', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -28.18\n",
      "['-0.50', '+0.33', '-0.94', '-0.58', '+0.10', '-0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward -48.41\n",
      "['-0.70', '+0.17', '-0.98', '-0.63', '+0.06', '-0.04', '+0.00', '+0.00']\n",
      "step 120 total_reward -63.48\n",
      "['-0.90', '-0.10', '-0.82', '-0.62', '-0.12', '-4.12', '+1.00', '+1.00']\n",
      "step 140 total_reward -181.38\n",
      "['-0.01', '+0.94', '-0.75', '-0.19', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.06\n",
      "['-0.16', '+0.79', '-0.75', '-0.72', '+0.18', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.29\n",
      "['-0.35', '+0.62', '-1.16', '-0.38', '+0.33', '+0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -60.61\n",
      "['-0.63', '+0.56', '-1.59', '-0.20', '+0.43', '+0.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -126.34\n",
      "['-0.95', '+0.42', '-1.59', '-0.73', '+0.53', '+0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward -170.41\n",
      "['-1.01', '+0.37', '-1.59', '-0.84', '+0.55', '+0.10', '+0.00', '+0.00']\n",
      "step 84 total_reward -278.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.01', '+0.95', '+0.69', '+0.24', '-0.02', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.18\n",
      "['+0.15', '+0.94', '+0.69', '-0.30', '-0.17', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.74\n",
      "['+0.29', '+0.77', '+0.76', '-0.68', '-0.32', '-0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.19\n",
      "['+0.49', '+0.62', '+1.22', '-0.40', '-0.52', '-0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -96.73\n",
      "['+0.82', '+0.53', '+1.90', '-0.31', '-0.67', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -199.42\n",
      "['+1.00', '+0.47', '+1.90', '-0.57', '-0.74', '-0.15', '+0.00', '+0.00']\n",
      "step 90 total_reward -323.52\n",
      "['+0.00', '+0.95', '+0.12', '+0.45', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.68\n",
      "['+0.03', '+1.01', '+0.12', '-0.08', '-0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward +25.42\n",
      "['+0.05', '+0.90', '+0.12', '-0.62', '-0.06', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -14.62\n",
      "['+0.08', '+0.65', '+0.16', '-0.78', '-0.09', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -11.78\n",
      "['+0.12', '+0.47', '+0.31', '-0.36', '-0.14', '-0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward +24.89\n",
      "['+0.20', '+0.43', '+0.50', '+0.07', '-0.18', '-0.08', '+0.00', '+0.00']\n",
      "step 100 total_reward +13.23\n",
      "['+0.34', '+0.51', '+0.88', '+0.45', '-0.23', '-0.00', '+0.00', '+0.00']\n",
      "step 120 total_reward -59.84\n",
      "['+0.54', '+0.69', '+1.12', '+0.77', '-0.26', '-0.07', '+0.00', '+0.00']\n",
      "step 140 total_reward -132.54\n",
      "['+0.81', '+0.95', '+1.43', '+0.79', '-0.29', '-0.01', '+0.00', '+0.00']\n",
      "step 160 total_reward -203.01\n",
      "['+1.01', '+1.08', '+1.43', '+0.42', '-0.29', '-0.01', '+0.00', '+0.00']\n",
      "step 174 total_reward -310.77\n",
      "['-0.01', '+0.92', '-0.29', '-0.55', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.73\n",
      "['-0.07', '+0.69', '-0.32', '-0.86', '+0.07', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.80\n",
      "['-0.15', '+0.50', '-0.48', '-0.41', '+0.10', '+0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward +20.98\n",
      "['-0.25', '+0.44', '-0.65', '-0.06', '+0.17', '+0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward +7.78\n",
      "['-0.41', '+0.45', '-0.87', '-0.09', '+0.34', '+0.48', '+0.00', '+0.00']\n",
      "step 80 total_reward -44.72\n",
      "['-0.58', '+0.34', '-0.92', '-0.64', '+1.02', '+0.71', '+0.00', '+0.00']\n",
      "step 100 total_reward -144.60\n",
      "['-0.76', '+0.07', '-0.91', '-1.17', '+1.73', '+0.71', '+0.00', '+0.00']\n",
      "step 120 total_reward -261.69\n",
      "['-0.87', '-0.13', '-0.70', '-0.10', '+2.06', '-1.83', '+0.00', '+1.00']\n",
      "step 131 total_reward -428.60\n",
      "['-0.01', '+0.95', '-0.52', '+0.45', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.51\n",
      "['-0.11', '+1.00', '-0.52', '-0.09', '+0.13', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -0.79\n",
      "['-0.21', '+0.89', '-0.52', '-0.62', '+0.24', '+0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -31.62\n",
      "['-0.33', '+0.67', '-0.76', '-0.66', '+0.37', '+0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -49.69\n",
      "['-0.53', '+0.51', '-1.35', '-0.42', '+0.48', '+0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -107.22\n",
      "['-0.80', '+0.30', '-1.35', '-0.95', '+0.56', '+0.07', '+0.00', '+0.00']\n",
      "step 100 total_reward -150.31\n",
      "['-1.01', '+0.04', '-1.35', '-1.35', '+0.61', '+0.07', '+0.00', '+0.00']\n",
      "step 115 total_reward -292.98\n",
      "['+0.00', '+0.95', '+0.21', '+0.45', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.44\n",
      "['+0.05', '+1.01', '+0.21', '-0.08', '-0.05', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward +18.72\n",
      "['+0.09', '+0.90', '+0.21', '-0.62', '-0.10', '-0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -17.87\n",
      "['+0.13', '+0.66', '+0.32', '-0.74', '-0.15', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -17.31\n",
      "['+0.22', '+0.50', '+0.57', '-0.33', '-0.16', '-0.00', '+0.00', '+0.00']\n",
      "step 80 total_reward +1.87\n",
      "['+0.36', '+0.46', '+0.80', '+0.03', '-0.16', '+0.01', '+0.00', '+0.00']\n",
      "step 100 total_reward -21.79\n",
      "['+0.55', '+0.52', '+1.05', '+0.37', '-0.13', '+0.06', '+0.00', '+0.00']\n",
      "step 120 total_reward -73.33\n",
      "['+0.77', '+0.68', '+1.12', '+0.66', '-0.07', '+0.01', '+0.00', '+0.00']\n",
      "step 140 total_reward -118.53\n",
      "['+1.00', '+0.82', '+1.14', '+0.21', '-0.05', '+0.02', '+0.00', '+0.00']\n",
      "step 160 total_reward -129.27\n",
      "['+1.01', '+0.82', '+1.14', '+0.19', '-0.05', '+0.02', '+0.00', '+0.00']\n",
      "step 161 total_reward -229.27\n",
      "['-0.00', '+0.93', '-0.11', '-0.36', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.03\n",
      "['-0.02', '+0.74', '-0.10', '-0.86', '+0.03', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.58\n",
      "['-0.04', '+0.53', '-0.12', '-0.55', '+0.09', '+0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward +3.51\n",
      "['-0.08', '+0.42', '-0.32', '-0.17', '+0.19', '+0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward +17.48\n",
      "['-0.17', '+0.43', '-0.64', '+0.16', '+0.24', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -27.37\n",
      "['-0.33', '+0.54', '-0.95', '+0.51', '+0.31', '+0.09', '+0.00', '+0.00']\n",
      "step 100 total_reward -98.58\n",
      "['-0.57', '+0.75', '-1.45', '+0.85', '+0.44', '+0.10', '+0.00', '+0.00']\n",
      "step 120 total_reward -207.99\n",
      "['-0.87', '+0.94', '-1.53', '+0.38', '+0.51', '+0.07', '+0.00', '+0.00']\n",
      "step 140 total_reward -238.97\n",
      "['-1.01', '+0.97', '-1.53', '+0.14', '+0.54', '+0.07', '+0.00', '+0.00']\n",
      "step 149 total_reward -348.79\n",
      "['+0.01', '+0.93', '+0.68', '-0.24', '-0.02', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.13\n",
      "['+0.15', '+0.78', '+0.68', '-0.77', '-0.17', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.00\n",
      "['+0.30', '+0.60', '+0.94', '-0.45', '-0.36', '-0.23', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.83\n",
      "['+0.55', '+0.50', '+1.58', '-0.18', '-0.56', '-0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -135.94\n",
      "['+0.92', '+0.45', '+1.97', '-0.34', '-0.77', '-0.20', '+0.00', '+0.00']\n",
      "step 80 total_reward -228.42\n",
      "['+1.00', '+0.42', '+1.97', '-0.45', '-0.81', '-0.20', '+0.00', '+0.00']\n",
      "step 84 total_reward -337.57\n",
      "['-0.01', '+0.95', '-0.64', '+0.44', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.17\n",
      "['-0.14', '+1.00', '-0.64', '-0.09', '+0.16', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.02\n",
      "['-0.27', '+0.89', '-0.64', '-0.63', '+0.30', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.99\n",
      "['-0.42', '+0.66', '-1.00', '-0.71', '+0.42', '+0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -71.26\n",
      "['-0.65', '+0.44', '-1.23', '-0.93', '+0.48', '+0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -112.27\n",
      "['-0.84', '+0.19', '-1.25', '-0.64', '+0.42', '-4.06', '+0.00', '+1.00']\n",
      "step 95 total_reward -228.83\n",
      "['-0.01', '+0.95', '-0.58', '+0.16', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.09\n",
      "['-0.13', '+0.91', '-0.58', '-0.38', '+0.14', '+0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.35\n",
      "['-0.24', '+0.73', '-0.71', '-0.65', '+0.27', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.96\n",
      "['-0.42', '+0.59', '-1.13', '-0.32', '+0.39', '+0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -81.06\n",
      "['-0.68', '+0.51', '-1.35', '-0.45', '+0.53', '+0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -135.27\n",
      "['-0.95', '+0.29', '-1.35', '-0.98', '+0.68', '+0.15', '+0.00', '+0.00']\n",
      "step 100 total_reward -189.27\n",
      "['-1.01', '+0.23', '-1.35', '-1.09', '+0.71', '+0.15', '+0.00', '+0.00']\n",
      "step 104 total_reward -298.99\n",
      "['-0.01', '+0.93', '-0.59', '-0.45', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.45\n",
      "['-0.13', '+0.71', '-0.59', '-0.88', '+0.15', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.30\n",
      "['-0.26', '+0.51', '-0.80', '-0.48', '+0.36', '+0.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -26.70\n",
      "['-0.47', '+0.40', '-1.19', '-0.49', '+0.71', '+0.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -105.36\n",
      "['-0.71', '+0.18', '-1.19', '-1.03', '+1.17', '+0.46', '+0.00', '+0.00']\n",
      "step 80 total_reward -190.50\n",
      "['-0.88', '+0.01', '-1.50', '-0.46', '+2.86', '+4.29', '+0.00', '+0.00']\n",
      "step 93 total_reward -438.06\n",
      "['+0.00', '+0.94', '+0.25', '+0.01', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.07\n",
      "['+0.05', '+0.86', '+0.25', '-0.53', '-0.06', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.97\n",
      "['+0.11', '+0.66', '+0.31', '-0.54', '-0.13', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -25.65\n",
      "['+0.18', '+0.54', '+0.47', '-0.28', '-0.24', '-0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -24.29\n",
      "['+0.31', '+0.51', '+0.87', '+0.04', '-0.37', '-0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -79.13\n",
      "['+0.54', '+0.56', '+1.47', '+0.31', '-0.49', '-0.10', '+0.00', '+0.00']\n",
      "step 100 total_reward -179.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.89', '+0.66', '+1.78', '+0.17', '-0.60', '-0.11', '+0.00', '+0.00']\n",
      "step 120 total_reward -254.16\n",
      "['+1.01', '+0.67', '+1.78', '-0.01', '-0.64', '-0.11', '+0.00', '+0.00']\n",
      "step 127 total_reward -365.88\n",
      "['-0.00', '+0.93', '-0.24', '-0.49', '+0.01', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.81\n",
      "['-0.05', '+0.70', '-0.28', '-0.88', '+0.06', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -23.35\n",
      "['-0.11', '+0.50', '-0.35', '-0.49', '+0.15', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward +12.44\n",
      "['-0.21', '+0.41', '-0.64', '-0.12', '+0.21', '+0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward +0.81\n",
      "['-0.38', '+0.45', '-0.98', '+0.25', '+0.26', '+0.18', '+0.00', '+0.00']\n",
      "step 80 total_reward -59.00\n",
      "['-0.58', '+0.44', '-1.09', '-0.31', '+0.79', '+0.64', '+0.00', '+0.00']\n",
      "step 100 total_reward -137.58\n",
      "['-0.80', '+0.27', '-1.09', '-0.85', '+1.43', '+0.64', '+0.00', '+0.00']\n",
      "step 120 total_reward -237.18\n",
      "['-0.88', '+0.17', '+0.07', '-0.04', '+1.64', '-0.27', '+0.00', '+1.00']\n",
      "step 128 total_reward -376.56\n",
      "['+0.01', '+0.95', '+0.70', '+0.38', '-0.02', '-0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.01\n",
      "['+0.15', '+0.98', '+0.70', '-0.15', '-0.17', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.64\n",
      "['+0.29', '+0.86', '+0.70', '-0.68', '-0.33', '-0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.31\n",
      "['+0.46', '+0.67', '+1.08', '-0.55', '-0.52', '-0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -83.34\n",
      "['+0.76', '+0.55', '+1.90', '-0.30', '-0.76', '-0.25', '+0.00', '+0.00']\n",
      "step 80 total_reward -196.55\n",
      "['+1.01', '+0.46', '+1.96', '-0.61', '-0.92', '-0.24', '+0.00', '+0.00']\n",
      "step 93 total_reward -340.43\n",
      "['-0.01', '+0.92', '-0.75', '-0.58', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['-0.17', '+0.68', '-0.82', '-0.86', '+0.18', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.17\n",
      "['-0.37', '+0.47', '-1.24', '-0.55', '+0.32', '+0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.23\n",
      "['-0.62', '+0.22', '-1.28', '-1.10', '+0.57', '+0.27', '+0.00', '+0.00']\n",
      "step 60 total_reward -110.57\n",
      "['-0.72', '+0.08', '-0.63', '+0.10', '+0.72', '+1.79', '+0.00', '+1.00']\n",
      "step 68 total_reward -227.24\n",
      "['-0.01', '+0.95', '-0.33', '+0.33', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.03\n",
      "['-0.07', '+0.97', '-0.33', '-0.20', '+0.08', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -0.02\n",
      "['-0.14', '+0.82', '-0.33', '-0.74', '+0.15', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.82\n",
      "['-0.22', '+0.60', '-0.54', '-0.62', '+0.22', '+0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -29.50\n",
      "['-0.36', '+0.46', '-0.82', '-0.49', '+0.31', '+0.25', '+0.00', '+0.00']\n",
      "step 80 total_reward -51.55\n",
      "['-0.53', '+0.23', '-1.00', '-0.95', '+0.77', '+0.50', '+0.00', '+0.00']\n",
      "step 100 total_reward -140.20\n",
      "['-0.76', '-0.10', '-1.30', '-1.34', '+1.28', '+0.51', '+0.00', '+1.00']\n",
      "step 120 total_reward -249.73\n",
      "['-0.80', '-0.13', '-0.97', '+0.19', '+1.67', '+0.86', '+0.00', '+1.00']\n",
      "step 123 total_reward -348.90\n",
      "['-0.01', '+0.93', '-0.74', '-0.49', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.41\n",
      "['-0.16', '+0.70', '-0.78', '-0.87', '+0.18', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.51\n",
      "['-0.35', '+0.50', '-1.14', '-0.50', '+0.37', '+0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.00\n",
      "['-0.60', '+0.31', '-1.28', '-0.90', '+0.70', '+0.38', '+0.00', '+0.00']\n",
      "step 60 total_reward -119.88\n",
      "['-0.80', '+0.04', '-1.17', '-0.09', '+1.03', '+3.26', '+0.00', '+1.00']\n",
      "step 76 total_reward -275.74\n",
      "['-0.00', '+0.95', '-0.06', '+0.14', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.13\n",
      "['-0.01', '+0.90', '-0.06', '-0.39', '+0.01', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.83\n",
      "['-0.02', '+0.71', '-0.06', '-0.76', '+0.03', '+0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.92\n",
      "['-0.04', '+0.54', '-0.06', '-0.34', '+0.06', '+0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward +11.79\n",
      "['-0.06', '+0.51', '-0.17', '+0.12', '+0.15', '+0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward +12.07\n",
      "['-0.12', '+0.61', '-0.49', '+0.53', '+0.22', '+0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -62.75\n",
      "['-0.25', '+0.80', '-0.79', '+0.77', '+0.27', '+0.04', '+0.00', '+0.00']\n",
      "step 120 total_reward -133.64\n",
      "['-0.40', '+0.95', '-0.79', '+0.24', '+0.32', '+0.04', '+0.00', '+0.00']\n",
      "step 140 total_reward -129.34\n",
      "['-0.56', '+0.94', '-0.79', '-0.30', '+0.36', '+0.04', '+0.00', '+0.00']\n",
      "step 160 total_reward -141.69\n",
      "['-0.72', '+0.76', '-0.79', '-0.83', '+0.41', '+0.04', '+0.00', '+0.00']\n",
      "step 180 total_reward -172.02\n",
      "['-0.87', '+0.43', '-0.79', '-1.36', '+0.45', '+0.04', '+0.00', '+0.00']\n",
      "step 200 total_reward -212.16\n",
      "['-1.01', '+0.02', '-0.79', '-1.82', '+0.49', '+0.04', '+0.00', '+0.00']\n",
      "step 217 total_reward -356.46\n",
      "['+0.02', '+0.95', '+0.78', '+0.26', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.33\n",
      "['+0.17', '+0.94', '+0.78', '-0.27', '-0.19', '-0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.31\n",
      "['+0.33', '+0.78', '+0.91', '-0.59', '-0.37', '-0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -53.10\n",
      "['+0.57', '+0.66', '+1.54', '-0.26', '-0.53', '-0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -124.53\n",
      "['+0.93', '+0.58', '+1.84', '-0.45', '-0.70', '-0.18', '+0.00', '+0.00']\n",
      "step 80 total_reward -200.22\n",
      "['+1.02', '+0.54', '+1.84', '-0.58', '-0.75', '-0.18', '+0.00', '+0.00']\n",
      "step 85 total_reward -311.38\n",
      "['-0.01', '+0.95', '-0.49', '+0.46', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.59\n",
      "['-0.11', '+1.01', '-0.49', '-0.07', '+0.12', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward +1.54\n",
      "['-0.20', '+0.91', '-0.49', '-0.60', '+0.23', '+0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -28.87\n",
      "['-0.31', '+0.69', '-0.66', '-0.65', '+0.35', '+0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -42.19\n",
      "['-0.50', '+0.55', '-1.28', '-0.31', '+0.51', '+0.14', '+0.00', '+0.00']\n",
      "step 80 total_reward -101.96\n",
      "['-0.78', '+0.40', '-1.37', '-0.75', '+0.64', '+0.13', '+0.00', '+0.00']\n",
      "step 100 total_reward -153.59\n",
      "['-1.01', '+0.15', '-1.37', '-1.20', '+0.75', '+0.13', '+0.00', '+0.00']\n",
      "step 117 total_reward -301.81\n",
      "['+0.02', '+0.93', '+0.76', '-0.48', '-0.02', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.40\n",
      "['+0.17', '+0.71', '+0.81', '-0.83', '-0.19', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.35\n",
      "['+0.36', '+0.51', '+1.12', '-0.47', '-0.39', '-0.23', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.09\n",
      "['+0.64', '+0.42', '+1.80', '-0.18', '-0.62', '-0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -150.05\n",
      "['+1.01', '+0.36', '+2.07', '-0.44', '-0.81', '-0.21', '+0.00', '+0.00']\n",
      "step 78 total_reward -327.72\n",
      "['+0.01', '+0.94', '+0.74', '+0.00', '-0.02', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.69\n",
      "['+0.16', '+0.86', '+0.74', '-0.53', '-0.18', '-0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.52\n",
      "['+0.32', '+0.66', '+0.98', '-0.60', '-0.34', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -56.77\n",
      "['+0.57', '+0.53', '+1.52', '-0.29', '-0.49', '-0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -122.40\n",
      "['+0.92', '+0.44', '+1.83', '-0.48', '-0.64', '-0.14', '+0.00', '+0.00']\n",
      "step 80 total_reward -198.74\n",
      "['+1.01', '+0.40', '+1.83', '-0.61', '-0.68', '-0.14', '+0.00', '+0.00']\n",
      "step 85 total_reward -309.96\n",
      "['+0.01', '+0.95', '+0.56', '+0.24', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.12\n",
      "['+0.12', '+0.94', '+0.56', '-0.30', '-0.14', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.54\n",
      "['+0.23', '+0.76', '+0.58', '-0.79', '-0.26', '-0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.52\n",
      "['+0.40', '+0.59', '+1.04', '-0.42', '-0.37', '-0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -68.38\n",
      "['+0.66', '+0.52', '+1.62', '-0.06', '-0.52', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -152.80\n",
      "['+1.01', '+0.49', '+1.90', '-0.29', '-0.65', '-0.13', '+0.00', '+0.00']\n",
      "step 99 total_reward -323.08\n",
      "['+0.01', '+0.92', '+0.58', '-0.56', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.52\n",
      "['+0.13', '+0.69', '+0.69', '-0.76', '-0.13', '-0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.49\n",
      "['+0.29', '+0.52', '+0.97', '-0.34', '-0.20', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -17.20\n",
      "['+0.51', '+0.46', '+1.21', '-0.07', '-0.24', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -55.74\n",
      "['+0.79', '+0.50', '+1.52', '+0.17', '-0.31', '-0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -123.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+1.00', '+0.50', '+1.52', '-0.20', '-0.36', '-0.07', '+0.00', '+0.00']\n",
      "step 94 total_reward -245.18\n",
      "['+0.02', '+0.94', '+0.80', '+0.09', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.61\n",
      "['+0.17', '+0.89', '+0.80', '-0.45', '-0.20', '-0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.59\n",
      "['+0.34', '+0.69', '+0.98', '-0.57', '-0.38', '-0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -55.10\n",
      "['+0.60', '+0.57', '+1.64', '-0.25', '-0.58', '-0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -139.90\n",
      "['+0.98', '+0.50', '+1.97', '-0.43', '-0.84', '-0.26', '+0.00', '+0.00']\n",
      "step 80 total_reward -230.07\n",
      "['+1.02', '+0.49', '+1.97', '-0.48', '-0.86', '-0.26', '+0.00', '+0.00']\n",
      "step 82 total_reward -333.39\n",
      "['-0.01', '+0.94', '-0.71', '-0.19', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.05\n",
      "['-0.16', '+0.79', '-0.71', '-0.73', '+0.18', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.25\n",
      "['-0.33', '+0.60', '-1.09', '-0.47', '+0.30', '+0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -55.30\n",
      "['-0.59', '+0.50', '-1.44', '-0.35', '+0.39', '+0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -107.36\n",
      "['-0.88', '+0.31', '-1.44', '-0.88', '+0.52', '+0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -156.36\n",
      "['-1.01', '+0.18', '-1.44', '-1.12', '+0.57', '+0.13', '+0.00', '+0.00']\n",
      "step 89 total_reward -281.37\n",
      "['-0.02', '+0.94', '-0.81', '-0.16', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.02\n",
      "['-0.18', '+0.81', '-0.81', '-0.69', '+0.20', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.88\n",
      "['-0.36', '+0.62', '-1.15', '-0.45', '+0.38', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -61.41\n",
      "['-0.64', '+0.52', '-1.48', '-0.43', '+0.59', '+0.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -128.30\n",
      "['-0.93', '+0.31', '-1.48', '-0.97', '+0.83', '+0.23', '+0.00', '+0.00']\n",
      "step 80 total_reward -189.82\n",
      "['-1.01', '+0.23', '-1.48', '-1.10', '+0.88', '+0.23', '+0.00', '+0.00']\n",
      "step 85 total_reward -304.44\n",
      "['+0.01', '+0.95', '+0.57', '+0.45', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.37\n",
      "['+0.12', '+1.00', '+0.57', '-0.09', '-0.14', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -3.26\n",
      "['+0.24', '+0.89', '+0.57', '-0.62', '-0.27', '-0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -33.96\n",
      "['+0.36', '+0.67', '+0.83', '-0.68', '-0.39', '-0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -56.33\n",
      "['+0.60', '+0.52', '+1.49', '-0.40', '-0.46', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -119.53\n",
      "['+0.93', '+0.41', '+1.70', '-0.53', '-0.56', '-0.12', '+0.00', '+0.00']\n",
      "step 100 total_reward -178.96\n",
      "['+1.01', '+0.36', '+1.70', '-0.66', '-0.59', '-0.12', '+0.00', '+0.00']\n",
      "step 105 total_reward -289.72\n",
      "['+0.00', '+0.93', '+0.15', '-0.33', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.98\n",
      "['+0.03', '+0.75', '+0.15', '-0.86', '-0.04', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.46\n",
      "['+0.07', '+0.53', '+0.26', '-0.51', '-0.05', '+0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward +7.16\n",
      "['+0.15', '+0.38', '+0.51', '-0.29', '-0.19', '-0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -1.40\n",
      "['+0.28', '+0.35', '+0.82', '-0.04', '-0.34', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -48.50\n",
      "['+0.44', '+0.26', '+0.82', '-0.54', '-0.50', '-0.16', '+0.00', '+0.00']\n",
      "step 100 total_reward -87.87\n",
      "['+0.56', '+0.15', '+0.34', '-0.23', '-1.58', '-2.03', '+0.00', '+0.00']\n",
      "step 120 total_reward -144.62\n",
      "['+0.57', '+0.14', '+0.38', '-0.02', '-1.95', '-0.23', '+0.00', '+0.00']\n",
      "step 124 total_reward -280.45\n",
      "['+0.01', '+0.95', '+0.26', '+0.15', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.88\n",
      "['+0.06', '+0.91', '+0.26', '-0.38', '-0.06', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.58\n",
      "['+0.11', '+0.72', '+0.27', '-0.68', '-0.13', '-0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -33.72\n",
      "['+0.18', '+0.57', '+0.50', '-0.28', '-0.23', '-0.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -21.49\n",
      "['+0.32', '+0.54', '+0.91', '+0.02', '-0.29', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -70.67\n",
      "['+0.55', '+0.60', '+1.35', '+0.37', '-0.29', '+0.00', '+0.00', '+0.00']\n",
      "step 100 total_reward -143.50\n",
      "['+0.86', '+0.74', '+1.61', '+0.38', '-0.27', '+0.02', '+0.00', '+0.00']\n",
      "step 120 total_reward -202.16\n",
      "['+1.00', '+0.77', '+1.61', '+0.14', '-0.25', '+0.02', '+0.00', '+0.00']\n",
      "step 129 total_reward -309.51\n",
      "['+0.01', '+0.92', '+0.55', '-0.55', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.53\n",
      "['+0.12', '+0.69', '+0.58', '-0.85', '-0.14', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.08\n",
      "['+0.26', '+0.50', '+0.86', '-0.42', '-0.31', '-0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -20.52\n",
      "['+0.48', '+0.42', '+1.40', '-0.09', '-0.45', '-0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -93.00\n",
      "['+0.83', '+0.43', '+1.92', '-0.05', '-0.58', '-0.11', '+0.00', '+0.00']\n",
      "step 80 total_reward -191.19\n",
      "['+1.00', '+0.40', '+1.92', '-0.29', '-0.63', '-0.11', '+0.00', '+0.00']\n",
      "step 89 total_reward -310.31\n",
      "['+0.00', '+0.95', '+0.03', '+0.44', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.87\n",
      "['+0.01', '+1.00', '+0.03', '-0.09', '-0.01', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward +30.74\n",
      "['+0.01', '+0.89', '+0.03', '-0.63', '-0.02', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -11.76\n",
      "['+0.02', '+0.64', '+0.02', '-0.78', '-0.03', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -5.39\n",
      "['+0.03', '+0.48', '+0.08', '-0.33', '-0.05', '-0.02', '+0.00', '+0.00']\n",
      "step 80 total_reward +46.94\n",
      "['+0.05', '+0.45', '+0.14', '+0.11', '-0.09', '-0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward +55.60\n",
      "['+0.10', '+0.56', '+0.28', '+0.59', '-0.16', '-0.11', '+0.00', '+0.00']\n",
      "step 120 total_reward -16.28\n",
      "['+0.18', '+0.81', '+0.53', '+0.92', '-0.28', '-0.12', '+0.00', '+0.00']\n",
      "step 140 total_reward -99.93\n",
      "['+0.28', '+1.00', '+0.53', '+0.39', '-0.39', '-0.12', '+0.00', '+0.00']\n",
      "step 160 total_reward -92.28\n",
      "['+0.39', '+1.03', '+0.53', '-0.15', '-0.51', '-0.12', '+0.00', '+0.00']\n",
      "step 180 total_reward -99.79\n",
      "['+0.49', '+0.90', '+0.61', '-0.63', '-0.62', '-0.10', '+0.00', '+0.00']\n",
      "step 200 total_reward -137.46\n",
      "['+0.70', '+0.75', '+1.41', '-0.48', '-0.72', '-0.10', '+0.00', '+0.00']\n",
      "step 220 total_reward -213.10\n",
      "['+0.99', '+0.53', '+1.44', '-0.98', '-0.83', '-0.11', '+0.00', '+0.00']\n",
      "step 240 total_reward -259.49\n",
      "['+1.01', '+0.52', '+1.44', '-1.00', '-0.84', '-0.11', '+0.00', '+0.00']\n",
      "step 241 total_reward -359.49\n",
      "['+0.01', '+0.95', '+0.45', '+0.34', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.63\n",
      "['+0.10', '+0.97', '+0.45', '-0.19', '-0.11', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.04\n",
      "['+0.19', '+0.83', '+0.45', '-0.72', '-0.21', '-0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.00\n",
      "['+0.29', '+0.60', '+0.71', '-0.64', '-0.32', '-0.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -44.79\n",
      "['+0.48', '+0.45', '+1.16', '-0.37', '-0.40', '-0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward -85.13\n",
      "['+0.77', '+0.39', '+1.76', '-0.03', '-0.53', '-0.14', '+0.00', '+0.00']\n",
      "step 100 total_reward -179.07\n",
      "['+1.02', '+0.35', '+1.76', '-0.40', '-0.63', '-0.14', '+0.00', '+0.00']\n",
      "step 114 total_reward -311.49\n",
      "['-0.01', '+0.94', '-0.63', '-0.05', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.58\n",
      "['-0.13', '+0.84', '-0.60', '-0.58', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.45\n",
      "['-0.25', '+0.66', '-0.58', '-0.48', '-0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward +6.14\n",
      "['-0.36', '+0.59', '-0.53', '-0.01', '-0.05', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward +20.85\n",
      "['-0.46', '+0.66', '-0.42', '+0.44', '-0.07', '-0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward -5.72\n",
      "['-0.53', '+0.85', '-0.33', '+0.87', '-0.08', '-0.03', '+0.00', '+0.00']\n",
      "step 100 total_reward -65.18\n",
      "['-0.59', '+1.06', '-0.30', '+0.46', '-0.10', '-0.02', '+0.00', '+0.00']\n",
      "step 120 total_reward -50.89\n",
      "['-0.65', '+1.11', '-0.30', '-0.08', '-0.12', '-0.02', '+0.00', '+0.00']\n",
      "step 140 total_reward -36.96\n",
      "['-0.71', '+1.01', '-0.30', '-0.61', '-0.14', '-0.02', '+0.00', '+0.00']\n",
      "step 160 total_reward -69.91\n",
      "['-0.78', '+0.74', '-0.30', '-1.14', '-0.16', '-0.02', '+0.00', '+0.00']\n",
      "step 180 total_reward -105.60\n",
      "['-0.84', '+0.31', '-0.30', '-1.68', '-0.18', '-0.02', '+0.00', '+0.00']\n",
      "step 200 total_reward -141.70\n",
      "['-0.86', '+0.10', '-0.76', '-0.90', '-0.10', '+6.20', '+1.00', '+0.00']\n",
      "step 208 total_reward -248.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.01', '+0.93', '+0.57', '-0.42', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['+0.12', '+0.72', '+0.58', '-0.89', '-0.14', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.69\n",
      "['+0.26', '+0.51', '+0.79', '-0.58', '-0.28', '-0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -26.16\n",
      "['+0.46', '+0.40', '+1.24', '-0.17', '-0.51', '-0.27', '+0.00', '+0.00']\n",
      "step 60 total_reward -85.83\n",
      "['+0.77', '+0.37', '+1.76', '-0.16', '-0.77', '-0.29', '+0.00', '+0.00']\n",
      "step 80 total_reward -194.15\n",
      "['+1.00', '+0.30', '+1.76', '-0.51', '-0.96', '-0.29', '+0.00', '+0.00']\n",
      "step 93 total_reward -334.53\n",
      "['+0.00', '+0.95', '+0.21', '+0.27', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.46\n",
      "['+0.04', '+0.95', '+0.21', '-0.26', '-0.05', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.89\n",
      "['+0.09', '+0.79', '+0.21', '-0.79', '-0.10', '-0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -39.52\n",
      "['+0.14', '+0.58', '+0.31', '-0.49', '-0.15', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -7.03\n",
      "['+0.22', '+0.51', '+0.57', '-0.05', '-0.27', '-0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -20.01\n",
      "['+0.38', '+0.56', '+1.06', '+0.38', '-0.42', '-0.15', '+0.00', '+0.00']\n",
      "step 100 total_reward -107.63\n",
      "['+0.65', '+0.71', '+1.66', '+0.59', '-0.57', '-0.15', '+0.00', '+0.00']\n",
      "step 120 total_reward -221.05\n",
      "['+1.00', '+0.86', '+1.91', '+0.34', '-0.71', '-0.15', '+0.00', '+0.00']\n",
      "step 139 total_reward -389.37\n",
      "['-0.01', '+0.95', '-0.62', '+0.29', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.07\n",
      "['-0.13', '+0.95', '-0.62', '-0.25', '+0.15', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.32\n",
      "['-0.26', '+0.79', '-0.61', '-0.76', '+0.29', '+0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -45.54\n",
      "['-0.43', '+0.63', '-1.17', '-0.40', '+0.42', '+0.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -84.28\n",
      "['-0.70', '+0.52', '-1.42', '-0.54', '+0.54', '+0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -137.87\n",
      "['-0.99', '+0.27', '-1.42', '-1.07', '+0.67', '+0.13', '+0.00', '+0.00']\n",
      "step 100 total_reward -191.63\n",
      "['-1.00', '+0.25', '-1.42', '-1.10', '+0.67', '+0.13', '+0.00', '+0.00']\n",
      "step 101 total_reward -291.63\n",
      "['-0.01', '+0.94', '-0.74', '+0.07', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.55\n",
      "['-0.16', '+0.88', '-0.74', '-0.46', '+0.18', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.27\n",
      "['-0.32', '+0.71', '-0.98', '-0.47', '+0.35', '+0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.67\n",
      "['-0.57', '+0.64', '-1.52', '-0.08', '+0.56', '+0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -133.30\n",
      "['-0.88', '+0.54', '-1.56', '-0.58', '+0.81', '+0.24', '+0.00', '+0.00']\n",
      "step 80 total_reward -190.29\n",
      "['-1.00', '+0.46', '-1.56', '-0.80', '+0.90', '+0.24', '+0.00', '+0.00']\n",
      "step 88 total_reward -312.33\n",
      "['+0.01', '+0.94', '+0.56', '+0.09', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.21\n",
      "['+0.12', '+0.89', '+0.56', '-0.44', '-0.14', '-0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.40\n",
      "['+0.24', '+0.69', '+0.64', '-0.66', '-0.27', '-0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.89\n",
      "['+0.40', '+0.55', '+1.05', '-0.36', '-0.43', '-0.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -77.71\n",
      "['+0.67', '+0.49', '+1.68', '-0.09', '-0.66', '-0.24', '+0.00', '+0.00']\n",
      "step 80 total_reward -178.70\n",
      "['+1.00', '+0.42', '+1.85', '-0.46', '-0.85', '-0.21', '+0.00', '+0.00']\n",
      "step 98 total_reward -344.48\n",
      "['+0.00', '+0.93', '+0.15', '-0.47', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.89\n",
      "['+0.03', '+0.71', '+0.16', '-0.84', '-0.04', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.06\n",
      "['+0.08', '+0.53', '+0.24', '-0.40', '-0.05', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward +27.29\n",
      "['+0.13', '+0.48', '+0.28', '+0.06', '-0.10', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward +39.03\n",
      "['+0.21', '+0.55', '+0.48', '+0.40', '-0.16', '-0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward -16.30\n",
      "['+0.32', '+0.72', '+0.72', '+0.75', '-0.21', '-0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -88.72\n",
      "['+0.49', '+1.01', '+0.97', '+1.07', '-0.31', '-0.11', '+0.00', '+0.00']\n",
      "step 120 total_reward -177.79\n",
      "['+0.68', '+1.25', '+0.97', '+0.53', '-0.42', '-0.11', '+0.00', '+0.00']\n",
      "step 140 total_reward -185.52\n",
      "['+0.88', '+1.33', '+0.97', '-0.00', '-0.53', '-0.11', '+0.00', '+0.00']\n",
      "step 160 total_reward -199.70\n",
      "['+1.00', '+1.29', '+0.97', '-0.35', '-0.61', '-0.11', '+0.00', '+0.00']\n",
      "step 173 total_reward -315.82\n",
      "['-0.01', '+0.95', '-0.66', '+0.41', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.10\n",
      "['-0.14', '+0.99', '-0.66', '-0.12', '+0.16', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -8.94\n",
      "['-0.27', '+0.87', '-0.66', '-0.65', '+0.31', '+0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.74\n",
      "['-0.43', '+0.66', '-1.07', '-0.59', '+0.45', '+0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -75.73\n",
      "['-0.70', '+0.50', '-1.39', '-0.70', '+0.57', '+0.12', '+0.00', '+0.00']\n",
      "step 80 total_reward -131.66\n",
      "['-0.98', '+0.21', '-1.39', '-1.23', '+0.69', '+0.12', '+0.00', '+0.00']\n",
      "step 100 total_reward -187.91\n",
      "['-1.00', '+0.17', '-1.39', '-1.28', '+0.71', '+0.12', '+0.00', '+0.00']\n",
      "step 102 total_reward -291.29\n",
      "['+0.01', '+0.93', '+0.48', '-0.53', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.58\n",
      "['+0.11', '+0.69', '+0.53', '-0.85', '-0.12', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.93\n",
      "['+0.24', '+0.50', '+0.77', '-0.44', '-0.19', '-0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -7.83\n",
      "['+0.42', '+0.44', '+1.10', '-0.05', '-0.26', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -47.32\n",
      "['+0.69', '+0.51', '+1.53', '+0.44', '-0.32', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -132.87\n",
      "['+1.00', '+0.59', '+1.57', '+0.02', '-0.42', '-0.10', '+0.00', '+0.00']\n",
      "step 100 total_reward -269.55\n",
      "['+0.01', '+0.95', '+0.64', '+0.36', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.10\n",
      "['+0.14', '+0.98', '+0.64', '-0.17', '-0.16', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.57\n",
      "['+0.27', '+0.84', '+0.64', '-0.71', '-0.30', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.49\n",
      "['+0.43', '+0.64', '+1.03', '-0.50', '-0.47', '-0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -72.86\n",
      "['+0.70', '+0.54', '+1.75', '-0.27', '-0.70', '-0.22', '+0.00', '+0.00']\n",
      "step 80 total_reward -175.86\n",
      "['+1.01', '+0.43', '+1.83', '-0.65', '-0.90', '-0.24', '+0.00', '+0.00']\n",
      "step 97 total_reward -331.34\n",
      "['+0.00', '+0.94', '+0.12', '-0.10', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.49\n",
      "['+0.03', '+0.82', '+0.12', '-0.63', '-0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.82\n",
      "['+0.05', '+0.61', '+0.17', '-0.59', '-0.05', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.80\n",
      "['+0.09', '+0.50', '+0.26', '-0.19', '-0.05', '+0.00', '+0.00', '+0.00']\n",
      "step 60 total_reward +9.93\n",
      "['+0.16', '+0.50', '+0.35', '+0.18', '-0.03', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -3.22\n",
      "['+0.22', '+0.62', '+0.30', '+0.57', '-0.03', '-0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -48.12\n",
      "['+0.29', '+0.86', '+0.36', '+0.88', '-0.08', '-0.05', '+0.00', '+0.00']\n",
      "step 120 total_reward -114.01\n",
      "['+0.36', '+1.04', '+0.36', '+0.35', '-0.13', '-0.05', '+0.00', '+0.00']\n",
      "step 140 total_reward -93.30\n",
      "['+0.43', '+1.06', '+0.36', '-0.18', '-0.18', '-0.05', '+0.00', '+0.00']\n",
      "step 160 total_reward -93.00\n",
      "['+0.51', '+0.93', '+0.36', '-0.72', '-0.23', '-0.05', '+0.00', '+0.00']\n",
      "step 180 total_reward -128.12\n",
      "['+0.62', '+0.77', '+0.76', '-0.29', '-0.27', '-0.02', '+0.00', '+0.00']\n",
      "step 200 total_reward -133.42\n",
      "['+0.81', '+0.73', '+1.12', '-0.17', '-0.26', '+0.05', '+0.00', '+0.00']\n",
      "step 220 total_reward -177.93\n",
      "['+1.00', '+0.62', '+1.12', '-0.62', '-0.22', '+0.05', '+0.00', '+0.00']\n",
      "step 237 total_reward -296.04\n",
      "['+0.01', '+0.95', '+0.29', '+0.18', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.85\n",
      "['+0.06', '+0.92', '+0.29', '-0.36', '-0.07', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.74\n",
      "['+0.12', '+0.73', '+0.31', '-0.80', '-0.14', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.82\n",
      "['+0.20', '+0.54', '+0.50', '-0.42', '-0.21', '-0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -20.63\n",
      "['+0.34', '+0.49', '+0.92', '+0.02', '-0.31', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -64.83\n",
      "['+0.58', '+0.55', '+1.43', '+0.37', '-0.38', '-0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward -153.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.89', '+0.66', '+1.59', '+0.18', '-0.46', '-0.09', '+0.00', '+0.00']\n",
      "step 120 total_reward -207.79\n",
      "['+1.01', '+0.67', '+1.59', '-0.03', '-0.50', '-0.09', '+0.00', '+0.00']\n",
      "step 128 total_reward -319.50\n",
      "['-0.01', '+0.96', '-0.73', '+0.48', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.04\n",
      "['-0.16', '+1.02', '-0.74', '-0.05', '+0.18', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -9.60\n",
      "['-0.31', '+0.92', '-0.74', '-0.59', '+0.35', '+0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.09\n",
      "['-0.47', '+0.70', '-1.02', '-0.68', '+0.52', '+0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -77.09\n",
      "['-0.74', '+0.51', '-1.42', '-0.83', '+0.70', '+0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -145.90\n",
      "['-1.00', '+0.20', '-1.42', '-1.34', '+0.86', '+0.16', '+0.00', '+0.00']\n",
      "step 99 total_reward -301.22\n",
      "['+0.01', '+0.95', '+0.66', '+0.29', '-0.01', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.02\n",
      "['+0.14', '+0.95', '+0.66', '-0.24', '-0.16', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.17\n",
      "['+0.27', '+0.80', '+0.66', '-0.77', '-0.31', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.33\n",
      "['+0.45', '+0.61', '+1.18', '-0.45', '-0.47', '-0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -86.35\n",
      "['+0.75', '+0.52', '+1.82', '-0.28', '-0.64', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -181.92\n",
      "['+1.01', '+0.42', '+1.82', '-0.65', '-0.74', '-0.15', '+0.00', '+0.00']\n",
      "step 94 total_reward -316.27\n",
      "['+0.01', '+0.95', '+0.38', '+0.32', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.80\n",
      "['+0.08', '+0.96', '+0.39', '-0.22', '-0.09', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -3.55\n",
      "['+0.16', '+0.81', '+0.39', '-0.75', '-0.18', '-0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.62\n",
      "['+0.27', '+0.62', '+0.70', '-0.46', '-0.24', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -33.30\n",
      "['+0.45', '+0.54', '+1.07', '-0.09', '-0.26', '-0.02', '+0.00', '+0.00']\n",
      "step 80 total_reward -67.35\n",
      "['+0.70', '+0.58', '+1.41', '+0.33', '-0.27', '-0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -132.89\n",
      "['+1.00', '+0.64', '+1.52', '-0.03', '-0.29', '-0.02', '+0.00', '+0.00']\n",
      "step 120 total_reward -270.88\n",
      "['+0.01', '+0.92', '+0.42', '-0.56', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.63\n",
      "['+0.09', '+0.68', '+0.45', '-0.83', '-0.10', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.20\n",
      "['+0.20', '+0.49', '+0.64', '-0.47', '-0.18', '-0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward +3.48\n",
      "['+0.36', '+0.42', '+0.96', '-0.06', '-0.27', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -29.92\n",
      "['+0.60', '+0.46', '+1.40', '+0.25', '-0.32', '-0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -107.58\n",
      "['+0.91', '+0.53', '+1.58', '+0.03', '-0.34', '-0.02', '+0.00', '+0.00']\n",
      "step 100 total_reward -157.48\n",
      "['+1.00', '+0.52', '+1.58', '-0.13', '-0.35', '-0.02', '+0.00', '+0.00']\n",
      "step 106 total_reward -264.96\n",
      "['+0.00', '+0.95', '+0.25', '+0.44', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.33\n",
      "['+0.05', '+1.00', '+0.25', '-0.09', '-0.06', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward +15.19\n",
      "['+0.10', '+0.89', '+0.25', '-0.62', '-0.12', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -20.38\n",
      "['+0.16', '+0.66', '+0.37', '-0.71', '-0.17', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -19.92\n",
      "['+0.26', '+0.50', '+0.60', '-0.37', '-0.23', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -9.94\n",
      "['+0.41', '+0.45', '+0.98', '-0.03', '-0.31', '-0.06', '+0.00', '+0.00']\n",
      "step 100 total_reward -56.10\n",
      "['+0.65', '+0.50', '+1.48', '+0.35', '-0.38', '-0.05', '+0.00', '+0.00']\n",
      "step 120 total_reward -145.42\n",
      "['+0.97', '+0.57', '+1.61', '+0.04', '-0.45', '-0.06', '+0.00', '+0.00']\n",
      "step 140 total_reward -193.15\n",
      "['+1.02', '+0.57', '+1.61', '-0.04', '-0.46', '-0.06', '+0.00', '+0.00']\n",
      "step 143 total_reward -296.51\n",
      "['+0.00', '+0.94', '+0.25', '-0.06', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.63\n",
      "['+0.05', '+0.84', '+0.25', '-0.59', '-0.06', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.91\n",
      "['+0.11', '+0.63', '+0.34', '-0.57', '-0.12', '-0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -25.73\n",
      "['+0.19', '+0.51', '+0.56', '-0.22', '-0.16', '-0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -20.74\n",
      "['+0.33', '+0.49', '+0.79', '+0.03', '-0.18', '+0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward -51.18\n",
      "['+0.50', '+0.56', '+1.00', '+0.41', '-0.22', '-0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward -107.57\n",
      "['+0.74', '+0.73', '+1.33', '+0.69', '-0.23', '+0.00', '+0.00', '+0.00']\n",
      "step 120 total_reward -184.60\n",
      "['+1.01', '+0.86', '+1.33', '+0.16', '-0.22', '+0.00', '+0.00', '+0.00']\n",
      "step 140 total_reward -295.38\n",
      "['-0.02', '+0.95', '-0.79', '+0.40', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.20\n",
      "['-0.17', '+0.99', '-0.79', '-0.13', '+0.19', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.34\n",
      "['-0.33', '+0.87', '-0.79', '-0.66', '+0.37', '+0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.35\n",
      "['-0.52', '+0.67', '-1.28', '-0.55', '+0.56', '+0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -99.47\n",
      "['-0.80', '+0.47', '-1.40', '-0.92', '+0.78', '+0.22', '+0.00', '+0.00']\n",
      "step 80 total_reward -158.05\n",
      "['-1.00', '+0.22', '-1.40', '-1.32', '+0.95', '+0.22', '+0.00', '+0.00']\n",
      "step 95 total_reward -306.06\n",
      "['-0.01', '+0.95', '-0.63', '+0.36', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.13\n",
      "['-0.14', '+0.97', '-0.63', '-0.18', '+0.15', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.17\n",
      "['-0.26', '+0.84', '-0.63', '-0.71', '+0.29', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.15\n",
      "['-0.43', '+0.66', '-1.18', '-0.42', '+0.41', '+0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -81.48\n",
      "['-0.72', '+0.55', '-1.48', '-0.53', '+0.47', '+0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward -133.92\n",
      "['-1.01', '+0.31', '-1.48', '-1.07', '+0.52', '+0.05', '+0.00', '+0.00']\n",
      "step 100 total_reward -276.88\n",
      "['+0.01', '+0.95', '+0.51', '+0.17', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.11\n",
      "['+0.11', '+0.91', '+0.51', '-0.36', '-0.12', '-0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.46\n",
      "['+0.21', '+0.72', '+0.54', '-0.78', '-0.24', '-0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -45.90\n",
      "['+0.36', '+0.55', '+0.95', '-0.36', '-0.35', '-0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -59.83\n",
      "['+0.60', '+0.50', '+1.47', '-0.01', '-0.50', '-0.18', '+0.00', '+0.00']\n",
      "step 80 total_reward -139.36\n",
      "['+0.95', '+0.49', '+1.77', '-0.25', '-0.65', '-0.15', '+0.00', '+0.00']\n",
      "step 100 total_reward -216.32\n",
      "['+1.02', '+0.47', '+1.77', '-0.36', '-0.68', '-0.15', '+0.00', '+0.00']\n",
      "step 104 total_reward -323.95\n",
      "total rewards [-277.2890488763223, -336.56450409532255, -274.20223210260394, -372.2972861927164, -289.20826685782004, -331.79291468000196, -334.57655615738537, -145.30013485902302, -292.8731308573286, -236.45346721722282, -338.58895794992037, -300.51123195062684, -309.7205365339973, -311.88542021789146, -430.6262023057509, -319.17663905067513, -266.9788916680857, -387.401742059212, -224.6724670386687, -306.09281934008436, -287.799784091559, -231.06290894768279, -311.1266524238999, -222.47440854997467, -293.2237386580541, -329.63106504299964, -348.02353530377354, -295.12966309608305, -305.72375785643237, -291.5674863377722, -306.29137911797955, -313.68188529625024, -286.14759926350877, -353.14920042513495, -304.30992540863895, -276.9344331423797, -377.0880866638418, -319.8810535083121, -331.74192665240594, -400.15628708811624, -311.6738632802878, -399.0638899738961, -356.52293414118685, -329.33323488929335, -321.23259453433366, -457.8920894560656, -299.7476676215798, -431.6997185964311, -325.55949051382714, -341.830017857351, -202.94347538074948, -196.78264484232878, -322.0115920739547, -323.9350281706255, -275.87399455324544, -319.23456655605264, -320.6652438481877, -252.02194005230214, -266.84633381027436, -339.7005958727033, -295.8454112564417, -317.5250637789269, -279.07777083779007, -368.7023392726052, -317.2971879734763, -334.76933372763165, -267.95726794320257, -594.4242997272208, -342.16096963738846, -297.2391167744196, -305.38967963399597, -320.0995138483314, -274.90846237464626, -321.5141711896767, -300.5426211860027, -255.4290510473479, -237.4185460123732, -317.66793658837497, -467.91789926510813, -238.15812241481194, -397.3420795919234, -227.70470486286536, -322.67222757480056, -360.8268688184776, -371.46358826116915, -290.6141249614871, -332.21801018048075, -531.557999036218, -387.1866398818599, -368.1654391431957, -312.77598080442993, -277.90301571105636, -299.6977406044528, -321.6574107052679, -362.88750906286964, -361.6086122777674, -275.81637875460933, -339.6417610333431, -323.6539560344793, -287.87344226548373, -294.644900929375, -279.7704119816712, -284.50904526150583, -262.7239968686632, -299.40985225138684, -470.91172119002, -400.705908036993, -302.3533471305907, -220.9353741492045, -305.9940187377576, -241.34543688922122, -255.17764429607757, -499.53743539029693, -196.83253732393445, -320.7441078189992, -303.3604432062645, -307.7981190389294, -299.3411716168873, -330.24116656880494, -366.9082957169212, -351.8560107515435, -314.69493095730974, -320.8862908253453, -260.1109481966572, -389.95126336152657, -232.76069407650454, -301.7944186280024, -347.03098827187364, -236.055527881128, -303.1727251261929, -314.15043753589555, -301.19066903054136, -308.8582132954129, -293.1392274583246, -293.2996266946346, -307.2003852947196, -301.16722339039734, -439.43820549552817, -351.89306144038414, -257.0407342065523, -302.4146355252383, -412.27667695462975, -263.20462917355894, -302.4606189114632, -306.6529223173689, -181.3760685922294, -278.4713620953054, -323.5164949997408, -310.76555267477454, -428.5997982180949, -292.97713170994405, -229.27341837781267, -348.78640160299653, -337.5657634422084, -228.82573025796427, -298.9914957721019, -438.0632488509165, -365.88123626359754, -376.5556291399713, -340.4342239830517, -227.24208791170054, -348.89572747279374, -275.7413925005162, -356.45690060224695, -311.3808765499899, -301.80914904910077, -327.71778152610614, -309.9646758231673, -323.08057472754535, -245.17809576962372, -333.39181241678136, -281.36636651823915, -304.4417466051906, -289.7220481299246, -280.44640500144624, -309.50892944022524, -310.31467273462266, -359.49162131140866, -311.485659208954, -248.17310387006046, -334.52997709822614, -389.3680732295564, -291.62517780836777, -312.3334311484708, -344.48332722675946, -315.8215011234423, -291.28640176509106, -269.54717798551053, -331.33778307738913, -296.04298214977166, -319.49558330834543, -301.22462286152114, -316.2726720186713, -270.8831766923619, -264.95926532047474, -296.51287366974043, -295.3751564274197, -306.05827139872997, -276.88290040055125, -323.9525030709835]\n",
      "average total reward -314.5653621855106\n"
     ]
    }
   ],
   "source": [
    "!python lunar_lander_ml_images_player_model1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_model_reward_comparisons[\"Model 1\"] = '-314.5653621855106'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-04-23 00:23:54.768592: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "['+0.02', '+0.93', '+0.79', '-0.46', '-0.02', '-0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.23\n",
      "['+0.16', '+0.71', '+0.73', '-0.84', '-0.03', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.63\n",
      "['+0.32', '+0.41', '+0.90', '-1.26', '-0.32', '-0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -58.28\n",
      "['+0.50', '-0.01', '+0.63', '+0.42', '-1.59', '-2.68', '+1.00', '+0.00']\n",
      "step 59 total_reward -297.34\n",
      "['-0.00', '+0.95', '-0.07', '+0.16', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.50\n",
      "['-0.02', '+0.91', '-0.14', '-0.37', '+0.19', '+0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.33\n",
      "['-0.04', '+0.72', '-0.19', '-0.71', '+0.42', '+0.23', '+0.00', '+0.00']\n",
      "step 40 total_reward -76.14\n",
      "['-0.14', '+0.54', '-0.73', '-0.67', '+0.70', '+0.47', '+0.00', '+0.00']\n",
      "step 60 total_reward -117.02\n",
      "['-0.32', '+0.28', '-1.28', '-1.11', '+1.52', '+1.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -259.22\n",
      "['-0.49', '+0.09', '-1.41', '-0.21', '+1.92', '-2.78', '+0.00', '+1.00']\n",
      "step 91 total_reward -451.76\n",
      "['-0.01', '+0.95', '-0.66', '+0.21', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.46\n",
      "['-0.14', '+0.93', '-0.58', '-0.31', '+0.20', '-0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.25\n",
      "['-0.25', '+0.77', '-0.49', '-0.57', '-0.17', '-0.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -10.82\n",
      "['-0.31', '+0.66', '+0.03', '-0.23', '-0.55', '-0.34', '+0.00', '+0.00']\n",
      "step 60 total_reward +4.70\n",
      "['-0.28', '+0.56', '+0.22', '-0.46', '-0.59', '+0.22', '+0.00', '+0.00']\n",
      "step 80 total_reward -19.47\n",
      "['-0.25', '+0.34', '+0.04', '-0.90', '+0.05', '+0.96', '+0.00', '+0.00']\n",
      "step 100 total_reward +14.69\n",
      "['-0.24', '+0.08', '+0.03', '-1.06', '+0.87', '+0.46', '+0.00', '+0.00']\n",
      "step 120 total_reward -68.52\n",
      "['-0.23', '+0.02', '+0.49', '-0.07', '+1.62', '+0.63', '+0.00', '+1.00']\n",
      "step 126 total_reward -181.11\n",
      "['+0.00', '+0.93', '+0.02', '-0.49', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.19\n",
      "['+0.02', '+0.70', '+0.17', '-0.91', '-0.31', '-0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.77\n",
      "['+0.08', '+0.39', '+0.41', '-1.30', '-0.92', '-1.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -131.14\n",
      "['+0.22', '+0.01', '+0.61', '+0.09', '-1.96', '-0.83', '+1.00', '+0.00']\n",
      "step 59 total_reward -347.05\n",
      "['-0.01', '+0.93', '-0.59', '-0.49', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.65\n",
      "['-0.12', '+0.70', '-0.51', '-0.82', '-0.03', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.75\n",
      "['-0.23', '+0.43', '-0.65', '-1.11', '+0.12', '+0.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -25.02\n",
      "['-0.34', '+0.07', '+0.36', '+0.14', '+1.36', '+2.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -178.38\n",
      "['+0.01', '+0.95', '+0.30', '+0.38', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.75\n",
      "['+0.06', '+0.98', '+0.33', '-0.15', '+0.01', '-0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward +9.63\n",
      "['+0.11', '+0.85', '+0.18', '-0.69', '+0.24', '+0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -36.15\n",
      "['+0.15', '+0.59', '+0.16', '-1.02', '+0.50', '+0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -70.93\n",
      "['+0.19', '+0.22', '+0.27', '-1.41', '+0.12', '-0.55', '+0.00', '+0.00']\n",
      "step 80 total_reward -43.01\n",
      "['+0.22', '-0.03', '+0.18', '+0.06', '-0.06', '+1.17', '+1.00', '+1.00']\n",
      "step 91 total_reward -143.11\n",
      "['+0.01', '+0.96', '+0.37', '+0.48', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.57\n",
      "['+0.09', '+1.01', '+0.53', '-0.07', '-0.32', '-0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.34\n",
      "['+0.19', '+0.92', '+0.44', '-0.55', '-0.87', '-0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -94.06\n",
      "['+0.29', '+0.68', '+0.89', '-0.93', '-1.06', '-0.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -153.72\n",
      "['+0.47', '+0.30', '+0.92', '-1.60', '-1.91', '-1.33', '+0.00', '+0.00']\n",
      "step 80 total_reward -277.35\n",
      "['+0.58', '+0.02', '+1.40', '-0.48', '-2.81', '-5.33', '+0.00', '+0.00']\n",
      "step 91 total_reward -482.70\n",
      "['-0.01', '+0.94', '-0.26', '-0.15', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.04\n",
      "['-0.04', '+0.81', '-0.16', '-0.69', '-0.28', '-0.38', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.16\n",
      "['-0.04', '+0.61', '+0.31', '-0.58', '-0.69', '-0.43', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.23\n",
      "['+0.09', '+0.42', '+1.08', '-0.73', '-1.22', '-0.72', '+0.00', '+0.00']\n",
      "step 60 total_reward -182.55\n",
      "['+0.43', '+0.15', '+1.92', '-1.20', '-1.94', '-0.70', '+0.00', '+0.00']\n",
      "step 80 total_reward -354.80\n",
      "['+0.46', '+0.13', '+0.92', '+0.24', '-2.08', '-2.43', '+0.00', '+0.00']\n",
      "step 82 total_reward -466.15\n",
      "['+0.01', '+0.94', '+0.67', '-0.19', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.04\n",
      "['+0.13', '+0.79', '+0.54', '-0.73', '+0.26', '+0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.16\n",
      "['+0.20', '+0.58', '+0.14', '-0.70', '+0.66', '+0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.43\n",
      "['+0.25', '+0.29', '+0.31', '-1.15', '+0.44', '-0.66', '+0.00', '+0.00']\n",
      "step 60 total_reward -41.84\n",
      "['+0.29', '-0.01', '-0.06', '-0.83', '+0.20', '+5.22', '+1.00', '+1.00']\n",
      "step 75 total_reward -109.91\n",
      "['+0.00', '+0.95', '+0.05', '+0.16', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.49\n",
      "['-0.01', '+0.91', '-0.14', '-0.40', '+0.41', '+0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.33\n",
      "['-0.04', '+0.71', '-0.40', '-0.91', '+1.30', '+0.86', '+0.00', '+0.00']\n",
      "step 40 total_reward -188.61\n",
      "['-0.20', '+0.36', '-0.82', '-1.49', '+2.42', '+1.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -343.83\n",
      "['-0.38', '-0.08', '-1.82', '-0.52', '+3.55', '-0.00', '+0.00', '+0.00']\n",
      "step 76 total_reward -613.73\n",
      "['+0.01', '+0.93', '+0.63', '-0.29', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.12\n",
      "['+0.13', '+0.76', '+0.57', '-0.82', '+0.13', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.35\n",
      "['+0.25', '+0.47', '+0.69', '-1.21', '+0.02', '-0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -31.87\n",
      "['+0.40', '+0.02', '+0.79', '-1.75', '-0.79', '-0.90', '+0.00', '+0.00']\n",
      "step 60 total_reward -149.47\n",
      "['+0.42', '-0.05', '+1.26', '-0.87', '-0.87', '+3.82', '+1.00', '+0.00']\n",
      "step 63 total_reward -254.03\n",
      "['+0.00', '+0.93', '+0.17', '-0.40', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.27\n",
      "['+0.02', '+0.72', '+0.05', '-0.88', '+0.33', '+0.46', '+0.00', '+0.00']\n",
      "step 20 total_reward -59.09\n",
      "['+0.01', '+0.44', '-0.02', '-1.12', '+0.56', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -80.72\n",
      "['-0.07', '+0.16', '-0.69', '-0.80', '+0.43', '-0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -39.30\n",
      "['-0.20', '-0.03', '-0.96', '-0.18', '+0.07', '-1.93', '+1.00', '+1.00']\n",
      "step 76 total_reward -108.50\n",
      "['-0.00', '+0.93', '-0.15', '-0.23', '+0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.30\n",
      "['-0.02', '+0.78', '-0.04', '-0.77', '-0.34', '-0.47', '+0.00', '+0.00']\n",
      "step 20 total_reward -69.20\n",
      "['+0.01', '+0.55', '+0.35', '-0.93', '-0.91', '-0.88', '+0.00', '+0.00']\n",
      "step 40 total_reward -128.12\n",
      "['+0.09', '+0.18', '+0.37', '-1.57', '-2.29', '-1.71', '+0.00', '+0.00']\n",
      "step 60 total_reward -293.10\n",
      "['+0.12', '-0.00', '+0.36', '+0.04', '-3.16', '-0.29', '+0.00', '+0.00']\n",
      "step 68 total_reward -465.71\n",
      "['+0.01', '+0.94', '+0.73', '-0.11', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.21\n",
      "['+0.14', '+0.82', '+0.61', '-0.64', '+0.23', '+0.35', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.85\n",
      "['+0.25', '+0.61', '+0.46', '-0.76', '+0.53', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.12\n",
      "['+0.36', '+0.30', '+0.63', '-1.24', '+0.24', '-0.67', '+0.00', '+0.00']\n",
      "step 60 total_reward -45.32\n",
      "['+0.46', '-0.02', '+0.15', '-0.16', '-0.47', '+2.55', '+1.00', '+0.00']\n",
      "step 75 total_reward -193.57\n",
      "['-0.01', '+0.95', '-0.74', '+0.33', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.02\n",
      "['-0.16', '+0.96', '-0.72', '-0.21', '+0.18', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.08\n",
      "['-0.29', '+0.82', '-0.62', '-0.73', '-0.06', '-0.31', '+0.00', '+0.00']\n",
      "step 40 total_reward -11.80\n",
      "['-0.41', '+0.61', '-0.43', '-0.59', '-0.20', '-0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward +5.48\n",
      "['-0.50', '+0.39', '-0.58', '-0.99', '+0.03', '+0.61', '+0.00', '+0.00']\n",
      "step 80 total_reward -10.42\n",
      "['-0.61', '+0.12', '-0.23', '+0.02', '+0.67', '+1.17', '+0.00', '+1.00']\n",
      "step 97 total_reward -178.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.00', '+0.94', '-0.22', '+0.02', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.87\n",
      "['-0.06', '+0.86', '-0.28', '-0.51', '+0.30', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.25\n",
      "['-0.12', '+0.65', '-0.52', '-0.79', '+0.57', '+0.31', '+0.00', '+0.00']\n",
      "step 40 total_reward -103.95\n",
      "['-0.24', '+0.32', '-0.81', '-1.37', '+1.37', '+1.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -224.41\n",
      "['-0.42', '-0.03', '-0.34', '-0.09', '+2.25', '+3.58', '+0.00', '+0.00']\n",
      "step 76 total_reward -452.68\n",
      "['-0.01', '+0.93', '-0.45', '-0.46', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.68\n",
      "['-0.10', '+0.72', '-0.57', '-0.76', '+0.28', '+0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.13\n",
      "['-0.24', '+0.47', '-0.81', '-1.08', '+0.76', '+0.87', '+0.00', '+0.00']\n",
      "step 40 total_reward -113.06\n",
      "['-0.46', '+0.08', '-1.34', '-1.51', '+1.84', '+0.83', '+0.00', '+0.00']\n",
      "step 60 total_reward -282.70\n",
      "['-0.50', '+0.03', '-1.73', '-0.23', '+2.29', '+1.54', '+0.00', '+0.00']\n",
      "step 63 total_reward -396.33\n",
      "['+0.01', '+0.94', '+0.61', '-0.03', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.76\n",
      "['+0.12', '+0.85', '+0.44', '-0.57', '+0.30', '+0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.55\n",
      "['+0.18', '+0.63', '-0.01', '-0.72', '+0.89', '+0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -73.80\n",
      "['+0.19', '+0.35', '+0.09', '-1.14', '+0.97', '-0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -98.28\n",
      "['+0.21', '-0.01', '+0.26', '-0.47', '+0.35', '-4.82', '+1.00', '+1.00']\n",
      "step 78 total_reward -147.49\n",
      "['-0.00', '+0.95', '-0.02', '+0.20', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.93\n",
      "['-0.02', '+0.92', '-0.21', '-0.36', '+0.45', '+0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.79\n",
      "['-0.06', '+0.74', '-0.47', '-0.80', '+1.20', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -172.55\n",
      "['-0.25', '+0.45', '-1.17', '-1.29', '+2.10', '+1.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -323.97\n",
      "['-0.52', '-0.06', '-1.37', '-1.99', '+3.38', '+1.30', '+0.00', '+0.00']\n",
      "step 80 total_reward -523.85\n",
      "['-0.53', '-0.08', '-1.13', '-0.58', '+3.30', '-5.55', '+0.00', '+0.00']\n",
      "step 81 total_reward -623.85\n",
      "['-0.01', '+0.95', '-0.72', '+0.32', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.25\n",
      "['-0.17', '+0.96', '-0.90', '-0.25', '+0.62', '+1.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -80.32\n",
      "['-0.35', '+0.80', '-0.92', '-0.86', '+1.83', '+1.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -223.58\n",
      "['-0.63', '+0.39', '-1.44', '-1.80', '+3.45', '+1.84', '+0.00', '+0.00']\n",
      "step 60 total_reward -481.44\n",
      "['-0.82', '+0.02', '-1.70', '-0.43', '+4.69', '+1.08', '+1.00', '+0.00']\n",
      "step 73 total_reward -722.34\n",
      "['+0.00', '+0.94', '+0.09', '-0.12', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.11\n",
      "['+0.00', '+0.82', '-0.07', '-0.66', '+0.41', '+0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.27\n",
      "['-0.05', '+0.60', '-0.75', '-0.74', '+1.08', '+0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -170.89\n",
      "['-0.23', '+0.29', '-0.96', '-1.39', '+2.20', '+1.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -323.92\n",
      "['-0.36', '+0.03', '-0.00', '+0.00', '+2.92', '+0.00', '+0.00', '+0.00']\n",
      "step 72 total_reward -541.97\n",
      "['-0.01', '+0.94', '-0.43', '-0.10', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.21\n",
      "['-0.08', '+0.82', '-0.31', '-0.63', '-0.28', '-0.42', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.27\n",
      "['-0.11', '+0.63', '+0.16', '-0.59', '-0.71', '-0.39', '+0.00', '+0.00']\n",
      "step 40 total_reward -61.03\n",
      "['+0.00', '+0.45', '+1.00', '-0.60', '-0.95', '-0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -127.92\n",
      "['+0.32', '+0.25', '+1.93', '-0.82', '-1.11', '-0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -236.18\n",
      "['+0.71', '-0.08', '+1.93', '-1.35', '-1.24', '-0.25', '+1.00', '+0.00']\n",
      "step 100 total_reward -295.79\n",
      "['+0.73', '-0.08', '+1.72', '-0.09', '-1.50', '-5.92', '+1.00', '+0.00']\n",
      "step 101 total_reward -395.79\n",
      "['-0.01', '+0.95', '-0.54', '+0.37', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.80\n",
      "['-0.12', '+0.98', '-0.55', '-0.17', '+0.22', '+0.20', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.23\n",
      "['-0.22', '+0.85', '-0.45', '-0.68', '+0.20', '-0.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -28.86\n",
      "['-0.33', '+0.69', '-0.63', '-0.33', '-0.02', '-0.27', '+0.00', '+0.00']\n",
      "step 60 total_reward +6.35\n",
      "['-0.45', '+0.65', '-0.45', '+0.03', '-0.30', '-0.28', '+0.00', '+0.00']\n",
      "step 80 total_reward -4.48\n",
      "['-0.50', '+0.70', '+0.03', '+0.27', '-0.65', '-0.38', '+0.00', '+0.00']\n",
      "step 100 total_reward -34.91\n",
      "['-0.40', '+0.79', '+1.01', '+0.31', '-1.00', '-0.34', '+0.00', '+0.00']\n",
      "step 120 total_reward -156.95\n",
      "['-0.08', '+0.87', '+1.85', '+0.07', '-1.34', '-0.34', '+0.00', '+0.00']\n",
      "step 140 total_reward -272.91\n",
      "['+0.30', '+0.81', '+2.20', '-0.51', '-1.68', '-0.30', '+0.00', '+0.00']\n",
      "step 160 total_reward -347.63\n",
      "['+0.81', '+0.56', '+2.60', '-1.16', '-2.19', '-0.94', '+0.00', '+0.00']\n",
      "step 180 total_reward -472.93\n",
      "['+1.02', '+0.41', '+2.60', '-1.37', '-2.58', '-0.98', '+0.00', '+0.00']\n",
      "step 188 total_reward -624.65\n",
      "['+0.01', '+0.94', '+0.47', '-0.01', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.77\n",
      "['+0.09', '+0.85', '+0.30', '-0.55', '+0.31', '+0.61', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.88\n",
      "['+0.13', '+0.63', '-0.01', '-0.86', '+0.91', '+0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -100.62\n",
      "['+0.13', '+0.31', '-0.30', '-1.16', '+1.01', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -116.44\n",
      "['-0.05', '+0.04', '-1.53', '-0.24', '+1.68', '+2.93', '+0.00', '+1.00']\n",
      "step 80 total_reward -186.50\n",
      "['-0.17', '+0.01', '-1.58', '-0.23', '+2.72', '+3.80', '+0.00', '+0.00']\n",
      "step 87 total_reward -404.53\n",
      "['-0.01', '+0.95', '-0.32', '+0.30', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.19\n",
      "['-0.06', '+0.96', '-0.19', '-0.24', '-0.18', '-0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.20\n",
      "['-0.09', '+0.80', '-0.08', '-0.80', '-0.97', '-0.93', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.34\n",
      "['+0.04', '+0.54', '+1.14', '-1.10', '-1.96', '-1.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -275.40\n",
      "['+0.28', '+0.10', '+1.16', '-1.83', '-3.47', '-1.60', '+0.00', '+0.00']\n",
      "step 80 total_reward -460.58\n",
      "['+0.31', '+0.05', '+0.29', '+0.06', '-3.62', '-0.91', '+0.00', '+0.00']\n",
      "step 82 total_reward -571.18\n",
      "['+0.01', '+0.93', '+0.64', '-0.24', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.13\n",
      "['+0.13', '+0.78', '+0.53', '-0.78', '+0.22', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.27\n",
      "['+0.22', '+0.54', '+0.46', '-0.90', '+0.50', '+0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.72\n",
      "['+0.33', '+0.19', '+0.60', '-1.39', '+0.12', '-0.61', '+0.00', '+0.00']\n",
      "step 60 total_reward -41.79\n",
      "['+0.38', '-0.01', '-0.52', '-0.51', '-0.09', '+3.72', '+1.00', '+0.00']\n",
      "step 69 total_reward -136.27\n",
      "['+0.00', '+0.94', '+0.19', '-0.16', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.41\n",
      "['+0.03', '+0.80', '+0.06', '-0.70', '+0.37', '+0.53', '+0.00', '+0.00']\n",
      "step 20 total_reward -69.66\n",
      "['-0.00', '+0.59', '-0.54', '-0.66', '+0.91', '+0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -120.98\n",
      "['-0.20', '+0.36', '-1.31', '-0.98', '+1.47', '+0.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -242.54\n",
      "['-0.58', '+0.00', '-2.02', '-1.49', '+2.18', '+0.68', '+0.00', '+0.00']\n",
      "step 80 total_reward -419.81\n",
      "['-0.72', '-0.16', '-2.29', '-0.38', '+2.48', '+0.44', '+0.00', '+0.00']\n",
      "step 87 total_reward -567.88\n",
      "['-0.01', '+0.94', '-0.32', '-0.11', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.24\n",
      "['-0.06', '+0.82', '-0.21', '-0.64', '-0.15', '-0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.70\n",
      "['-0.07', '+0.63', '+0.08', '-0.59', '-0.43', '-0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -45.58\n",
      "['-0.01', '+0.47', '+0.70', '-0.41', '-0.68', '-0.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -80.97\n",
      "['+0.22', '+0.36', '+1.54', '-0.41', '-0.90', '-0.30', '+0.00', '+0.00']\n",
      "step 80 total_reward -182.60\n",
      "['+0.58', '+0.17', '+2.31', '-0.87', '-1.55', '-0.73', '+0.00', '+0.00']\n",
      "step 100 total_reward -355.81\n",
      "['+0.84', '+0.00', '+2.18', '-0.27', '-1.93', '+2.81', '+1.00', '+0.00']\n",
      "step 111 total_reward -523.41\n",
      "['+0.01', '+0.96', '+0.39', '+0.48', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.08\n",
      "['+0.10', '+1.01', '+0.59', '-0.09', '-0.57', '-0.95', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.23', '+0.90', '+0.65', '-0.65', '-1.83', '-1.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -210.66\n",
      "['+0.37', '+0.58', '+0.72', '-1.54', '-3.48', '-1.85', '+0.00', '+0.00']\n",
      "step 60 total_reward -432.07\n",
      "['+0.51', '+0.06', '+0.70', '-1.32', '-5.80', '-3.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -626.85\n",
      "['+0.52', '+0.04', '+0.68', '-0.82', '-6.10', '-6.44', '+1.00', '+0.00']\n",
      "step 81 total_reward -726.85\n",
      "['-0.00', '+0.93', '-0.24', '-0.31', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.53\n",
      "['-0.05', '+0.76', '-0.18', '-0.74', '-0.06', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.15\n",
      "['-0.07', '+0.60', '-0.07', '-0.26', '-0.19', '-0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward +17.67\n",
      "['-0.04', '+0.59', '+0.36', '+0.13', '-0.33', '-0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -12.25\n",
      "['+0.09', '+0.69', '+0.92', '+0.49', '-0.42', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -103.21\n",
      "['+0.33', '+0.88', '+1.53', '+0.80', '-0.52', '-0.10', '+0.00', '+0.00']\n",
      "step 100 total_reward -212.38\n",
      "['+0.68', '+1.12', '+1.82', '+0.59', '-0.63', '-0.11', '+0.00', '+0.00']\n",
      "step 120 total_reward -282.11\n",
      "['+1.01', '+1.21', '+1.82', '+0.11', '-0.73', '-0.11', '+0.00', '+0.00']\n",
      "step 138 total_reward -408.03\n",
      "['-0.02', '+0.93', '-0.80', '-0.21', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.30\n",
      "['-0.17', '+0.79', '-0.77', '-0.63', '+0.13', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.50\n",
      "['-0.35', '+0.65', '-0.97', '-0.45', '+0.17', '+0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -27.43\n",
      "['-0.58', '+0.56', '-1.39', '-0.15', '+0.28', '+0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -84.07\n",
      "['-0.89', '+0.52', '-1.65', '-0.32', '+0.55', '+0.62', '+0.00', '+0.00']\n",
      "step 80 total_reward -165.08\n",
      "['-1.00', '+0.48', '-1.70', '-0.53', '+0.83', '+0.92', '+0.00', '+0.00']\n",
      "step 87 total_reward -304.06\n",
      "['+0.00', '+0.95', '+0.13', '+0.39', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.13\n",
      "['+0.03', '+0.99', '+0.15', '-0.14', '-0.14', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward +5.55\n",
      "['+0.05', '+0.86', '+0.01', '-0.66', '-0.02', '+0.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -17.23\n",
      "['+0.05', '+0.59', '+0.01', '-1.09', '+0.34', '+0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -64.82\n",
      "['+0.05', '+0.23', '-0.10', '-1.13', '+0.19', '-0.21', '+0.00', '+0.00']\n",
      "step 80 total_reward -22.49\n",
      "['+0.02', '-0.03', '-0.02', '+0.02', '-0.00', '-0.08', '+1.00', '+1.00']\n",
      "step 96 total_reward -64.68\n",
      "['+0.01', '+0.94', '+0.44', '-0.20', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.43\n",
      "['+0.09', '+0.79', '+0.41', '-0.73', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.61\n",
      "['+0.17', '+0.52', '+0.49', '-1.10', '-0.20', '-0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -57.19\n",
      "['+0.28', '+0.11', '+0.65', '-1.70', '-1.14', '-1.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -188.47\n",
      "['+0.32', '+0.02', '+0.58', '+0.15', '-2.04', '-1.86', '+1.00', '+0.00']\n",
      "step 65 total_reward -303.91\n",
      "['-0.00', '+0.95', '-0.01', '+0.43', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.72\n",
      "['+0.01', '+1.00', '+0.11', '-0.11', '-0.16', '-0.44', '+0.00', '+0.00']\n",
      "step 20 total_reward +8.48\n",
      "['+0.01', '+0.89', '-0.05', '-0.61', '-0.20', '+0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -31.66\n",
      "['+0.01', '+0.64', '+0.07', '-0.80', '-0.10', '+0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -18.57\n",
      "['+0.04', '+0.33', '+0.35', '-1.06', '-0.38', '-0.43', '+0.00', '+0.00']\n",
      "step 80 total_reward -49.96\n",
      "['+0.13', '-0.01', '+0.18', '-0.26', '-0.49', '+3.45', '+0.00', '+0.00']\n",
      "step 100 total_reward -155.20\n",
      "['+0.00', '+0.94', '+0.14', '-0.19', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.06\n",
      "['+0.02', '+0.79', '+0.03', '-0.73', '+0.34', '+0.44', '+0.00', '+0.00']\n",
      "step 20 total_reward -70.09\n",
      "['-0.01', '+0.56', '-0.40', '-0.78', '+0.75', '+0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -106.48\n",
      "['-0.11', '+0.27', '-0.54', '-1.25', '+0.94', '+0.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -147.82\n",
      "['-0.21', '+0.01', '-0.09', '+0.07', '+1.51', '+0.40', '+0.00', '+1.00']\n",
      "step 74 total_reward -238.68\n",
      "['+0.01', '+0.94', '+0.49', '-0.07', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.47\n",
      "['+0.09', '+0.83', '+0.32', '-0.61', '+0.30', '+0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.11\n",
      "['+0.13', '+0.61', '+0.03', '-0.87', '+0.85', '+0.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -93.02\n",
      "['+0.14', '+0.28', '-0.12', '-1.19', '+0.76', '-0.36', '+0.00', '+0.00']\n",
      "step 60 total_reward -87.17\n",
      "['+0.06', '-0.01', '-0.41', '-0.43', '+0.38', '-4.19', '+0.00', '+0.00']\n",
      "step 77 total_reward -141.06\n",
      "['+0.01', '+0.93', '+0.66', '-0.21', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.02\n",
      "['+0.13', '+0.79', '+0.52', '-0.74', '+0.24', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.20\n",
      "['+0.22', '+0.54', '+0.45', '-0.96', '+0.57', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -60.12\n",
      "['+0.32', '+0.18', '+0.58', '-1.45', '+0.15', '-0.63', '+0.00', '+0.00']\n",
      "step 60 total_reward -47.64\n",
      "['+0.35', '+0.07', '+0.40', '-0.91', '+0.15', '+5.32', '+1.00', '+0.00']\n",
      "step 65 total_reward -133.56\n",
      "['-0.00', '+0.94', '-0.03', '+0.03', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.30\n",
      "['-0.02', '+0.87', '-0.20', '-0.51', '+0.48', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.47\n",
      "['-0.08', '+0.66', '-0.69', '-0.75', '+1.07', '+0.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -178.68\n",
      "['-0.23', '+0.35', '-0.74', '-1.39', '+2.11', '+1.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -314.88\n",
      "['-0.40', '-0.07', '-0.26', '-0.00', '+3.21', '-0.62', '+0.00', '+0.00']\n",
      "step 77 total_reward -585.14\n",
      "['+0.00', '+0.94', '+0.00', '-0.12', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.53\n",
      "['-0.01', '+0.82', '-0.10', '-0.66', '+0.36', '+0.44', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.26\n",
      "['-0.06', '+0.59', '-0.57', '-0.69', '+0.85', '+0.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -135.11\n",
      "['-0.21', '+0.32', '-0.93', '-1.23', '+1.71', '+1.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -266.42\n",
      "['-0.37', '+0.05', '-0.97', '-0.11', '+2.58', '+3.95', '+0.00', '+0.00']\n",
      "step 74 total_reward -479.69\n",
      "['-0.00', '+0.96', '-0.05', '+0.47', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.93\n",
      "['-0.01', '+1.01', '-0.03', '-0.06', '-0.11', '-0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward +25.87\n",
      "['-0.01', '+0.91', '+0.04', '-0.61', '-0.29', '-0.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -36.25\n",
      "['+0.00', '+0.66', '+0.31', '-0.91', '-0.87', '-0.63', '+0.00', '+0.00']\n",
      "step 60 total_reward -106.30\n",
      "['+0.09', '+0.32', '+0.70', '-1.50', '-1.90', '-1.31', '+0.00', '+0.00']\n",
      "step 80 total_reward -247.53\n",
      "['+0.20', '+0.01', '+0.00', '+0.00', '-2.71', '-0.00', '+0.00', '+0.00']\n",
      "step 93 total_reward -453.72\n",
      "['-0.00', '+0.95', '-0.15', '+0.15', '+0.00', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.73\n",
      "['-0.05', '+0.91', '-0.33', '-0.40', '+0.51', '+0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.31\n",
      "['-0.12', '+0.72', '-0.64', '-0.79', '+1.13', '+0.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -172.79\n",
      "['-0.30', '+0.41', '-0.97', '-1.34', '+1.96', '+1.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -299.89\n",
      "['-0.45', '+0.09', '-0.12', '-0.76', '+2.82', '-5.81', '+0.00', '+0.00']\n",
      "step 74 total_reward -524.45\n",
      "['-0.01', '+0.93', '-0.29', '-0.34', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.65\n",
      "['-0.05', '+0.74', '-0.14', '-0.82', '-0.32', '-0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.17\n",
      "['-0.03', '+0.55', '+0.31', '-0.78', '-0.85', '-0.80', '+0.00', '+0.00']\n",
      "step 40 total_reward -90.41\n",
      "['+0.06', '+0.22', '+0.81', '-1.44', '-2.09', '-1.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -266.91\n",
      "['+0.16', '-0.00', '+1.05', '-0.60', '-3.07', '-6.19', '+0.00', '+0.00']\n",
      "step 70 total_reward -454.08\n",
      "['+0.01', '+0.93', '+0.32', '-0.23', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.25\n",
      "['+0.06', '+0.78', '+0.22', '-0.77', '+0.27', '+0.39', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.30\n",
      "['+0.10', '+0.49', '+0.28', '-1.18', '+0.43', '-0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -82.31\n",
      "['+0.15', '+0.09', '+0.20', '-1.52', '-0.00', '-0.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -41.07\n",
      "['+0.16', '-0.03', '+0.14', '-0.78', '-0.00', '+5.40', '+1.00', '+1.00']\n",
      "step 65 total_reward -144.83\n",
      "['+0.01', '+0.93', '+0.61', '-0.29', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.12', '+0.76', '+0.49', '-0.83', '+0.24', '+0.41', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.89\n",
      "['+0.22', '+0.46', '+0.58', '-1.23', '+0.37', '-0.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -62.84\n",
      "['+0.35', '+0.01', '+0.63', '-1.74', '-0.13', '-0.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -72.31\n",
      "['+0.37', '-0.09', '+1.00', '-0.17', '-0.33', '+0.38', '+1.00', '+1.00']\n",
      "step 64 total_reward -166.60\n",
      "['-0.01', '+0.92', '-0.35', '-0.57', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.53\n",
      "['-0.08', '+0.68', '-0.40', '-0.80', '+0.14', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.75\n",
      "['-0.17', '+0.41', '-0.58', '-1.17', '+0.55', '+0.79', '+0.00', '+0.00']\n",
      "step 40 total_reward -76.25\n",
      "['-0.31', '+0.02', '-1.02', '-0.91', '+1.54', '+3.82', '+0.00', '+1.00']\n",
      "step 60 total_reward -160.06\n",
      "['-0.36', '-0.01', '-1.11', '+0.04', '+2.21', '+1.73', '+0.00', '+1.00']\n",
      "step 64 total_reward -322.16\n",
      "['+0.00', '+0.94', '+0.20', '-0.06', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.40\n",
      "['+0.03', '+0.84', '+0.03', '-0.61', '+0.39', '+0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.89\n",
      "['+0.01', '+0.60', '-0.31', '-0.91', '+1.07', '+0.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -150.23\n",
      "['-0.09', '+0.27', '-0.82', '-1.32', '+1.38', '+0.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -212.10\n",
      "['-0.26', '+0.00', '-1.13', '-0.00', '+1.75', '-0.80', '+0.00', '+1.00']\n",
      "step 73 total_reward -389.54\n",
      "['+0.01', '+0.94', '+0.42', '+0.04', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.03\n",
      "['+0.08', '+0.87', '+0.23', '-0.51', '+0.35', '+0.73', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.44\n",
      "['+0.12', '+0.64', '+0.20', '-1.00', '+1.11', '+0.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -141.42\n",
      "['+0.16', '+0.28', '-0.05', '-1.38', '+1.22', '-0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -156.80\n",
      "['+0.14', '+0.01', '-0.25', '+0.06', '+2.05', '+0.79', '+0.00', '+0.00']\n",
      "step 75 total_reward -257.03\n",
      "['+0.00', '+0.93', '+0.07', '-0.38', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.37\n",
      "['+0.03', '+0.73', '+0.16', '-0.91', '-0.35', '-0.42', '+0.00', '+0.00']\n",
      "step 20 total_reward -71.77\n",
      "['+0.09', '+0.44', '+0.41', '-1.26', '-1.01', '-1.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -151.23\n",
      "['+0.21', '-0.01', '+1.40', '-0.42', '-2.17', '-0.03', '+1.00', '+0.00']\n",
      "step 60 total_reward -395.57\n",
      "['+0.00', '+0.94', '+0.21', '+0.03', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.13\n",
      "['+0.03', '+0.87', '+0.02', '-0.52', '+0.38', '+0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.27\n",
      "['+0.01', '+0.65', '-0.54', '-0.82', '+1.18', '+0.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -167.52\n",
      "['-0.17', '+0.34', '-1.00', '-1.35', '+2.06', '+1.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -300.81\n",
      "['-0.35', '+0.02', '-0.86', '+0.08', '+3.00', '-0.07', '+0.00', '+0.00']\n",
      "step 74 total_reward -530.37\n",
      "['+0.00', '+0.95', '+0.16', '+0.40', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.14\n",
      "['+0.03', '+0.99', '+0.13', '-0.13', '+0.12', '+0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward +11.66\n",
      "['+0.05', '+0.87', '+0.01', '-0.68', '+0.48', '+0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -62.04\n",
      "['+0.04', '+0.60', '-0.26', '-1.02', '+1.12', '+0.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -137.66\n",
      "['-0.06', '+0.24', '-1.01', '-1.34', '+1.24', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -181.43\n",
      "['-0.24', '+0.00', '-1.21', '+0.17', '+1.61', '+0.39', '+0.00', '+1.00']\n",
      "step 93 total_reward -312.04\n",
      "['+0.00', '+0.93', '+0.04', '-0.44', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.86\n",
      "['+0.00', '+0.72', '-0.03', '-0.89', '+0.21', '+0.24', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.74\n",
      "['-0.02', '+0.47', '-0.15', '-0.92', '+0.33', '-0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.13\n",
      "['-0.08', '+0.20', '-0.37', '-0.98', '+0.21', '-0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -19.89\n",
      "['-0.13', '-0.03', '-0.08', '+0.00', '+0.00', '+0.00', '+1.00', '+1.00']\n",
      "step 74 total_reward -89.79\n",
      "['+0.00', '+0.94', '+0.05', '+0.10', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.61\n",
      "['-0.00', '+0.89', '-0.14', '-0.45', '+0.44', '+0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -71.58\n",
      "['-0.03', '+0.68', '-0.14', '-0.98', '+1.25', '+0.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -184.14\n",
      "['-0.10', '+0.32', '-0.36', '-1.54', '+2.29', '+1.41', '+0.00', '+0.00']\n",
      "step 60 total_reward -313.29\n",
      "['-0.16', '-0.00', '+0.10', '-0.02', '+3.11', '+0.13', '+0.00', '+0.00']\n",
      "step 73 total_reward -505.97\n",
      "['+0.00', '+0.94', '+0.24', '+0.13', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.00\n",
      "['+0.04', '+0.90', '+0.07', '-0.42', '+0.39', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -48.09\n",
      "['+0.06', '+0.70', '-0.02', '-0.89', '+0.98', '+0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -134.57\n",
      "['+0.06', '+0.36', '+0.08', '-1.30', '+1.01', '-0.42', '+0.00', '+0.00']\n",
      "step 60 total_reward -146.90\n",
      "['-0.00', '-0.01', '-0.22', '-0.50', '+0.41', '-4.74', '+0.00', '+1.00']\n",
      "step 79 total_reward -148.61\n",
      "['-0.00', '+0.93', '-0.08', '-0.41', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.33\n",
      "['-0.02', '+0.72', '-0.16', '-0.86', '+0.26', '+0.29', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.75\n",
      "['-0.09', '+0.49', '-0.44', '-0.93', '+0.66', '+0.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -91.46\n",
      "['-0.20', '+0.13', '-0.87', '-1.46', '+1.70', '+1.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -238.06\n",
      "['-0.28', '-0.03', '-1.56', '-0.18', '+2.23', '+2.14', '+0.00', '+1.00']\n",
      "step 68 total_reward -375.45\n",
      "['+0.00', '+0.94', '+0.06', '-0.06', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.40\n",
      "['-0.00', '+0.84', '-0.12', '-0.60', '+0.45', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.67\n",
      "['-0.06', '+0.61', '-0.79', '-0.83', '+1.19', '+0.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -197.17\n",
      "['-0.23', '+0.27', '-0.96', '-1.56', '+2.43', '+1.57', '+0.00', '+0.00']\n",
      "step 60 total_reward -365.06\n",
      "['-0.31', '+0.07', '+0.08', '-0.60', '+2.96', '-4.98', '+0.00', '+0.00']\n",
      "step 68 total_reward -527.88\n",
      "['-0.01', '+0.95', '-0.36', '+0.25', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.89\n",
      "['-0.06', '+0.94', '-0.16', '-0.30', '-0.39', '-0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.43\n",
      "['-0.08', '+0.76', '+0.14', '-0.91', '-1.60', '-1.38', '+0.00', '+0.00']\n",
      "step 40 total_reward -188.75\n",
      "['+0.04', '+0.39', '+0.64', '-1.58', '-3.10', '-1.78', '+0.00', '+0.00']\n",
      "step 60 total_reward -383.38\n",
      "['+0.13', '+0.00', '+0.82', '-0.41', '-4.33', '-6.19', '+0.00', '+1.00']\n",
      "step 73 total_reward -620.25\n",
      "['-0.00', '+0.95', '-0.21', '+0.21', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.64\n",
      "['-0.06', '+0.92', '-0.37', '-0.34', '+0.48', '+0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.93\n",
      "['-0.12', '+0.75', '-0.45', '-0.78', '+0.94', '+0.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -135.55\n",
      "['-0.26', '+0.46', '-0.74', '-1.26', '+1.63', '+1.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -241.68\n",
      "['-0.41', '+0.09', '-0.46', '-0.56', '+3.01', '+7.63', '+0.00', '+0.00']\n",
      "step 77 total_reward -494.64\n",
      "['-0.00', '+0.94', '-0.12', '+0.02', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.50\n",
      "['-0.01', '+0.86', '+0.09', '-0.53', '-0.44', '-0.85', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.03\n",
      "['+0.02', '+0.63', '+0.44', '-0.97', '-1.34', '-0.93', '+0.00', '+0.00']\n",
      "step 40 total_reward -198.20\n",
      "['+0.11', '+0.25', '+0.38', '-1.61', '-2.74', '-1.80', '+0.00', '+0.00']\n",
      "step 60 total_reward -362.46\n",
      "['+0.16', '+0.01', '+0.17', '-0.26', '-3.46', '+3.37', '+0.00', '+0.00']\n",
      "step 70 total_reward -555.26\n",
      "['+0.00', '+0.95', '+0.05', '+0.37', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.04\n",
      "['-0.01', '+0.98', '-0.15', '-0.18', '+0.45', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.82\n",
      "['-0.03', '+0.85', '-0.14', '-0.69', '+1.29', '+0.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -150.46\n",
      "['-0.11', '+0.56', '-0.64', '-1.28', '+2.02', '+0.98', '+0.00', '+0.00']\n",
      "step 60 total_reward -271.24\n",
      "['-0.25', '+0.08', '-0.66', '-1.93', '+3.36', '+1.29', '+0.00', '+0.00']\n",
      "step 80 total_reward -435.77\n",
      "['-0.25', '+0.05', '+0.45', '-0.17', '+3.42', '-1.42', '+0.00', '+0.00']\n",
      "step 81 total_reward -535.77\n",
      "['-0.01', '+0.94', '-0.68', '+0.10', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.69\n",
      "['-0.16', '+0.89', '-0.73', '-0.44', '+0.42', '+0.37', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.33', '+0.72', '-1.25', '-0.54', '+0.79', '+0.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -136.04\n",
      "['-0.70', '+0.56', '-2.46', '-0.57', '+1.14', '+0.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -302.11\n",
      "['-1.00', '+0.43', '-2.56', '-0.93', '+1.53', '+0.92', '+0.00', '+0.00']\n",
      "step 72 total_reward -473.69\n",
      "['+0.00', '+0.93', '+0.23', '-0.34', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.24\n",
      "['+0.04', '+0.75', '+0.11', '-0.87', '+0.32', '+0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.95\n",
      "['+0.03', '+0.49', '-0.01', '-1.03', '+0.56', '-0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -78.45\n",
      "['-0.01', '+0.16', '-0.30', '-1.20', '+0.28', '-0.31', '+0.00', '+0.00']\n",
      "step 60 total_reward -40.50\n",
      "['-0.05', '-0.03', '-0.27', '+0.00', '-0.00', '+0.00', '+1.00', '+1.00']\n",
      "step 71 total_reward -97.95\n",
      "['+0.00', '+0.92', '+0.08', '-0.55', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['+0.01', '+0.68', '-0.04', '-0.93', '+0.29', '+0.37', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.18\n",
      "['+0.00', '+0.35', '-0.06', '-1.16', '+0.34', '-0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.51\n",
      "['-0.04', '+0.02', '-0.25', '-1.08', '+0.12', '-0.32', '+0.00', '+0.00']\n",
      "step 60 total_reward +11.97\n",
      "['-0.05', '-0.03', '-0.09', '-0.00', '+0.00', '-0.00', '+1.00', '+1.00']\n",
      "step 63 total_reward -64.15\n",
      "['-0.01', '+0.94', '-0.46', '+0.08', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.11\n",
      "['-0.08', '+0.88', '-0.26', '-0.46', '-0.33', '-0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.82\n",
      "['-0.10', '+0.69', '+0.35', '-0.70', '-1.18', '-0.86', '+0.00', '+0.00']\n",
      "step 40 total_reward -127.24\n",
      "['+0.08', '+0.43', '+1.12', '-1.18', '-2.13', '-1.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -284.05\n",
      "['+0.33', '-0.03', '+1.26', '-1.82', '-3.54', '-1.27', '+0.00', '+0.00']\n",
      "step 80 total_reward -474.64\n",
      "['+0.34', '-0.04', '+1.59', '-0.29', '-3.36', '-0.33', '+0.00', '+0.00']\n",
      "step 81 total_reward -574.64\n",
      "['-0.01', '+0.95', '-0.70', '+0.41', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.09\n",
      "['-0.16', '+0.99', '-0.83', '-0.14', '+0.51', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.82\n",
      "['-0.32', '+0.88', '-0.79', '-0.67', '+1.00', '+0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -120.61\n",
      "['-0.56', '+0.61', '-1.85', '-1.15', '+1.79', '+0.80', '+0.00', '+0.00']\n",
      "step 60 total_reward -308.61\n",
      "['-1.01', '+0.18', '-2.40', '-1.86', '+2.69', '+1.26', '+0.00', '+0.00']\n",
      "step 79 total_reward -597.52\n",
      "['-0.01', '+0.95', '-0.30', '+0.19', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.13\n",
      "['-0.06', '+0.92', '-0.18', '-0.34', '-0.04', '-0.39', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.00\n",
      "['-0.09', '+0.74', '-0.05', '-0.78', '-0.67', '-0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -89.68\n",
      "['+0.00', '+0.51', '+0.75', '-0.94', '-1.37', '-0.88', '+0.00', '+0.00']\n",
      "step 60 total_reward -182.51\n",
      "['+0.15', '+0.14', '+0.70', '-1.57', '-2.74', '-1.68', '+0.00', '+0.00']\n",
      "step 80 total_reward -341.66\n",
      "['+0.20', '-0.00', '+0.18', '+0.01', '-3.04', '+0.08', '+0.00', '+0.00']\n",
      "step 86 total_reward -492.66\n",
      "['-0.01', '+0.93', '-0.54', '-0.37', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.25\n",
      "['-0.11', '+0.74', '-0.43', '-0.77', '-0.10', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.62\n",
      "['-0.18', '+0.51', '-0.38', '-0.77', '-0.13', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -0.96\n",
      "['-0.28', '+0.25', '-0.59', '-0.83', '+0.21', '+0.44', '+0.00', '+0.00']\n",
      "step 60 total_reward -11.27\n",
      "['-0.38', '-0.03', '-0.59', '-0.78', '+0.28', '-3.87', '+0.00', '+1.00']\n",
      "step 79 total_reward -139.66\n",
      "['-0.01', '+0.94', '-0.66', '+0.01', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.59\n",
      "['-0.15', '+0.86', '-0.72', '-0.52', '+0.46', '+0.41', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.12\n",
      "['-0.30', '+0.62', '-0.82', '-1.12', '+1.13', '+1.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -161.90\n",
      "['-0.47', '+0.19', '-0.87', '-1.77', '+2.68', '+1.66', '+0.00', '+0.00']\n",
      "step 60 total_reward -357.31\n",
      "['-0.53', '+0.03', '+0.37', '-0.48', '+3.05', '-4.10', '+0.00', '+0.00']\n",
      "step 66 total_reward -510.10\n",
      "['-0.01', '+0.93', '-0.64', '-0.48', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.37\n",
      "['-0.14', '+0.72', '-0.69', '-0.69', '+0.17', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.68\n",
      "['-0.30', '+0.48', '-0.92', '-1.03', '+0.54', '+0.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -81.90\n",
      "['-0.51', '+0.11', '-1.20', '-1.45', '+1.49', '+0.72', '+0.00', '+0.00']\n",
      "step 60 total_reward -224.23\n",
      "['-0.53', '+0.08', '-0.62', '+0.06', '+1.64', '-0.36', '+0.00', '+1.00']\n",
      "step 62 total_reward -319.71\n",
      "['-0.01', '+0.93', '-0.50', '-0.50', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.46\n",
      "['-0.10', '+0.71', '-0.39', '-0.76', '-0.09', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -5.74\n",
      "['-0.17', '+0.48', '-0.32', '-0.68', '-0.05', '+0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward +24.36\n",
      "['-0.25', '+0.24', '-0.64', '-0.70', '+0.37', '+0.47', '+0.00', '+0.00']\n",
      "step 60 total_reward -14.52\n",
      "['-0.37', '-0.03', '-0.55', '-1.13', '+0.47', '-0.05', '+0.00', '+1.00']\n",
      "step 80 total_reward -48.40\n",
      "['-0.38', '-0.06', '-0.69', '-0.78', '+0.41', '-3.48', '+0.00', '+1.00']\n",
      "step 82 total_reward -144.40\n",
      "['-0.01', '+0.93', '-0.38', '-0.28', '+0.01', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.65\n",
      "['-0.07', '+0.77', '-0.24', '-0.79', '-0.27', '-0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.96\n",
      "['-0.07', '+0.56', '+0.13', '-0.75', '-0.58', '-0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.97\n",
      "['+0.01', '+0.35', '+0.74', '-0.57', '-0.61', '-0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -59.86\n",
      "['+0.22', '+0.19', '+1.14', '-0.74', '-0.65', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -103.19\n",
      "['+0.41', '-0.01', '+0.30', '+0.21', '-1.33', '-0.82', '+1.00', '+0.00']\n",
      "step 97 total_reward -209.93\n",
      "['-0.00', '+0.93', '-0.11', '-0.53', '+0.00', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.34\n",
      "['-0.03', '+0.69', '-0.17', '-0.87', '+0.12', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.76\n",
      "['-0.08', '+0.49', '-0.36', '-0.54', '+0.22', '+0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward +0.56\n",
      "['-0.19', '+0.28', '-0.78', '-0.82', '+0.73', '+0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -84.49\n",
      "['-0.38', '+0.00', '-1.02', '-1.14', '+1.18', '+0.24', '+0.00', '+0.00']\n",
      "step 80 total_reward -177.26\n",
      "['-0.44', '-0.03', '-0.65', '+0.29', '+1.73', '+1.50', '+0.00', '+0.00']\n",
      "step 85 total_reward -310.50\n",
      "['-0.01', '+0.94', '-0.39', '+0.03', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.56\n",
      "['-0.08', '+0.87', '-0.29', '-0.50', '+0.01', '-0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.63\n",
      "['-0.12', '+0.70', '-0.08', '-0.40', '-0.26', '-0.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -9.61\n",
      "['-0.11', '+0.61', '+0.09', '-0.39', '-0.42', '+0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -19.62\n",
      "['-0.08', '+0.48', '+0.23', '-0.32', '-0.04', '+0.49', '+0.00', '+0.00']\n",
      "step 80 total_reward +28.00\n",
      "['-0.03', '+0.45', '+0.05', '+0.05', '+0.52', '+0.61', '+0.00', '+0.00']\n",
      "step 100 total_reward +10.86\n",
      "['-0.11', '+0.49', '-0.78', '+0.06', '+1.14', '+0.73', '+0.00', '+0.00']\n",
      "step 120 total_reward -132.14\n",
      "['-0.27', '+0.41', '-0.75', '-0.60', '+2.38', '+1.72', '+0.00', '+0.00']\n",
      "step 140 total_reward -274.02\n",
      "['-0.42', '+0.08', '-0.64', '-1.58', '+4.25', '+1.68', '+0.00', '+0.00']\n",
      "step 160 total_reward -531.90\n",
      "['-0.44', '+0.01', '-0.72', '-0.64', '+4.58', '+6.42', '+1.00', '+0.00']\n",
      "step 163 total_reward -655.02\n",
      "['+0.00', '+0.95', '+0.18', '+0.33', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.23\n",
      "['+0.03', '+0.97', '+0.16', '-0.20', '+0.21', '+0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -8.57\n",
      "['+0.05', '+0.82', '+0.06', '-0.75', '+0.61', '+0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -84.25\n",
      "['+0.06', '+0.52', '+0.07', '-1.19', '+0.98', '+0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -136.69\n",
      "['+0.03', '+0.13', '-0.55', '-1.34', '+0.78', '-0.22', '+0.00', '+0.00']\n",
      "step 80 total_reward -107.25\n",
      "['-0.02', '-0.00', '-0.63', '-0.10', '+0.65', '-1.93', '+0.00', '+1.00']\n",
      "step 87 total_reward -190.54\n",
      "['-0.01', '+0.93', '-0.48', '-0.40', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.44\n",
      "['-0.09', '+0.73', '-0.30', '-0.72', '-0.24', '-0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.39\n",
      "['-0.14', '+0.53', '-0.27', '-0.83', '-0.43', '+0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -34.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.19', '+0.25', '-0.31', '-0.93', '+0.02', '+0.62', '+0.00', '+0.00']\n",
      "step 60 total_reward +16.17\n",
      "['-0.23', '-0.04', '-0.58', '-0.34', '+0.39', '+3.22', '+1.00', '+1.00']\n",
      "step 78 total_reward -128.29\n",
      "['-0.00', '+0.94', '-0.05', '+0.11', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.52\n",
      "['+0.00', '+0.89', '+0.14', '-0.44', '-0.41', '-0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -68.54\n",
      "['+0.04', '+0.68', '+0.42', '-0.93', '-1.35', '-1.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -197.70\n",
      "['+0.13', '+0.31', '+0.35', '-1.60', '-2.87', '-1.92', '+0.00', '+0.00']\n",
      "step 60 total_reward -378.32\n",
      "['+0.18', '+0.01', '+0.42', '+0.00', '-3.97', '-1.26', '+0.00', '+1.00']\n",
      "step 72 total_reward -595.53\n",
      "['-0.01', '+0.94', '-0.36', '-0.11', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.24\n",
      "['-0.06', '+0.82', '-0.21', '-0.65', '-0.34', '-0.53', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.27\n",
      "['-0.05', '+0.63', '+0.50', '-0.56', '-0.86', '-0.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -97.30\n",
      "['+0.09', '+0.41', '+0.77', '-1.05', '-1.69', '-1.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -216.40\n",
      "['+0.30', '+0.00', '+0.63', '+0.03', '-3.07', '+0.00', '+0.00', '+0.00']\n",
      "step 80 total_reward -512.13\n",
      "['-0.01', '+0.95', '-0.38', '+0.40', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.02\n",
      "['-0.06', '+0.99', '-0.17', '-0.14', '-0.31', '-0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -0.92\n",
      "['-0.09', '+0.86', '-0.05', '-0.75', '-1.50', '-1.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -160.81\n",
      "['-0.04', '+0.52', '+0.46', '-1.69', '-2.95', '-1.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -375.47\n",
      "['+0.04', '+0.01', '+0.74', '-0.06', '-4.09', '-2.23', '+0.00', '+1.00']\n",
      "step 76 total_reward -607.38\n",
      "['+0.01', '+0.95', '+0.52', '+0.18', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.11\n",
      "['+0.10', '+0.92', '+0.31', '-0.37', '+0.33', '+0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.51\n",
      "['+0.16', '+0.73', '+0.31', '-0.89', '+1.16', '+0.67', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.87\n",
      "['+0.23', '+0.40', '+0.36', '-1.29', '+1.32', '-0.32', '+0.00', '+0.00']\n",
      "step 60 total_reward -161.94\n",
      "['+0.31', '-0.03', '+0.67', '-1.13', '+1.39', '+3.71', '+0.00', '+1.00']\n",
      "step 80 total_reward -141.98\n",
      "['+0.33', '-0.05', '+1.13', '-0.29', '+1.65', '+0.85', '+0.00', '+1.00']\n",
      "step 82 total_reward -261.02\n",
      "['-0.00', '+0.93', '-0.18', '-0.31', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.58\n",
      "['-0.05', '+0.75', '-0.26', '-0.79', '+0.27', '+0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -59.08\n",
      "['-0.13', '+0.54', '-0.56', '-0.89', '+0.64', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -101.85\n",
      "['-0.28', '+0.19', '-1.19', '-1.40', '+1.69', '+1.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -267.04\n",
      "['-0.44', '-0.09', '-1.12', '+0.04', '+2.42', '+3.38', '+0.00', '+1.00']\n",
      "step 73 total_reward -446.21\n",
      "['+0.01', '+0.94', '+0.59', '-0.14', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.24\n",
      "['+0.12', '+0.81', '+0.45', '-0.68', '+0.26', '+0.48', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.78\n",
      "['+0.17', '+0.61', '-0.08', '-0.60', '+0.75', '+0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.64\n",
      "['+0.14', '+0.37', '-0.25', '-0.90', '+0.85', '-0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -69.15\n",
      "['-0.01', '+0.12', '-1.25', '-0.77', '+0.65', '-0.23', '+0.00', '+0.00']\n",
      "step 80 total_reward -80.74\n",
      "['-0.33', '-0.01', '-1.42', '+0.16', '-0.17', '-0.00', '+1.00', '+0.00']\n",
      "step 100 total_reward -161.90\n",
      "['+0.01', '+0.93', '+0.33', '-0.48', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.89\n",
      "['+0.06', '+0.70', '+0.24', '-0.92', '+0.22', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.49\n",
      "['+0.11', '+0.41', '+0.31', '-1.20', '+0.20', '-0.39', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.24\n",
      "['+0.15', '+0.07', '+0.14', '-1.27', '-0.17', '-0.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -15.64\n",
      "['+0.15', '-0.02', '+0.16', '-0.90', '-0.09', '+4.75', '+1.00', '+1.00']\n",
      "step 65 total_reward -116.19\n",
      "['-0.01', '+0.95', '-0.63', '+0.29', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.90\n",
      "['-0.12', '+0.95', '-0.45', '-0.25', '-0.24', '-0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -6.35\n",
      "['-0.20', '+0.79', '-0.21', '-0.74', '-1.16', '-1.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -109.86\n",
      "['-0.16', '+0.53', '+0.30', '-1.11', '-1.98', '-0.39', '+0.00', '+0.00']\n",
      "step 60 total_reward -205.40\n",
      "['-0.09', '+0.12', '+0.45', '-1.63', '-1.98', '-0.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -221.06\n",
      "['-0.07', '+0.00', '+0.20', '-0.28', '-1.94', '+3.92', '+1.00', '+0.00']\n",
      "step 85 total_reward -329.17\n",
      "['-0.01', '+0.93', '-0.51', '-0.53', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.58\n",
      "['-0.10', '+0.70', '-0.37', '-0.74', '-0.05', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward +4.30\n",
      "['-0.19', '+0.43', '-0.52', '-1.16', '+0.16', '+0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -28.37\n",
      "['-0.30', '+0.03', '-0.59', '-1.50', '+0.73', '+0.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -105.34\n",
      "['-0.34', '-0.08', '-0.95', '-0.98', '+0.61', '-4.35', '+0.00', '+0.00']\n",
      "step 65 total_reward -205.97\n",
      "['+0.01', '+0.93', '+0.25', '-0.39', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.98\n",
      "['+0.05', '+0.73', '+0.17', '-0.87', '+0.20', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.07\n",
      "['+0.09', '+0.44', '+0.29', '-1.18', '+0.19', '-0.39', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.31\n",
      "['+0.14', '+0.09', '+0.23', '-1.31', '-0.25', '-0.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -40.75\n",
      "['+0.15', '-0.03', '+0.11', '-0.75', '-0.04', '+5.17', '+1.00', '+1.00']\n",
      "step 66 total_reward -140.60\n",
      "['-0.02', '+0.93', '-0.79', '-0.28', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.09\n",
      "['-0.17', '+0.77', '-0.80', '-0.67', '+0.22', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.29\n",
      "['-0.37', '+0.60', '-1.16', '-0.63', '+0.48', '+0.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -78.62\n",
      "['-0.64', '+0.38', '-1.49', '-1.00', '+1.24', '+1.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -207.32\n",
      "['-0.86', '+0.13', '-0.81', '-0.12', '+2.50', '+4.87', '+0.00', '+0.00']\n",
      "step 75 total_reward -449.05\n",
      "['+0.01', '+0.94', '+0.30', '-0.18', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.54\n",
      "['+0.05', '+0.80', '+0.18', '-0.72', '+0.29', '+0.41', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.59\n",
      "['+0.10', '+0.50', '+0.28', '-1.22', '+0.55', '-0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -103.14\n",
      "['+0.14', '+0.10', '+0.12', '-1.51', '+0.31', '-0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -73.71\n",
      "['+0.14', '-0.02', '+0.24', '-0.79', '+0.18', '-5.61', '+0.00', '+1.00']\n",
      "step 65 total_reward -171.64\n",
      "['-0.01', '+0.94', '-0.35', '-0.08', '+0.01', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.66\n",
      "['-0.06', '+0.83', '-0.20', '-0.62', '-0.34', '-0.58', '+0.00', '+0.00']\n",
      "step 20 total_reward -51.99\n",
      "['-0.06', '+0.63', '+0.40', '-0.65', '-0.93', '-0.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -106.36\n",
      "['+0.09', '+0.40', '+0.82', '-1.10', '-1.75', '-1.24', '+0.00', '+0.00']\n",
      "step 60 total_reward -229.24\n",
      "['+0.29', '+0.00', '+0.76', '+0.05', '-3.05', '+0.00', '+0.00', '+0.00']\n",
      "step 79 total_reward -495.30\n",
      "['+0.01', '+0.95', '+0.71', '+0.22', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.01\n",
      "['+0.15', '+0.93', '+0.57', '-0.31', '+0.10', '+0.46', '+0.00', '+0.00']\n",
      "step 20 total_reward +1.94\n",
      "['+0.25', '+0.76', '+0.30', '-0.63', '+0.73', '+0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.66\n",
      "['+0.24', '+0.56', '-0.16', '-0.83', '+1.26', '+0.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -106.60\n",
      "['+0.21', '+0.25', '-0.11', '-1.26', '+1.01', '-0.57', '+0.00', '+0.00']\n",
      "step 80 total_reward -94.86\n",
      "['+0.20', '-0.00', '+0.17', '-0.23', '+0.57', '-2.85', '+0.00', '+1.00']\n",
      "step 94 total_reward -75.05\n",
      "['+0.00', '+0.93', '+0.22', '-0.45', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.22\n",
      "['+0.04', '+0.71', '+0.10', '-0.92', '+0.25', '+0.31', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.51\n",
      "['+0.05', '+0.41', '+0.13', '-1.15', '+0.32', '-0.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -50.42\n",
      "['+0.04', '+0.13', '-0.11', '-0.78', '+0.06', '-0.23', '+0.00', '+0.00']\n",
      "step 60 total_reward +34.64\n",
      "['+0.02', '-0.03', '-0.04', '+0.03', '-0.02', '-0.14', '+1.00', '+1.00']\n",
      "step 72 total_reward -54.76\n",
      "['-0.00', '+0.96', '-0.12', '+0.47', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.01', '+1.01', '+0.06', '-0.07', '-0.40', '-0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -5.42\n",
      "['+0.01', '+0.90', '+0.17', '-0.71', '-1.62', '-1.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -180.10\n",
      "['+0.06', '+0.60', '+0.20', '-1.36', '-3.35', '-1.89', '+0.00', '+0.00']\n",
      "step 60 total_reward -389.34\n",
      "['+0.09', '+0.10', '+0.15', '-1.83', '-5.63', '-2.47', '+0.00', '+0.00']\n",
      "step 80 total_reward -617.07\n",
      "['+0.09', '-0.03', '+0.01', '+0.00', '-6.28', '+0.00', '+1.00', '+1.00']\n",
      "step 85 total_reward -756.13\n",
      "['+0.00', '+0.93', '+0.15', '-0.53', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.38\n",
      "['+0.04', '+0.69', '+0.28', '-0.93', '-0.35', '-0.42', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.82\n",
      "['+0.12', '+0.35', '+0.53', '-1.43', '-1.12', '-1.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -158.36\n",
      "['+0.23', '+0.00', '+0.56', '-0.31', '-1.97', '+3.72', '+1.00', '+0.00']\n",
      "step 55 total_reward -364.22\n",
      "['-0.01', '+0.93', '-0.75', '-0.26', '+0.02', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.23\n",
      "['-0.15', '+0.77', '-0.65', '-0.69', '-0.12', '-0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.62\n",
      "['-0.26', '+0.65', '-0.43', '-0.28', '-0.29', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward +17.89\n",
      "['-0.33', '+0.55', '-0.29', '-0.27', '-0.16', '+0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward +43.77\n",
      "['-0.39', '+0.43', '-0.42', '-0.66', '+0.37', '+0.90', '+0.00', '+0.00']\n",
      "step 80 total_reward -10.63\n",
      "['-0.52', '+0.17', '-1.15', '-1.02', '+1.51', '+1.18', '+0.00', '+0.00']\n",
      "step 100 total_reward -201.45\n",
      "['-0.62', '+0.06', '-0.93', '+0.14', '+2.01', '+1.35', '+0.00', '+1.00']\n",
      "step 107 total_reward -359.11\n",
      "['-0.00', '+0.95', '-0.15', '+0.14', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.54\n",
      "['-0.04', '+0.90', '-0.19', '-0.39', '+0.34', '+0.23', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.53\n",
      "['-0.08', '+0.71', '-0.29', '-0.73', '+0.47', '+0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -84.09\n",
      "['-0.18', '+0.47', '-0.61', '-1.06', '+0.80', '+0.67', '+0.00', '+0.00']\n",
      "step 60 total_reward -140.92\n",
      "['-0.35', '+0.09', '-1.09', '-1.45', '+1.69', '+0.66', '+0.00', '+0.00']\n",
      "step 80 total_reward -278.45\n",
      "['-0.37', '+0.05', '-0.88', '-0.13', '+1.72', '-2.32', '+0.00', '+1.00']\n",
      "step 82 total_reward -383.64\n",
      "['-0.00', '+0.93', '-0.01', '-0.48', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.02\n",
      "['+0.00', '+0.71', '+0.08', '-0.79', '-0.03', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.24\n",
      "['+0.02', '+0.47', '+0.20', '-0.97', '-0.25', '-0.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -36.80\n",
      "['+0.12', '+0.19', '+0.73', '-0.98', '-0.91', '-0.65', '+0.00', '+0.00']\n",
      "step 60 total_reward -104.82\n",
      "['+0.22', '+0.02', '+0.41', '+0.17', '-1.83', '-1.47', '+1.00', '+0.00']\n",
      "step 74 total_reward -261.99\n",
      "['+0.00', '+0.94', '+0.03', '-0.03', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.22\n",
      "['-0.01', '+0.85', '-0.14', '-0.57', '+0.45', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -92.12\n",
      "['-0.08', '+0.64', '-0.91', '-0.73', '+1.16', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -202.60\n",
      "['-0.27', '+0.32', '-1.00', '-1.45', '+2.36', '+1.56', '+0.00', '+0.00']\n",
      "step 60 total_reward -361.21\n",
      "['-0.47', '-0.10', '-1.87', '-0.50', '+3.61', '+0.60', '+0.00', '+0.00']\n",
      "step 76 total_reward -629.80\n",
      "['-0.01', '+0.95', '-0.39', '+0.45', '+0.01', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.69\n",
      "['-0.07', '+1.01', '-0.19', '-0.10', '-0.38', '-0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.87\n",
      "['-0.10', '+0.89', '-0.08', '-0.72', '-1.59', '-1.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -165.04\n",
      "['-0.08', '+0.56', '+0.24', '-1.63', '-3.18', '-1.51', '+0.00', '+0.00']\n",
      "step 60 total_reward -385.21\n",
      "['-0.03', '-0.01', '+0.33', '-1.02', '-4.55', '-8.68', '+0.00', '+1.00']\n",
      "step 78 total_reward -624.54\n",
      "['+0.00', '+0.94', '+0.17', '+0.03', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.42\n",
      "['+0.02', '+0.87', '-0.03', '-0.52', '+0.40', '+0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -66.74\n",
      "['-0.00', '+0.64', '-0.50', '-0.89', '+1.25', '+0.84', '+0.00', '+0.00']\n",
      "step 40 total_reward -181.33\n",
      "['-0.12', '+0.30', '-0.59', '-1.47', '+2.19', '+1.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -301.38\n",
      "['-0.23', '-0.00', '-1.05', '-0.86', '+3.12', '+7.15', '+0.00', '+0.00']\n",
      "step 72 total_reward -514.58\n",
      "['+0.01', '+0.94', '+0.50', '-0.11', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.53\n",
      "['+0.10', '+0.82', '+0.40', '-0.65', '+0.26', '+0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.33\n",
      "['+0.17', '+0.57', '+0.35', '-0.97', '+0.52', '+0.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -70.20\n",
      "['+0.25', '+0.21', '+0.50', '-1.46', '+0.08', '-0.69', '+0.00', '+0.00']\n",
      "step 60 total_reward -50.76\n",
      "['+0.29', '+0.02', '-0.14', '-0.57', '-0.18', '+4.85', '+1.00', '+0.00']\n",
      "step 68 total_reward -164.78\n",
      "['+0.01', '+0.93', '+0.42', '-0.45', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.66\n",
      "['+0.08', '+0.71', '+0.34', '-0.91', '+0.19', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.62\n",
      "['+0.16', '+0.39', '+0.48', '-1.31', '+0.09', '-0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.56\n",
      "['+0.24', '-0.02', '+0.40', '-1.57', '-0.38', '-0.27', '+0.00', '+0.00']\n",
      "step 60 total_reward -73.75\n",
      "['+0.26', '-0.05', '+1.02', '-0.32', '-0.44', '-0.00', '+1.00', '+1.00']\n",
      "step 62 total_reward -157.59\n",
      "['+0.01', '+0.93', '+0.32', '-0.37', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.98\n",
      "['+0.06', '+0.74', '+0.23', '-0.90', '+0.24', '+0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.06\n",
      "['+0.12', '+0.40', '+0.38', '-1.36', '+0.29', '-0.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -70.57\n",
      "['+0.18', '-0.02', '-0.00', '-0.00', '+0.08', '-0.00', '+1.00', '+1.00']\n",
      "step 58 total_reward -147.02\n",
      "['-0.01', '+0.92', '-0.32', '-0.55', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.64\n",
      "['-0.08', '+0.69', '-0.41', '-0.81', '+0.30', '+0.35', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.92\n",
      "['-0.21', '+0.46', '-0.92', '-0.85', '+0.79', '+0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -106.67\n",
      "['-0.46', '+0.14', '-1.63', '-1.28', '+1.64', '+0.65', '+0.00', '+0.00']\n",
      "step 60 total_reward -274.67\n",
      "['-0.63', '-0.08', '-0.53', '+0.13', '+2.05', '+1.71', '+0.00', '+0.00']\n",
      "step 71 total_reward -441.13\n",
      "['-0.01', '+0.94', '-0.33', '-0.15', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.40\n",
      "['-0.08', '+0.81', '-0.39', '-0.69', '+0.33', '+0.35', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.81\n",
      "['-0.20', '+0.61', '-0.98', '-0.55', '+0.69', '+0.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -122.81\n",
      "['-0.44', '+0.39', '-1.25', '-1.05', '+1.33', '+1.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -233.03\n",
      "['-0.77', '-0.01', '-1.83', '-1.65', '+2.54', '+1.25', '+0.00', '+0.00']\n",
      "step 80 total_reward -458.38\n",
      "['-0.79', '-0.01', '-1.82', '-0.23', '+2.75', '+4.81', '+0.00', '+0.00']\n",
      "step 81 total_reward -558.38\n",
      "['-0.01', '+0.94', '-0.50', '+0.08', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.36\n",
      "['-0.11', '+0.89', '-0.42', '-0.44', '+0.17', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.57\n",
      "['-0.19', '+0.69', '-0.41', '-0.72', '-0.05', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -17.35\n",
      "['-0.29', '+0.39', '-0.62', '-1.25', '+0.24', '+0.67', '+0.00', '+0.00']\n",
      "step 60 total_reward -70.61\n",
      "['-0.38', '+0.07', '-0.07', '+0.01', '+0.76', '+0.19', '+0.00', '+1.00']\n",
      "step 76 total_reward -230.39\n",
      "['-0.00', '+0.94', '-0.04', '-0.16', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.03\n",
      "['+0.01', '+0.80', '+0.11', '-0.71', '-0.42', '-0.61', '+0.00', '+0.00']\n",
      "step 20 total_reward -85.80\n",
      "['+0.06', '+0.56', '+0.54', '-0.95', '-1.10', '-0.94', '+0.00', '+0.00']\n",
      "step 40 total_reward -169.96\n",
      "['+0.18', '+0.18', '+0.53', '-1.58', '-2.51', '-1.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -338.79\n",
      "['+0.23', '-0.01', '+0.76', '-0.48', '-3.28', '-6.09', '+0.00', '+0.00']\n",
      "step 68 total_reward -510.77\n",
      "['+0.01', '+0.93', '+0.72', '-0.24', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.06\n",
      "['+0.14', '+0.78', '+0.62', '-0.78', '+0.17', '+0.29', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.92\n",
      "['+0.24', '+0.61', '+0.32', '-0.41', '+0.52', '+0.43', '+0.00', '+0.00']\n",
      "step 40 total_reward -5.18\n",
      "['+0.32', '+0.42', '+0.45', '-0.83', '+0.55', '-0.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -38.80\n",
      "['+0.42', '+0.10', '+0.63', '-1.34', '-0.23', '-1.17', '+1.00', '+0.00']\n",
      "step 80 total_reward -40.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.43', '+0.06', '-0.07', '-0.26', '-0.25', '+2.97', '+1.00', '+0.00']\n",
      "step 82 total_reward -138.58\n",
      "['-0.00', '+0.94', '-0.07', '+0.08', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.83\n",
      "['-0.00', '+0.88', '+0.09', '-0.46', '-0.27', '-0.57', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.78\n",
      "['+0.03', '+0.67', '+0.49', '-0.80', '-0.95', '-0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -152.81\n",
      "['+0.16', '+0.36', '+0.66', '-1.38', '-1.97', '-1.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -287.30\n",
      "['+0.25', '+0.03', '-0.38', '-0.49', '-3.11', '+3.94', '+0.00', '+0.00']\n",
      "step 74 total_reward -515.08\n",
      "['-0.01', '+0.94', '-0.28', '+0.03', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.28\n",
      "['-0.08', '+0.87', '-0.43', '-0.51', '+0.51', '+0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.54\n",
      "['-0.21', '+0.66', '-1.20', '-0.74', '+1.16', '+0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -208.73\n",
      "['-0.48', '+0.37', '-1.36', '-1.32', '+2.16', '+1.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -348.81\n",
      "['-0.77', '-0.07', '-0.13', '+0.05', '+3.45', '+0.44', '+0.00', '+0.00']\n",
      "step 77 total_reward -661.79\n",
      "['-0.01', '+0.94', '-0.62', '-0.17', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.02\n",
      "['-0.13', '+0.80', '-0.58', '-0.70', '+0.12', '+0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.02\n",
      "['-0.27', '+0.64', '-0.76', '-0.47', '+0.15', '+0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -21.66\n",
      "['-0.44', '+0.50', '-1.02', '-0.58', '+0.44', '+0.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -79.47\n",
      "['-0.70', '+0.28', '-1.79', '-0.77', '+1.23', '+0.86', '+0.00', '+0.00']\n",
      "step 80 total_reward -248.52\n",
      "['-1.01', '+0.02', '-1.81', '-1.29', '+2.27', '+1.48', '+0.00', '+0.00']\n",
      "step 97 total_reward -496.79\n",
      "['-0.00', '+0.94', '-0.23', '-0.11', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.11\n",
      "['-0.04', '+0.82', '-0.14', '-0.65', '-0.26', '-0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -55.37\n",
      "['-0.04', '+0.63', '+0.34', '-0.51', '-0.60', '-0.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -69.61\n",
      "['+0.08', '+0.45', '+0.70', '-0.88', '-1.18', '-0.99', '+0.00', '+0.00']\n",
      "step 60 total_reward -163.70\n",
      "['+0.28', '+0.11', '+1.08', '-1.43', '-2.30', '-1.11', '+0.00', '+0.00']\n",
      "step 80 total_reward -328.61\n",
      "['+0.33', '+0.03', '+0.90', '-0.24', '-2.65', '-4.82', '+0.00', '+0.00']\n",
      "step 84 total_reward -453.34\n",
      "['+0.01', '+0.95', '+0.49', '+0.41', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.45\n",
      "['+0.10', '+0.99', '+0.45', '-0.12', '+0.06', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward +8.99\n",
      "['+0.18', '+0.87', '+0.29', '-0.68', '+0.44', '+0.76', '+0.00', '+0.00']\n",
      "step 40 total_reward -45.54\n",
      "['+0.22', '+0.60', '-0.04', '-1.04', '+1.23', '+0.67', '+0.00', '+0.00']\n",
      "step 60 total_reward -133.20\n",
      "['+0.22', '+0.23', '-0.00', '-1.47', '+1.39', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -158.81\n",
      "['+0.09', '+0.01', '-1.09', '-0.16', '+2.74', '+3.33', '+0.00', '+0.00']\n",
      "step 98 total_reward -329.22\n",
      "['-0.00', '+0.93', '-0.10', '-0.30', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.69\n",
      "['-0.01', '+0.76', '+0.02', '-0.76', '-0.30', '-0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -59.57\n",
      "['+0.05', '+0.58', '+0.53', '-0.64', '-0.73', '-0.58', '+0.00', '+0.00']\n",
      "step 40 total_reward -96.63\n",
      "['+0.17', '+0.30', '+0.94', '-1.23', '-1.75', '-1.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -248.28\n",
      "['+0.32', '+0.04', '+0.79', '-0.52', '-2.78', '-6.41', '+0.00', '+0.00']\n",
      "step 73 total_reward -461.97\n",
      "['+0.02', '+0.94', '+0.77', '-0.12', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.32\n",
      "['+0.15', '+0.82', '+0.60', '-0.66', '+0.25', '+0.53', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.85\n",
      "['+0.24', '+0.59', '+0.22', '-0.75', '+0.76', '+0.33', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.08\n",
      "['+0.30', '+0.30', '+0.36', '-1.19', '+0.62', '-0.56', '+0.00', '+0.00']\n",
      "step 60 total_reward -59.67\n",
      "['+0.36', '-0.04', '+0.67', '-0.89', '+0.09', '-5.57', '+0.00', '+1.00']\n",
      "step 76 total_reward -133.65\n",
      "['+0.01', '+0.95', '+0.53', '+0.34', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.17\n",
      "['+0.11', '+0.97', '+0.43', '-0.19', '+0.15', '+0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward +0.61\n",
      "['+0.18', '+0.83', '+0.29', '-0.77', '+0.87', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -93.69\n",
      "['+0.16', '+0.55', '-0.34', '-1.12', '+1.73', '+0.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -191.65\n",
      "['+0.03', '+0.12', '-1.19', '-1.84', '+1.99', '+0.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -277.13\n",
      "['-0.03', '-0.00', '-1.26', '-0.28', '+1.83', '-3.87', '+0.00', '+1.00']\n",
      "step 85 total_reward -389.94\n",
      "['-0.02', '+0.95', '-0.77', '+0.18', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.55\n",
      "['-0.18', '+0.91', '-0.94', '-0.38', '+0.65', '+0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -86.50\n",
      "['-0.38', '+0.71', '-1.20', '-1.05', '+2.05', '+1.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -272.85\n",
      "['-0.71', '+0.26', '-1.51', '-1.76', '+4.03', '+2.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -540.55\n",
      "['-0.97', '-0.12', '-2.12', '-0.42', '+4.61', '-1.25', '+0.00', '+0.00']\n",
      "step 76 total_reward -704.89\n",
      "['+0.01', '+0.94', '+0.56', '-0.07', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.56\n",
      "['+0.11', '+0.83', '+0.39', '-0.61', '+0.30', '+0.61', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.16\n",
      "['+0.15', '+0.62', '-0.18', '-0.70', '+0.91', '+0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -78.82\n",
      "['+0.11', '+0.35', '-0.45', '-1.02', '+1.09', '-0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -111.34\n",
      "['-0.09', '+0.02', '-1.51', '-1.13', '+0.97', '-0.04', '+0.00', '+1.00']\n",
      "step 80 total_reward -145.35\n",
      "['-0.12', '+0.01', '-1.40', '-0.03', '+1.11', '+2.03', '+0.00', '+1.00']\n",
      "step 82 total_reward -253.15\n",
      "['-0.02', '+0.95', '-0.77', '+0.39', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.14\n",
      "['-0.18', '+0.99', '-0.93', '-0.16', '+0.59', '+0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -71.43\n",
      "['-0.37', '+0.86', '-0.96', '-0.75', '+1.59', '+1.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -192.46\n",
      "['-0.69', '+0.49', '-1.97', '-1.82', '+2.93', '+1.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -471.00\n",
      "['-0.97', '+0.06', '-1.18', '-0.64', '+4.16', '+9.06', '+0.00', '+0.00']\n",
      "step 74 total_reward -706.35\n",
      "['-0.00', '+0.93', '-0.12', '-0.31', '+0.00', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.70\n",
      "['-0.03', '+0.76', '-0.11', '-0.80', '+0.11', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.79\n",
      "['-0.06', '+0.56', '-0.28', '-0.56', '+0.21', '+0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -21.91\n",
      "['-0.14', '+0.39', '-0.47', '-0.83', '+0.66', '+0.78', '+0.00', '+0.00']\n",
      "step 60 total_reward -85.70\n",
      "['-0.29', '+0.10', '-0.96', '-1.13', '+1.56', '+0.68', '+0.00', '+0.00']\n",
      "step 80 total_reward -221.67\n",
      "['-0.42', '-0.10', '-1.10', '-0.24', '+2.65', '+5.30', '+0.00', '+0.00']\n",
      "step 92 total_reward -421.56\n",
      "['-0.00', '+0.94', '-0.20', '-0.16', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.60\n",
      "['-0.03', '+0.80', '-0.10', '-0.69', '-0.12', '-0.29', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.86\n",
      "['-0.03', '+0.63', '+0.27', '-0.44', '-0.39', '-0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.14\n",
      "['+0.07', '+0.48', '+0.59', '-0.78', '-0.86', '-0.84', '+0.00', '+0.00']\n",
      "step 60 total_reward -122.38\n",
      "['+0.24', '+0.17', '+1.05', '-1.31', '-1.92', '-1.06', '+0.00', '+0.00']\n",
      "step 80 total_reward -280.79\n",
      "['+0.35', '-0.03', '+1.37', '-0.29', '-2.36', '+0.91', '+1.00', '+0.00']\n",
      "step 90 total_reward -452.03\n",
      "['-0.01', '+0.95', '-0.58', '+0.46', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.73\n",
      "['-0.14', '+1.01', '-0.76', '-0.10', '+0.59', '+0.96', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.89\n",
      "['-0.30', '+0.90', '-0.80', '-0.66', '+1.75', '+1.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -203.00\n",
      "['-0.52', '+0.57', '-1.31', '-1.71', '+3.09', '+1.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -436.34\n",
      "['-0.73', '+0.02', '-1.27', '-0.91', '+4.42', '+8.87', '+1.00', '+0.00']\n",
      "step 77 total_reward -701.38\n",
      "['+0.01', '+0.94', '+0.53', '+0.01', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.88\n",
      "['+0.10', '+0.86', '+0.37', '-0.53', '+0.31', '+0.56', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.85\n",
      "['+0.18', '+0.64', '+0.18', '-0.76', '+0.62', '+0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -61.00\n",
      "['+0.23', '+0.34', '+0.35', '-1.22', '+0.38', '-0.65', '+0.00', '+0.00']\n",
      "step 60 total_reward -60.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.30', '-0.07', '+1.14', '-0.33', '-0.41', '+0.00', '+1.00', '+1.00']\n",
      "step 79 total_reward -190.12\n",
      "['-0.01', '+0.95', '-0.36', '+0.32', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.05\n",
      "['-0.09', '+0.96', '-0.55', '-0.23', '+0.53', '+0.89', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.44\n",
      "['-0.19', '+0.82', '-0.47', '-0.74', '+1.05', '+0.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.98\n",
      "['-0.33', '+0.53', '-0.75', '-1.32', '+1.91', '+1.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -262.84\n",
      "['-0.47', '+0.09', '-1.05', '-0.13', '+3.32', '+0.00', '+0.00', '+0.00']\n",
      "step 78 total_reward -545.40\n",
      "['-0.01', '+0.95', '-0.38', '+0.15', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.85\n",
      "['-0.09', '+0.91', '-0.39', '-0.37', '+0.36', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.20\n",
      "['-0.17', '+0.74', '-0.45', '-0.61', '+0.34', '-0.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -52.43\n",
      "['-0.28', '+0.53', '-0.67', '-0.92', '+0.59', '+0.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -102.74\n",
      "['-0.44', '+0.18', '-1.04', '-1.42', '+1.56', '+1.01', '+0.00', '+0.00']\n",
      "step 80 total_reward -250.10\n",
      "['-0.60', '-0.13', '-1.82', '-0.40', '+2.54', '+2.52', '+0.00', '+1.00']\n",
      "step 94 total_reward -445.52\n",
      "['-0.01', '+0.95', '-0.30', '+0.23', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.01\n",
      "['-0.05', '+0.93', '-0.10', '-0.32', '-0.39', '-0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.49\n",
      "['-0.06', '+0.75', '+0.15', '-0.87', '-1.54', '-1.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -184.15\n",
      "['+0.08', '+0.39', '+0.71', '-1.55', '-2.94', '-1.71', '+0.00', '+0.00']\n",
      "step 60 total_reward -373.72\n",
      "['+0.19', '+0.01', '+1.02', '-0.15', '-4.19', '-3.57', '+0.00', '+1.00']\n",
      "step 74 total_reward -611.47\n",
      "['-0.01', '+0.95', '-0.61', '+0.18', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.51\n",
      "['-0.13', '+0.92', '-0.50', '-0.35', '+0.08', '-0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -3.60\n",
      "['-0.22', '+0.75', '-0.33', '-0.54', '-0.34', '-0.43', '+0.00', '+0.00']\n",
      "step 40 total_reward -19.95\n",
      "['-0.28', '+0.57', '-0.36', '-0.83', '-0.51', '+0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -50.20\n",
      "['-0.36', '+0.24', '-0.47', '-1.27', '+0.11', '+0.80', '+0.00', '+0.00']\n",
      "step 80 total_reward -37.66\n",
      "['-0.40', '+0.04', '-0.06', '-0.50', '+0.22', '-4.68', '+0.00', '+1.00']\n",
      "step 90 total_reward -160.49\n",
      "['-0.01', '+0.94', '-0.75', '+0.13', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.74\n",
      "['-0.18', '+0.90', '-0.89', '-0.42', '+0.62', '+0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.18\n",
      "['-0.36', '+0.69', '-0.94', '-1.04', '+1.67', '+1.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -214.73\n",
      "['-0.55', '+0.29', '-0.80', '-1.62', '+3.61', '+2.32', '+0.00', '+0.00']\n",
      "step 60 total_reward -434.34\n",
      "['-0.59', '+0.07', '+0.76', '-0.06', '+3.74', '-2.21', '+1.00', '+0.00']\n",
      "step 71 total_reward -442.28\n",
      "['+0.01', '+0.96', '+0.40', '+0.47', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.14\n",
      "['+0.08', '+1.01', '+0.39', '-0.06', '-0.09', '-0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward +8.18\n",
      "['+0.16', '+0.91', '+0.32', '-0.59', '-0.12', '+0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -12.83\n",
      "['+0.22', '+0.70', '+0.25', '-0.59', '+0.21', '+0.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -3.61\n",
      "['+0.24', '+0.56', '+0.11', '-0.59', '+0.42', '-0.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -11.10\n",
      "['+0.28', '+0.31', '+0.30', '-1.09', '-0.10', '-0.90', '+0.00', '+0.00']\n",
      "step 100 total_reward -13.64\n",
      "['+0.33', '-0.02', '+0.39', '-0.12', '-0.82', '+1.54', '+1.00', '+0.00']\n",
      "step 117 total_reward -186.73\n",
      "['+0.01', '+0.96', '+0.64', '+0.47', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.19\n",
      "['+0.12', '+1.01', '+0.44', '-0.07', '+0.32', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.29\n",
      "['+0.20', '+0.90', '+0.32', '-0.70', '+1.49', '+1.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -142.35\n",
      "['+0.20', '+0.56', '-0.24', '-1.78', '+3.16', '+1.63', '+0.00', '+0.00']\n",
      "step 60 total_reward -384.16\n",
      "['+0.17', '-0.00', '+0.43', '-0.15', '+4.42', '+0.92', '+1.00', '+0.00']\n",
      "step 78 total_reward -631.77\n",
      "['+0.01', '+0.95', '+0.32', '+0.33', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.32\n",
      "['+0.08', '+0.97', '+0.51', '-0.23', '-0.53', '-0.91', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.99\n",
      "['+0.18', '+0.82', '+0.53', '-0.81', '-1.55', '-1.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -194.32\n",
      "['+0.40', '+0.46', '+1.20', '-1.56', '-2.92', '-1.62', '+0.00', '+0.00']\n",
      "step 60 total_reward -413.22\n",
      "['+0.61', '-0.04', '-0.38', '-0.35', '-4.62', '+3.08', '+0.00', '+1.00']\n",
      "step 78 total_reward -702.03\n",
      "['+0.00', '+0.94', '+0.08', '+0.07', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.59\n",
      "['+0.00', '+0.88', '-0.11', '-0.48', '+0.45', '+0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -74.71\n",
      "['-0.04', '+0.67', '-0.61', '-0.85', '+1.24', '+0.78', '+0.00', '+0.00']\n",
      "step 40 total_reward -190.50\n",
      "['-0.28', '+0.35', '-1.42', '-1.38', '+2.08', '+1.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -350.41\n",
      "['-0.54', '+0.01', '-2.38', '-0.61', '+2.94', '+4.64', '+0.00', '+0.00']\n",
      "step 74 total_reward -606.93\n",
      "['+0.01', '+0.95', '+0.51', '+0.44', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.43\n",
      "['+0.09', '+1.00', '+0.30', '-0.11', '+0.35', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -3.51\n",
      "['+0.14', '+0.88', '+0.18', '-0.74', '+1.60', '+1.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -162.17\n",
      "['+0.16', '+0.58', '+0.10', '-1.26', '+3.24', '+1.36', '+0.00', '+0.00']\n",
      "step 60 total_reward -347.68\n",
      "['+0.16', '+0.11', '-0.02', '-1.83', '+4.20', '+0.78', '+0.00', '+0.00']\n",
      "step 80 total_reward -459.34\n",
      "['+0.16', '-0.00', '-0.19', '-0.62', '+4.44', '+6.43', '+1.00', '+0.00']\n",
      "step 84 total_reward -575.58\n",
      "['+0.01', '+0.93', '+0.74', '-0.37', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.42\n",
      "['+0.15', '+0.73', '+0.63', '-0.91', '+0.19', '+0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.82\n",
      "['+0.26', '+0.46', '+0.57', '-1.09', '+0.34', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.90\n",
      "['+0.38', '+0.05', '+0.66', '-1.61', '-0.11', '-0.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -50.00\n",
      "['+0.43', '-0.15', '+1.32', '-0.41', '-0.41', '+0.38', '+1.00', '+1.00']\n",
      "step 68 total_reward -167.59\n",
      "['-0.01', '+0.94', '-0.25', '+0.12', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.64\n",
      "['-0.04', '+0.90', '-0.05', '-0.43', '-0.40', '-0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.34\n",
      "['-0.04', '+0.69', '+0.33', '-0.87', '-1.26', '-0.85', '+0.00', '+0.00']\n",
      "step 40 total_reward -166.32\n",
      "['+0.10', '+0.37', '+0.82', '-1.44', '-2.30', '-1.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -315.07\n",
      "['+0.24', '+0.02', '-0.04', '-0.41', '-2.96', '+4.34', '+0.00', '+0.00']\n",
      "step 74 total_reward -542.83\n",
      "['+0.01', '+0.93', '+0.73', '-0.32', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.05\n",
      "['+0.15', '+0.75', '+0.62', '-0.78', '+0.13', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.26\n",
      "['+0.25', '+0.59', '+0.38', '-0.46', '+0.33', '+0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward +11.54\n",
      "['+0.34', '+0.37', '+0.58', '-0.96', '+0.03', '-0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward +1.92\n",
      "['+0.48', '+0.11', '+1.04', '-0.75', '-0.72', '-0.75', '+0.00', '+0.00']\n",
      "step 80 total_reward -88.23\n",
      "['+0.72', '-0.07', '+0.91', '+0.38', '-1.84', '-2.72', '+1.00', '+0.00']\n",
      "step 98 total_reward -313.69\n",
      "['+0.00', '+0.94', '+0.12', '+0.03', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.86\n",
      "['+0.01', '+0.87', '+0.04', '-0.50', '+0.34', '+0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.09\n",
      "['+0.01', '+0.67', '-0.35', '-0.64', '+0.68', '+0.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -101.94\n",
      "['-0.15', '+0.49', '-1.35', '-0.59', '+1.04', '+0.36', '+0.00', '+0.00']\n",
      "step 60 total_reward -203.09\n",
      "['-0.47', '+0.23', '-2.03', '-1.13', '+1.71', '+0.82', '+0.00', '+0.00']\n",
      "step 80 total_reward -359.54\n",
      "['-0.78', '-0.01', '-2.06', '+0.12', '+2.33', '+2.28', '+0.00', '+0.00']\n",
      "step 93 total_reward -601.44\n",
      "['-0.01', '+0.95', '-0.70', '+0.36', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.36\n",
      "['-0.17', '+0.98', '-0.83', '-0.18', '+0.58', '+0.73', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.99\n",
      "['-0.33', '+0.84', '-0.85', '-0.76', '+1.36', '+1.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -166.65\n",
      "['-0.62', '+0.50', '-1.95', '-1.64', '+2.47', '+1.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -412.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.95', '+0.01', '-1.37', '-0.06', '+3.62', '-2.06', '+0.00', '+0.00']\n",
      "step 77 total_reward -663.37\n",
      "['+0.01', '+0.94', '+0.64', '-0.17', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.10\n",
      "['+0.12', '+0.80', '+0.46', '-0.71', '+0.30', '+0.59', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.23\n",
      "['+0.21', '+0.54', '+0.47', '-1.07', '+0.73', '+0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -88.59\n",
      "['+0.32', '+0.14', '+0.57', '-1.55', '+0.40', '-0.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -81.71\n",
      "['+0.36', '-0.05', '+0.90', '-0.84', '-0.06', '-5.96', '+0.00', '+1.00']\n",
      "step 68 total_reward -169.05\n",
      "['+0.01', '+0.94', '+0.63', '+0.03', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.56\n",
      "['+0.12', '+0.87', '+0.43', '-0.51', '+0.31', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.82\n",
      "['+0.18', '+0.65', '-0.09', '-0.80', '+1.16', '+0.87', '+0.00', '+0.00']\n",
      "step 40 total_reward -108.66\n",
      "['+0.17', '+0.35', '-0.11', '-1.22', '+1.52', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -158.40\n",
      "['+0.12', '+0.00', '-0.24', '+0.06', '+1.95', '+0.73', '+0.00', '+1.00']\n",
      "step 77 total_reward -260.57\n",
      "['+0.00', '+0.94', '+0.09', '-0.04', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.45\n",
      "['+0.00', '+0.84', '-0.07', '-0.58', '+0.41', '+0.67', '+0.00', '+0.00']\n",
      "step 20 total_reward -80.62\n",
      "['-0.03', '+0.61', '-0.60', '-0.86', '+1.08', '+0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -172.77\n",
      "['-0.21', '+0.30', '-1.05', '-1.40', '+2.05', '+1.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -318.37\n",
      "['-0.37', '+0.03', '-0.80', '+0.34', '+3.01', '+3.04', '+0.00', '+0.00']\n",
      "step 72 total_reward -531.21\n",
      "['+0.01', '+0.94', '+0.31', '-0.17', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.15\n",
      "['+0.05', '+0.80', '+0.19', '-0.71', '+0.30', '+0.46', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.73\n",
      "['+0.09', '+0.53', '+0.22', '-1.09', '+0.65', '+0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -102.55\n",
      "['+0.12', '+0.16', '+0.07', '-1.37', '+0.46', '-0.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -77.84\n",
      "['+0.13', '-0.01', '+0.18', '-0.64', '+0.28', '-5.36', '+0.00', '+1.00']\n",
      "step 68 total_reward -170.79\n",
      "['+0.01', '+0.93', '+0.27', '-0.24', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.49\n",
      "['+0.05', '+0.78', '+0.17', '-0.77', '+0.28', '+0.38', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.47\n",
      "['+0.06', '+0.55', '+0.03', '-0.89', '+0.59', '+0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -76.83\n",
      "['+0.05', '+0.25', '-0.22', '-0.85', '+0.31', '-0.31', '+0.00', '+0.00']\n",
      "step 60 total_reward -22.45\n",
      "['-0.03', '+0.07', '-0.50', '-0.40', '-0.03', '-0.37', '+0.00', '+0.00']\n",
      "step 80 total_reward +41.84\n",
      "['-0.12', '-0.03', '-0.37', '-0.11', '+0.03', '-0.61', '+1.00', '+0.00']\n",
      "step 99 total_reward -48.16\n",
      "['-0.01', '+0.92', '-0.45', '-0.56', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.47\n",
      "['-0.09', '+0.69', '-0.40', '-0.78', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward +2.60\n",
      "['-0.18', '+0.41', '-0.54', '-1.18', '+0.24', '+0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -38.77\n",
      "['-0.30', '+0.02', '-0.66', '-1.45', '+0.88', '+0.32', '+0.00', '+1.00']\n",
      "step 60 total_reward -110.55\n",
      "['-0.33', '-0.02', '-0.79', '-0.17', '+0.86', '-1.54', '+0.00', '+1.00']\n",
      "step 63 total_reward -194.94\n",
      "['+0.01', '+0.94', '+0.68', '+0.12', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.86\n",
      "['+0.13', '+0.90', '+0.50', '-0.41', '+0.20', '+0.57', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.53\n",
      "['+0.24', '+0.70', '+0.47', '-0.87', '+0.70', '+0.31', '+0.00', '+0.00']\n",
      "step 40 total_reward -79.14\n",
      "['+0.35', '+0.36', '+0.62', '-1.32', '+0.56', '-0.56', '+0.00', '+0.00']\n",
      "step 60 total_reward -89.64\n",
      "['+0.48', '-0.11', '+0.82', '-1.82', '-0.44', '-1.74', '+1.00', '+1.00']\n",
      "step 80 total_reward -110.93\n",
      "['+0.50', '-0.13', '+1.46', '-0.41', '-0.40', '+0.00', '+1.00', '+1.00']\n",
      "step 81 total_reward -210.93\n",
      "['-0.00', '+0.94', '-0.02', '-0.18', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.53\n",
      "['-0.02', '+0.80', '-0.13', '-0.71', '+0.37', '+0.48', '+0.00', '+0.00']\n",
      "step 20 total_reward -80.76\n",
      "['-0.09', '+0.59', '-0.62', '-0.77', '+0.88', '+0.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -140.64\n",
      "['-0.23', '+0.27', '-0.91', '-1.43', '+2.01', '+1.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -301.75\n",
      "['-0.34', '+0.04', '-0.81', '-0.66', '+2.82', '+6.77', '+0.00', '+0.00']\n",
      "step 70 total_reward -490.95\n",
      "['+0.01', '+0.95', '+0.71', '+0.26', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.66\n",
      "['+0.14', '+0.94', '+0.50', '-0.28', '+0.29', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.31\n",
      "['+0.23', '+0.78', '+0.24', '-0.78', '+1.27', '+1.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.19\n",
      "['+0.15', '+0.48', '-0.57', '-1.22', '+2.20', '+0.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -238.72\n",
      "['-0.02', '+0.01', '-1.51', '-0.25', '+2.63', '+4.68', '+0.00', '+0.00']\n",
      "step 79 total_reward -450.69\n",
      "['+0.01', '+0.93', '+0.30', '-0.23', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.71\n",
      "['+0.05', '+0.78', '+0.15', '-0.77', '+0.35', '+0.54', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.78\n",
      "['+0.07', '+0.50', '+0.15', '-1.14', '+0.66', '-0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -102.02\n",
      "['+0.08', '+0.12', '-0.04', '-1.43', '+0.37', '-0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -66.95\n",
      "['+0.08', '-0.02', '+0.08', '-0.71', '+0.23', '-5.35', '+0.00', '+1.00']\n",
      "step 66 total_reward -162.99\n",
      "['+0.00', '+0.93', '+0.17', '-0.46', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.42\n",
      "['+0.05', '+0.71', '+0.27', '-0.88', '-0.33', '-0.39', '+0.00', '+0.00']\n",
      "step 20 total_reward -57.62\n",
      "['+0.14', '+0.43', '+0.56', '-1.18', '-0.94', '-1.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.96\n",
      "['+0.29', '+0.04', '+0.19', '+0.04', '-2.11', '-0.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -387.94\n",
      "['-0.01', '+0.93', '-0.38', '-0.28', '+0.01', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.48\n",
      "['-0.07', '+0.76', '-0.26', '-0.76', '-0.25', '-0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.40\n",
      "['-0.08', '+0.59', '+0.27', '-0.44', '-0.60', '-0.37', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.46\n",
      "['+0.04', '+0.46', '+0.82', '-0.59', '-1.05', '-0.72', '+0.00', '+0.00']\n",
      "step 60 total_reward -122.32\n",
      "['+0.22', '+0.20', '+1.00', '-1.19', '-2.20', '-1.27', '+0.00', '+0.00']\n",
      "step 80 total_reward -275.24\n",
      "['+0.31', '+0.04', '+0.12', '+0.03', '-2.78', '-0.00', '+0.00', '+0.00']\n",
      "step 89 total_reward -443.80\n",
      "['-0.01', '+0.93', '-0.39', '-0.39', '+0.01', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.70\n",
      "['-0.07', '+0.73', '-0.26', '-0.80', '-0.28', '-0.44', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.71\n",
      "['-0.10', '+0.52', '-0.12', '-0.84', '-0.61', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -55.28\n",
      "['-0.13', '+0.20', '-0.13', '-1.23', '-0.25', '+0.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -29.81\n",
      "['-0.13', '-0.02', '-0.06', '-0.83', '-0.07', '+5.00', '+1.00', '+1.00']\n",
      "step 71 total_reward -118.12\n",
      "['+0.01', '+0.95', '+0.35', '+0.36', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.75\n",
      "['+0.08', '+0.98', '+0.33', '-0.18', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward +11.18\n",
      "['+0.13', '+0.84', '+0.19', '-0.72', '+0.36', '+0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.02\n",
      "['+0.15', '+0.58', '-0.07', '-0.97', '+0.80', '+0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -92.40\n",
      "['+0.14', '+0.22', '-0.13', '-1.34', '+0.59', '-0.43', '+0.00', '+0.00']\n",
      "step 80 total_reward -76.51\n",
      "['+0.12', '-0.02', '-0.02', '-0.64', '+0.23', '-5.53', '+0.00', '+1.00']\n",
      "step 91 total_reward -158.60\n",
      "['-0.01', '+0.95', '-0.59', '+0.20', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['-0.14', '+0.92', '-0.71', '-0.34', '+0.57', '+0.61', '+0.00', '+0.00']\n",
      "step 20 total_reward -71.45\n",
      "['-0.31', '+0.76', '-1.26', '-0.63', '+1.14', '+0.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -182.43\n",
      "['-0.72', '+0.54', '-2.68', '-0.97', '+1.73', '+0.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -399.50\n",
      "['-1.00', '+0.37', '-2.84', '-1.31', '+2.10', '+0.98', '+0.00', '+0.00']\n",
      "step 70 total_reward -574.42\n",
      "['-0.00', '+0.93', '-0.05', '-0.47', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.23\n",
      "['-0.02', '+0.71', '-0.23', '-0.84', '+0.28', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -48.69\n",
      "['-0.11', '+0.49', '-0.67', '-0.75', '+0.59', '+0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.28', '+0.19', '-1.16', '-1.22', '+1.45', '+0.87', '+0.00', '+0.00']\n",
      "step 60 total_reward -217.31\n",
      "['-0.39', '+0.02', '-0.65', '+0.20', '+2.02', '+2.15', '+0.00', '+1.00']\n",
      "step 70 total_reward -353.88\n",
      "['+0.02', '+0.94', '+0.76', '+0.06', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.32\n",
      "['+0.15', '+0.88', '+0.56', '-0.48', '+0.25', '+0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.87\n",
      "['+0.24', '+0.68', '+0.14', '-0.75', '+0.98', '+0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -76.38\n",
      "['+0.28', '+0.39', '+0.23', '-1.15', '+1.09', '-0.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -106.28\n",
      "['+0.32', '-0.00', '+0.28', '-0.53', '+0.42', '-5.34', '+0.00', '+1.00']\n",
      "step 79 total_reward -164.99\n",
      "['-0.01', '+0.94', '-0.33', '+0.04', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.20\n",
      "['-0.05', '+0.87', '-0.14', '-0.50', '-0.29', '-0.67', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.21\n",
      "['-0.06', '+0.67', '+0.35', '-0.68', '-0.97', '-0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -117.60\n",
      "['+0.06', '+0.42', '+0.67', '-1.15', '-1.94', '-1.38', '+0.00', '+0.00']\n",
      "step 60 total_reward -247.55\n",
      "['+0.26', '-0.01', '+1.53', '-1.05', '-3.31', '-6.01', '+0.00', '+0.00']\n",
      "step 79 total_reward -533.17\n",
      "['+0.01', '+0.95', '+0.70', '+0.38', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.15\n",
      "['+0.14', '+0.98', '+0.50', '-0.17', '+0.29', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -3.84\n",
      "['+0.23', '+0.84', '+0.37', '-0.78', '+1.41', '+1.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -139.00\n",
      "['+0.17', '+0.52', '-0.60', '-1.44', '+2.88', '+1.27', '+0.00', '+0.00']\n",
      "step 60 total_reward -326.55\n",
      "['+0.06', '+0.01', '-0.13', '+0.01', '+3.83', '+0.39', '+0.00', '+0.00']\n",
      "step 78 total_reward -555.38\n",
      "['+0.00', '+0.94', '+0.14', '-0.19', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.95\n",
      "['+0.02', '+0.79', '+0.03', '-0.73', '+0.34', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -70.54\n",
      "['-0.03', '+0.58', '-0.57', '-0.61', '+0.79', '+0.43', '+0.00', '+0.00']\n",
      "step 40 total_reward -109.82\n",
      "['-0.21', '+0.37', '-1.06', '-0.98', '+1.37', '+0.93', '+0.00', '+0.00']\n",
      "step 60 total_reward -215.44\n",
      "['-0.50', '+0.03', '-0.73', '+0.15', '+2.07', '+2.07', '+0.00', '+0.00']\n",
      "step 79 total_reward -464.66\n",
      "['+0.00', '+0.93', '+0.10', '-0.50', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['+0.02', '+0.70', '+0.06', '-0.89', '+0.13', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.04\n",
      "['+0.03', '+0.40', '+0.16', '-1.14', '-0.01', '-0.40', '+0.00', '+0.00']\n",
      "step 40 total_reward -16.89\n",
      "['+0.07', '+0.08', '+0.26', '-1.13', '-0.41', '-0.30', '+0.00', '+0.00']\n",
      "step 60 total_reward -31.98\n",
      "['+0.08', '-0.01', '+0.09', '-0.49', '-0.37', '+4.12', '+1.00', '+0.00']\n",
      "step 65 total_reward -124.11\n",
      "['+0.01', '+0.95', '+0.73', '+0.36', '-0.02', '-0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.98\n",
      "['+0.17', '+0.97', '+0.92', '-0.21', '-0.64', '-1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -80.60\n",
      "['+0.37', '+0.82', '+1.08', '-0.87', '-2.10', '-1.84', '+0.00', '+0.00']\n",
      "step 40 total_reward -264.16\n",
      "['+0.65', '+0.41', '+1.32', '-1.65', '-4.10', '-2.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -525.32\n",
      "['+0.84', '+0.01', '+0.31', '+0.05', '-6.04', '+0.00', '+1.00', '+1.00']\n",
      "step 75 total_reward -825.20\n",
      "['+0.01', '+0.93', '+0.50', '-0.28', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.17\n",
      "['+0.10', '+0.76', '+0.43', '-0.82', '+0.15', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.43\n",
      "['+0.18', '+0.51', '+0.47', '-1.06', '+0.15', '-0.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -34.98\n",
      "['+0.28', '+0.11', '+0.57', '-1.59', '-0.50', '-0.75', '+0.00', '+0.00']\n",
      "step 60 total_reward -101.00\n",
      "['+0.31', '-0.01', '+0.50', '-0.31', '-0.66', '+3.44', '+1.00', '+0.00']\n",
      "step 65 total_reward -216.43\n",
      "['-0.01', '+0.95', '-0.72', '+0.42', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.20\n",
      "['-0.17', '+0.99', '-0.90', '-0.14', '+0.63', '+0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.84\n",
      "['-0.36', '+0.86', '-0.95', '-0.80', '+2.11', '+1.97', '+0.00', '+0.00']\n",
      "step 40 total_reward -250.11\n",
      "['-0.60', '+0.43', '-0.98', '-2.08', '+4.12', '+2.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -543.92\n",
      "['-0.68', '+0.09', '-0.48', '+0.11', '+4.39', '-1.41', '+0.00', '+0.00']\n",
      "step 72 total_reward -524.84\n",
      "['+0.00', '+0.95', '+0.08', '+0.44', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.17\n",
      "['+0.02', '+1.00', '+0.11', '-0.10', '-0.07', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward +20.13\n",
      "['+0.03', '+0.89', '-0.03', '-0.63', '+0.10', '+0.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -20.36\n",
      "['+0.01', '+0.64', '-0.35', '-0.83', '+0.63', '+0.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -78.81\n",
      "['-0.05', '+0.33', '-0.40', '-1.23', '+0.69', '-0.14', '+0.00', '+0.00']\n",
      "step 80 total_reward -93.82\n",
      "['-0.15', '-0.02', '-0.47', '-0.54', '+0.18', '-4.84', '+0.00', '+1.00']\n",
      "step 98 total_reward -156.32\n",
      "['+0.01', '+0.93', '+0.43', '-0.29', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.50\n",
      "['+0.08', '+0.76', '+0.30', '-0.83', '+0.27', '+0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.30\n",
      "['+0.13', '+0.49', '+0.31', '-1.12', '+0.45', '-0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -68.36\n",
      "['+0.20', '+0.08', '+0.32', '-1.60', '-0.00', '-0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -41.93\n",
      "['+0.21', '-0.02', '-0.23', '-0.88', '+0.12', '+5.74', '+1.00', '+1.00']\n",
      "step 64 total_reward -133.64\n",
      "['+0.01', '+0.93', '+0.30', '-0.50', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.98\n",
      "['+0.06', '+0.69', '+0.22', '-1.04', '+0.23', '+0.29', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.86\n",
      "['+0.12', '+0.30', '+0.36', '-1.51', '+0.15', '-0.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -52.67\n",
      "['+0.16', '-0.03', '+0.00', '-1.03', '+0.01', '+6.17', '+1.00', '+1.00']\n",
      "step 53 total_reward -142.66\n",
      "['-0.01', '+0.93', '-0.44', '-0.36', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.63\n",
      "['-0.09', '+0.74', '-0.46', '-0.82', '+0.10', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.42\n",
      "['-0.19', '+0.48', '-0.58', '-1.08', '+0.35', '+0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -64.60\n",
      "['-0.32', '+0.09', '-0.72', '-1.53', '+1.16', '+0.57', '+0.00', '+1.00']\n",
      "step 60 total_reward -164.39\n",
      "['-0.32', '+0.07', '+0.14', '+0.04', '+1.53', '+0.92', '+0.00', '+1.00']\n",
      "step 62 total_reward -232.31\n",
      "['-0.00', '+0.95', '-0.26', '+0.23', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.07\n",
      "['-0.06', '+0.93', '-0.27', '-0.30', '+0.28', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.21\n",
      "['-0.10', '+0.76', '-0.17', '-0.71', '+0.08', '-0.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -29.76\n",
      "['-0.15', '+0.59', '-0.27', '-0.70', '-0.18', '+0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -29.44\n",
      "['-0.21', '+0.33', '-0.38', '-1.05', '+0.12', '+0.64', '+0.00', '+0.00']\n",
      "step 80 total_reward -39.65\n",
      "['-0.24', '+0.08', '+0.38', '-0.19', '+1.00', '+1.82', '+0.00', '+1.00']\n",
      "step 100 total_reward -36.60\n",
      "['-0.17', '+0.01', '+0.50', '+0.01', '+2.13', '+0.19', '+0.00', '+0.00']\n",
      "step 117 total_reward -271.67\n",
      "['-0.02', '+0.92', '-0.78', '-0.56', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.59\n",
      "['-0.16', '+0.69', '-0.68', '-0.76', '+0.03', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward +8.70\n",
      "['-0.31', '+0.42', '-0.85', '-1.12', '+0.33', '+0.63', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.54\n",
      "['-0.49', '+0.07', '-1.10', '-0.22', '+1.14', '+4.69', '+0.00', '+1.00']\n",
      "step 59 total_reward -225.25\n",
      "['-0.01', '+0.94', '-0.36', '+0.08', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.25\n",
      "['-0.06', '+0.88', '-0.18', '-0.46', '-0.36', '-0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.86\n",
      "['-0.08', '+0.69', '+0.25', '-0.72', '-1.04', '-0.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -119.07\n",
      "['+0.10', '+0.44', '+1.30', '-1.09', '-1.80', '-1.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -269.48\n",
      "['+0.34', '+0.08', '+0.32', '-0.55', '-2.91', '+5.13', '+0.00', '+0.00']\n",
      "step 78 total_reward -500.76\n",
      "['-0.01', '+0.94', '-0.25', '-0.08', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.60\n",
      "['-0.04', '+0.83', '-0.11', '-0.61', '-0.36', '-0.54', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.56\n",
      "['-0.02', '+0.64', '+0.54', '-0.59', '-0.91', '-0.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.61\n",
      "['+0.16', '+0.42', '+0.91', '-1.07', '-1.75', '-1.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -245.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.31', '+0.07', '-0.26', '-0.39', '-2.94', '+3.27', '+0.00', '+0.00']\n",
      "step 78 total_reward -503.80\n",
      "['-0.01', '+0.92', '-0.70', '-0.58', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.40\n",
      "['-0.14', '+0.69', '-0.60', '-0.74', '-0.10', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward +5.63\n",
      "['-0.26', '+0.46', '-0.67', '-0.87', '-0.08', '+0.24', '+0.00', '+0.00']\n",
      "step 40 total_reward +7.85\n",
      "['-0.41', '+0.13', '-0.70', '-1.30', '+0.35', '+0.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -47.65\n",
      "['-0.44', '-0.01', '+0.13', '-0.58', '-0.07', '-4.94', '+0.00', '+1.00']\n",
      "step 68 total_reward -54.34\n",
      "['-0.01', '+0.92', '-0.73', '-0.58', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.43\n",
      "['-0.15', '+0.68', '-0.60', '-0.82', '-0.13', '-0.20', '+0.00', '+0.00']\n",
      "step 20 total_reward +0.32\n",
      "['-0.29', '+0.35', '-0.80', '-1.34', '+0.10', '+0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -27.64\n",
      "['-0.39', '+0.09', '-0.15', '-0.28', '+0.30', '-3.81', '+0.00', '+1.00']\n",
      "step 53 total_reward -135.71\n",
      "['-0.01', '+0.95', '-0.37', '+0.40', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.18\n",
      "['-0.10', '+0.99', '-0.56', '-0.16', '+0.55', '+0.90', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.59\n",
      "['-0.19', '+0.87', '-0.48', '-0.63', '+1.12', '+0.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -130.29\n",
      "['-0.39', '+0.63', '-1.59', '-1.00', '+1.52', '+0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -268.57\n",
      "['-0.85', '+0.23', '-2.64', '-1.69', '+2.00', '+0.69', '+0.00', '+0.00']\n",
      "step 80 total_reward -459.83\n",
      "['-1.00', '+0.07', '-2.61', '-1.88', '+2.25', '+0.96', '+0.00', '+0.00']\n",
      "step 86 total_reward -598.36\n",
      "['-0.00', '+0.95', '-0.01', '+0.30', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.80\n",
      "['-0.00', '+0.96', '+0.00', '-0.23', '+0.03', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward +5.09\n",
      "['-0.01', '+0.81', '-0.07', '-0.76', '+0.27', '+0.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -57.80\n",
      "['-0.05', '+0.57', '-0.38', '-0.77', '+0.51', '+0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -71.60\n",
      "['-0.18', '+0.32', '-0.81', '-1.05', '+0.85', '+0.63', '+0.00', '+0.00']\n",
      "step 80 total_reward -134.14\n",
      "['-0.38', '-0.05', '-1.05', '-1.43', '+1.20', '+0.33', '+0.00', '+1.00']\n",
      "step 100 total_reward -208.35\n",
      "['-0.40', '-0.05', '-1.41', '-0.34', '+1.42', '+4.16', '+0.00', '+0.00']\n",
      "step 101 total_reward -308.35\n",
      "['-0.00', '+0.93', '-0.10', '-0.32', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.69\n",
      "['-0.01', '+0.75', '+0.04', '-0.81', '-0.32', '-0.39', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.51\n",
      "['+0.04', '+0.53', '+0.42', '-0.93', '-0.81', '-0.77', '+0.00', '+0.00']\n",
      "step 40 total_reward -115.12\n",
      "['+0.17', '+0.16', '+0.95', '-1.51', '-1.95', '-1.24', '+0.00', '+0.00']\n",
      "step 60 total_reward -278.73\n",
      "['+0.23', '+0.02', '+0.79', '+0.01', '-2.44', '-2.36', '+0.00', '+0.00']\n",
      "step 67 total_reward -429.25\n",
      "['-0.01', '+0.94', '-0.52', '-0.09', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.94\n",
      "['-0.12', '+0.83', '-0.52', '-0.62', '+0.25', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -44.11\n",
      "['-0.25', '+0.68', '-0.90', '-0.34', '+0.41', '+0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -69.12\n",
      "['-0.49', '+0.62', '-1.52', '-0.15', '+0.60', '+0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -158.07\n",
      "['-0.87', '+0.60', '-2.31', '+0.01', '+0.78', '+0.19', '+0.00', '+0.00']\n",
      "step 80 total_reward -286.29\n",
      "['-1.02', '+0.60', '-2.54', '-0.02', '+0.84', '+0.17', '+0.00', '+0.00']\n",
      "step 86 total_reward -419.68\n",
      "['-0.01', '+0.95', '-0.52', '+0.43', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.29\n",
      "['-0.12', '+1.00', '-0.54', '-0.10', '+0.26', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.86\n",
      "['-0.22', '+0.89', '-0.48', '-0.62', '+0.42', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.26\n",
      "['-0.34', '+0.68', '-0.81', '-0.56', '+0.40', '+0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -55.38\n",
      "['-0.57', '+0.57', '-1.45', '-0.26', '+0.36', '-0.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -110.35\n",
      "['-0.91', '+0.50', '-1.78', '-0.43', '+0.42', '+0.40', '+0.00', '+0.00']\n",
      "step 100 total_reward -177.63\n",
      "['-1.01', '+0.45', '-1.78', '-0.59', '+0.54', '+0.40', '+0.00', '+0.00']\n",
      "step 106 total_reward -297.29\n",
      "['+0.01', '+0.94', '+0.30', '-0.09', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.51\n",
      "['+0.05', '+0.83', '+0.18', '-0.62', '+0.34', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.17\n",
      "['+0.08', '+0.58', '+0.13', '-0.97', '+0.71', '+0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -103.91\n",
      "['+0.10', '+0.24', '-0.03', '-1.21', '+0.44', '-0.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -68.30\n",
      "['+0.09', '-0.03', '+0.01', '-0.83', '+0.02', '-5.53', '+1.00', '+1.00']\n",
      "step 73 total_reward -151.50\n",
      "['+0.01', '+0.95', '+0.69', '+0.20', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.92\n",
      "['+0.13', '+0.92', '+0.49', '-0.34', '+0.30', '+0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.43\n",
      "['+0.22', '+0.75', '+0.07', '-0.78', '+1.26', '+1.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -114.96\n",
      "['+0.09', '+0.45', '-0.83', '-1.21', '+2.13', '+0.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -242.40\n",
      "['-0.14', '+0.01', '-1.61', '-0.18', '+2.58', '+4.90', '+0.00', '+0.00']\n",
      "step 78 total_reward -473.58\n",
      "['+0.01', '+0.95', '+0.44', '+0.16', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.33\n",
      "['+0.08', '+0.91', '+0.24', '-0.39', '+0.34', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.98\n",
      "['+0.12', '+0.71', '+0.06', '-0.94', '+1.38', '+1.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -162.18\n",
      "['+0.09', '+0.37', '-0.29', '-1.42', '+2.04', '+0.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -246.77\n",
      "['-0.01', '+0.01', '-0.72', '-0.00', '+2.24', '-0.09', '+0.00', '+1.00']\n",
      "step 74 total_reward -418.68\n",
      "['+0.00', '+0.94', '+0.05', '-0.20', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.95\n",
      "['-0.00', '+0.79', '-0.04', '-0.73', '+0.32', '+0.37', '+0.00', '+0.00']\n",
      "step 20 total_reward -72.85\n",
      "['-0.06', '+0.58', '-0.67', '-0.56', '+0.67', '+0.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -107.08\n",
      "['-0.26', '+0.40', '-1.17', '-0.81', '+1.11', '+0.74', '+0.00', '+0.00']\n",
      "step 60 total_reward -198.56\n",
      "['-0.62', '+0.10', '-2.39', '-1.32', '+1.97', '+0.92', '+0.00', '+0.00']\n",
      "step 80 total_reward -434.82\n",
      "['-0.79', '-0.04', '-1.78', '+0.03', '+2.41', '+5.35', '+0.00', '+0.00']\n",
      "step 87 total_reward -587.90\n",
      "['-0.01', '+0.95', '-0.57', '+0.27', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.08\n",
      "['-0.11', '+0.95', '-0.39', '-0.27', '-0.28', '-0.64', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.18\n",
      "['-0.18', '+0.78', '-0.20', '-0.79', '-1.22', '-1.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -125.06\n",
      "['-0.13', '+0.49', '+0.38', '-1.18', '-2.04', '-0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -223.42\n",
      "['-0.02', '+0.06', '+0.81', '-1.85', '-2.13', '-0.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -266.53\n",
      "['-0.01', '+0.01', '+0.57', '-0.15', '-2.08', '+2.86', '+1.00', '+0.00']\n",
      "step 82 total_reward -369.76\n",
      "['-0.00', '+0.94', '-0.10', '+0.07', '+0.00', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.25\n",
      "['-0.04', '+0.88', '-0.30', '-0.49', '+0.50', '+0.88', '+0.00', '+0.00']\n",
      "step 20 total_reward -87.98\n",
      "['-0.13', '+0.66', '-0.85', '-0.92', '+1.43', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -231.89\n",
      "['-0.30', '+0.30', '-0.78', '-1.59', '+2.86', '+1.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -402.35\n",
      "['-0.41', '-0.03', '-0.35', '+0.14', '+3.97', '+3.69', '+1.00', '+0.00']\n",
      "step 72 total_reward -636.03\n",
      "['+0.01', '+0.96', '+0.46', '+0.47', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.67\n",
      "['+0.08', '+1.01', '+0.25', '-0.08', '+0.37', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.76\n",
      "['+0.12', '+0.90', '+0.14', '-0.72', '+1.60', '+1.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -161.66\n",
      "['+0.14', '+0.59', '+0.05', '-1.45', '+3.44', '+1.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -388.26\n",
      "['+0.14', '+0.03', '+0.16', '-2.10', '+4.88', '+1.59', '+1.00', '+0.00']\n",
      "step 80 total_reward -544.84\n",
      "['+0.15', '+0.00', '+0.11', '+0.01', '+4.38', '-0.12', '+1.00', '+0.00']\n",
      "step 82 total_reward -563.01\n",
      "['-0.02', '+0.95', '-0.79', '+0.22', '+0.02', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.67\n",
      "['-0.16', '+0.93', '-0.59', '-0.32', '-0.24', '-0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.32\n",
      "['-0.26', '+0.76', '-0.13', '-0.69', '-1.10', '-0.87', '+0.00', '+0.00']\n",
      "step 40 total_reward -85.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.24', '+0.50', '+0.08', '-1.07', '-1.68', '-0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -157.44\n",
      "['-0.17', '+0.11', '+0.66', '-1.60', '-1.54', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -176.19\n",
      "['-0.14', '+0.00', '+0.73', '+0.11', '-2.01', '-1.38', '+0.00', '+0.00']\n",
      "step 85 total_reward -269.65\n",
      "['+0.01', '+0.95', '+0.29', '+0.18', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.67\n",
      "['+0.05', '+0.91', '+0.09', '-0.37', '+0.39', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.85\n",
      "['+0.06', '+0.72', '+0.04', '-0.93', '+1.39', '+1.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -173.80\n",
      "['+0.02', '+0.39', '-0.45', '-1.41', '+2.04', '+0.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -262.23\n",
      "['-0.11', '+0.01', '-0.88', '+0.01', '+2.27', '+0.37', '+0.00', '+1.00']\n",
      "step 75 total_reward -456.95\n",
      "['+0.01', '+0.94', '+0.48', '-0.04', '-0.01', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.81\n",
      "['+0.12', '+0.84', '+0.68', '-0.61', '-0.60', '-0.97', '+0.00', '+0.00']\n",
      "step 20 total_reward -94.40\n",
      "['+0.29', '+0.59', '+1.05', '-1.13', '-1.96', '-1.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -275.43\n",
      "['+0.50', '+0.16', '+0.88', '-1.67', '-4.10', '-2.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -512.14\n",
      "['+0.53', '+0.06', '-0.10', '+0.11', '-4.01', '+1.59', '+0.00', '+0.00']\n",
      "step 66 total_reward -474.31\n",
      "['+0.00', '+0.95', '+0.06', '+0.16', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.54\n",
      "['-0.00', '+0.91', '-0.13', '-0.39', '+0.45', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.91\n",
      "['-0.03', '+0.71', '-0.15', '-0.93', '+1.39', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -191.46\n",
      "['-0.12', '+0.36', '-0.41', '-1.50', '+2.58', '+1.57', '+0.00', '+0.00']\n",
      "step 60 total_reward -338.62\n",
      "['-0.19', '+0.01', '+0.12', '-0.02', '+3.64', '-0.38', '+0.00', '+0.00']\n",
      "step 74 total_reward -555.77\n",
      "['+0.01', '+0.94', '+0.42', '+0.04', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.17\n",
      "['+0.08', '+0.87', '+0.23', '-0.51', '+0.36', '+0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.83\n",
      "['+0.11', '+0.64', '+0.06', '-0.94', '+1.20', '+0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -143.08\n",
      "['+0.12', '+0.30', '-0.25', '-1.36', '+1.38', '-0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -173.15\n",
      "['+0.01', '+0.00', '-0.77', '+0.13', '+1.99', '+1.51', '+0.00', '+1.00']\n",
      "step 75 total_reward -294.88\n",
      "['+0.01', '+0.92', '+0.70', '-0.54', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.66\n",
      "['+0.14', '+0.69', '+0.61', '-0.87', '+0.07', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -3.16\n",
      "['+0.28', '+0.34', '+0.81', '-1.41', '-0.24', '-0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -50.39\n",
      "['+0.42', '-0.05', '+0.84', '-0.04', '-0.92', '-0.53', '+1.00', '+0.00']\n",
      "step 57 total_reward -241.97\n",
      "['-0.01', '+0.94', '-0.66', '-0.01', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.73\n",
      "['-0.14', '+0.85', '-0.56', '-0.53', '+0.07', '-0.24', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.94\n",
      "['-0.25', '+0.67', '-0.59', '-0.69', '-0.13', '-0.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -18.14\n",
      "['-0.38', '+0.38', '-0.77', '-1.22', '+0.24', '+0.74', '+0.00', '+0.00']\n",
      "step 60 total_reward -66.36\n",
      "['-0.50', '+0.11', '-0.21', '-0.22', '+0.48', '-1.56', '+0.00', '+1.00']\n",
      "step 80 total_reward +35.79\n",
      "['-0.51', '+0.09', '-0.17', '-0.26', '+0.13', '-2.03', '+0.00', '+0.00']\n",
      "step 84 total_reward -54.27\n",
      "['-0.00', '+0.93', '-0.09', '-0.45', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.51\n",
      "['-0.01', '+0.71', '+0.05', '-0.88', '-0.28', '-0.31', '+0.00', '+0.00']\n",
      "step 20 total_reward -51.19\n",
      "['+0.02', '+0.42', '+0.25', '-1.23', '-0.82', '-0.90', '+0.00', '+0.00']\n",
      "step 40 total_reward -115.47\n",
      "['+0.12', '+0.02', '+0.54', '-1.60', '-1.75', '-0.80', '+1.00', '+0.00']\n",
      "step 60 total_reward -213.39\n",
      "['+0.13', '+0.01', '+0.45', '+0.09', '-2.04', '-1.16', '+1.00', '+0.00']\n",
      "step 62 total_reward -295.75\n",
      "['+0.02', '+0.93', '+0.77', '-0.45', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.60\n",
      "['+0.16', '+0.71', '+0.70', '-0.90', '+0.09', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.52\n",
      "['+0.30', '+0.40', '+0.80', '-1.28', '-0.02', '-0.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -22.87\n",
      "['+0.46', '-0.03', '+1.19', '-1.07', '-0.46', '+4.56', '+1.00', '+0.00']\n",
      "step 59 total_reward -209.62\n",
      "['-0.01', '+0.95', '-0.51', '+0.27', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.28\n",
      "['-0.09', '+0.95', '-0.31', '-0.28', '-0.35', '-0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.34\n",
      "['-0.15', '+0.78', '-0.09', '-0.84', '-1.43', '-1.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -153.55\n",
      "['-0.06', '+0.44', '+0.56', '-1.37', '-2.42', '-0.64', '+0.00', '+0.00']\n",
      "step 60 total_reward -285.07\n",
      "['+0.06', '-0.00', '+0.58', '+0.01', '-3.15', '-0.05', '+0.00', '+0.00']\n",
      "step 79 total_reward -448.60\n",
      "['-0.01', '+0.94', '-0.70', '-0.17', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.10\n",
      "['-0.16', '+0.80', '-0.74', '-0.71', '+0.39', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -57.44\n",
      "['-0.34', '+0.56', '-1.00', '-1.07', '+0.93', '+0.91', '+0.00', '+0.00']\n",
      "step 40 total_reward -142.66\n",
      "['-0.56', '+0.15', '-1.49', '-1.75', '+2.27', '+1.57', '+0.00', '+0.00']\n",
      "step 60 total_reward -354.35\n",
      "['-0.61', '+0.07', '-2.14', '-0.72', '+2.47', '-0.86', '+0.00', '+0.00']\n",
      "step 63 total_reward -483.15\n",
      "['+0.00', '+0.95', '+0.18', '+0.27', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.28\n",
      "['+0.03', '+0.95', '+0.09', '-0.27', '+0.21', '+0.35', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.97\n",
      "['+0.04', '+0.78', '+0.00', '-0.82', '+0.86', '+0.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -117.11\n",
      "['-0.03', '+0.49', '-0.90', '-1.17', '+1.54', '+0.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -225.03\n",
      "['-0.31', '+0.04', '-1.65', '-1.84', '+2.22', '+0.64', '+0.00', '+0.00']\n",
      "step 80 total_reward -376.90\n",
      "['-0.37', '-0.07', '-2.11', '-0.95', '+2.25', '-3.50', '+0.00', '+1.00']\n",
      "step 84 total_reward -497.61\n",
      "['-0.01', '+0.94', '-0.37', '+0.11', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.88\n",
      "['-0.08', '+0.90', '-0.27', '-0.41', '+0.04', '-0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.03\n",
      "['-0.12', '+0.70', '-0.12', '-0.63', '-0.35', '-0.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -39.89\n",
      "['-0.16', '+0.46', '-0.26', '-1.05', '-0.41', '+0.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -67.73\n",
      "['-0.21', '+0.06', '-0.23', '-1.56', '+0.11', '+0.26', '+0.00', '+0.00']\n",
      "step 80 total_reward -62.12\n",
      "['-0.22', '-0.01', '+0.26', '-0.77', '+0.08', '-5.41', '+0.00', '+1.00']\n",
      "step 83 total_reward -158.61\n",
      "['+0.01', '+0.95', '+0.46', '+0.18', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.51\n",
      "['+0.08', '+0.92', '+0.26', '-0.37', '+0.36', '+0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.10\n",
      "['+0.13', '+0.72', '+0.19', '-0.94', '+1.40', '+1.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -163.48\n",
      "['+0.11', '+0.37', '-0.23', '-1.41', '+2.28', '+0.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -266.83\n",
      "['+0.06', '+0.01', '-0.47', '-0.39', '+2.80', '+5.16', '+0.00', '+0.00']\n",
      "step 75 total_reward -418.50\n",
      "['-0.01', '+0.94', '-0.68', '-0.13', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.46\n",
      "['-0.13', '+0.81', '-0.55', '-0.67', '-0.24', '-0.42', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.55\n",
      "['-0.19', '+0.64', '-0.07', '-0.57', '-0.59', '-0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -22.19\n",
      "['-0.17', '+0.46', '+0.38', '-0.56', '-0.65', '-0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -25.97\n",
      "['-0.10', '+0.22', '+0.37', '-0.89', '-0.25', '+0.55', '+0.00', '+0.00']\n",
      "step 80 total_reward +8.26\n",
      "['-0.01', '-0.03', '+0.39', '+0.00', '+0.00', '+0.00', '+1.00', '+1.00']\n",
      "step 98 total_reward -53.84\n",
      "['-0.01', '+0.94', '-0.55', '+0.01', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.90\n",
      "['-0.13', '+0.86', '-0.67', '-0.53', '+0.54', '+0.63', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.45\n",
      "['-0.29', '+0.64', '-0.88', '-1.01', '+1.30', '+1.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -186.40\n",
      "['-0.47', '+0.24', '-1.01', '-1.83', '+2.89', '+1.86', '+0.00', '+0.00']\n",
      "step 60 total_reward -404.66\n",
      "['-0.61', '-0.19', '+0.26', '+0.01', '+3.99', '-0.80', '+1.00', '+0.00']\n",
      "step 74 total_reward -672.91\n",
      "['-0.01', '+0.94', '-0.46', '+0.00', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.82\n",
      "['-0.09', '+0.86', '-0.29', '-0.54', '-0.32', '-0.63', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.12', '+0.65', '+0.16', '-0.74', '-0.92', '-0.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -95.60\n",
      "['-0.09', '+0.36', '+0.06', '-1.17', '-0.94', '+0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -110.61\n",
      "['-0.05', '-0.01', '+0.22', '-0.33', '-0.45', '+3.79', '+1.00', '+0.00']\n",
      "step 79 total_reward -137.01\n",
      "['-0.01', '+0.93', '-0.60', '-0.27', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.03\n",
      "['-0.12', '+0.77', '-0.56', '-0.75', '-0.09', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.81\n",
      "['-0.22', '+0.63', '-0.35', '-0.26', '-0.26', '-0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward +18.13\n",
      "['-0.24', '+0.62', '+0.13', '+0.21', '-0.46', '-0.24', '+0.00', '+0.00']\n",
      "step 60 total_reward +10.38\n",
      "['-0.15', '+0.73', '+0.85', '+0.47', '-0.70', '-0.28', '+0.00', '+0.00']\n",
      "step 80 total_reward -100.83\n",
      "['+0.07', '+0.86', '+1.16', '+0.18', '-1.00', '-0.30', '+0.00', '+0.00']\n",
      "step 100 total_reward -163.15\n",
      "['+0.31', '+0.83', '+1.41', '-0.29', '-1.30', '-0.31', '+0.00', '+0.00']\n",
      "step 120 total_reward -223.51\n",
      "['+0.70', '+0.68', '+2.19', '-0.79', '-1.67', '-0.62', '+0.00', '+0.00']\n",
      "step 140 total_reward -362.83\n",
      "['+1.01', '+0.48', '+2.19', '-1.17', '-2.17', '-0.71', '+0.00', '+0.00']\n",
      "step 154 total_reward -535.51\n",
      "['-0.00', '+0.93', '-0.24', '-0.24', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.57\n",
      "['-0.04', '+0.78', '-0.11', '-0.78', '-0.35', '-0.50', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.95\n",
      "['-0.01', '+0.57', '+0.44', '-0.75', '-0.89', '-0.71', '+0.00', '+0.00']\n",
      "step 40 total_reward -110.44\n",
      "['+0.11', '+0.26', '+0.94', '-1.40', '-2.02', '-1.32', '+0.00', '+0.00']\n",
      "step 60 total_reward -278.80\n",
      "['+0.22', '+0.01', '+1.16', '-0.42', '-2.77', '-5.24', '+0.00', '+0.00']\n",
      "step 71 total_reward -459.23\n",
      "total rewards [-297.34392298072146, -451.75771113072994, -181.10656093281457, -347.04860193189757, -178.38471045169615, -143.10763389544178, -482.69749410912453, -466.15426880803363, -109.90692937415956, -613.7349391653531, -254.03442184583392, -108.50350509901834, -465.7109792119049, -193.5667054964319, -178.88373063021322, -452.67612684721416, -396.3250279161989, -147.49214963752704, -623.8451479822522, -722.3387233340424, -541.9700351327543, -395.7917357696395, -624.6544964208755, -404.5337777147032, -571.184476436774, -136.27368722342564, -567.8775366456621, -523.4050359468301, -726.8508227001292, -408.0326578287939, -304.0588915841337, -64.68271432676039, -303.910690021276, -155.19620378552895, -238.68352407575915, -141.0598157989093, -133.5623632037625, -585.1362576247341, -479.68672065318873, -453.71687832268555, -524.4496057441427, -454.08134642757057, -144.83152563470895, -166.60397735082705, -322.15780745140137, -389.54022738080187, -257.03267751695853, -395.57422513159696, -530.3715552172334, -312.03836708887485, -89.78709883947516, -505.96985117777166, -148.6094600261514, -375.45031748362754, -527.8783502169888, -620.2462129775173, -494.6441498100537, -555.262523686212, -535.7689734390781, -473.6853033537761, -97.95062890492167, -64.15187832950603, -574.6411383772331, -597.517810948349, -492.655336435922, -139.6602736376317, -510.0973069077919, -319.70760432086195, -144.4013321385471, -209.9343687480969, -310.49689846119406, -655.017207986002, -190.5406811422224, -128.2918408084437, -595.5326073009718, -512.1316896878332, -607.3839047770668, -261.0200881391456, -446.2112353221778, -161.903146967681, -116.18953745877275, -329.17076830670146, -205.96626638607907, -140.5960718368468, -449.04732201004276, -171.63685396224446, -495.30256676663765, -75.04578127290915, -54.762731668104564, -756.1306179572306, -364.224950105629, -359.1148505125316, -383.636197675143, -261.9890910203302, -629.7953011599919, -624.5411767682195, -514.5752070752117, -164.77626440774088, -157.58527963711464, -147.024177487761, -441.1254158297053, -558.3780659549432, -230.38508490396072, -510.7732699868606, -138.57534661881226, -515.0784179957639, -661.7884777799754, -496.79031276071765, -453.3370818573985, -329.21643365055246, -461.9691712322957, -133.64730576136816, -389.94476537789086, -704.8925311642591, -253.15061429262215, -706.351841343045, -421.5632053584832, -452.02615563292875, -701.375542641154, -190.1202486911912, -545.404387795146, -445.51514573497275, -611.4689447318154, -160.48704339069528, -442.2827686817915, -186.72901721407294, -631.7654283408214, -702.030035919486, -606.9257673649983, -575.5788296676492, -167.58528347036213, -542.8343012790531, -313.69327338036646, -601.4368930513782, -663.3717054046684, -169.04968956214503, -260.57081418456033, -531.2069032387758, -170.79494077562973, -48.156378870025435, -194.939849573951, -210.92531133066097, -490.95277553385887, -450.6923472606099, -162.99472583182882, -387.9404088428388, -443.79956443760227, -118.11726689071719, -158.60187333117457, -574.4214543768808, -353.8848136229266, -164.98757092481748, -533.1699021900974, -555.381706705097, -464.66133625077265, -124.1126796185477, -825.1979895007489, -216.42859247160297, -524.8355316814152, -156.3227877845162, -133.64407149043976, -142.6617609697539, -232.3116503431536, -271.6664448457175, -225.25166229856117, -500.76494642107315, -503.80088993997475, -54.342776347472885, -135.70545963559698, -598.3577862498967, -308.35296037077063, -429.25106373115375, -419.68269156434513, -297.29303974754004, -151.49921291389043, -473.5789016076958, -418.6818183405874, -587.8965182678788, -369.7642760776388, -636.0289437287613, -563.0117645225274, -269.646269096935, -456.9473188324161, -474.30667079070474, -555.7723853346857, -294.8797687309055, -241.96713877822657, -54.26752769627364, -295.7462872446299, -209.62459471012866, -448.6004090479086, -483.1538195653447, -497.6062096247691, -158.60836629700026, -418.4970130256836, -53.83798656025141, -672.9070597693557, -137.00678859762232, -535.5144115534385, -459.2339025603224]\n",
      "average total reward -371.15323528526295\n"
     ]
    }
   ],
   "source": [
    "!python lunar_lander_ml_images_player_model2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_model_reward_comparisons[\"Model 2\"] = '-371.15323528526295'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-04-23 00:30:55.980552: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "['+0.00', '+0.94', '+0.01', '-0.09', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.42\n",
      "['-0.02', '+0.83', '-0.16', '-0.64', '+0.47', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -95.53\n",
      "['-0.10', '+0.63', '-0.93', '-0.61', '+1.02', '+0.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -181.50\n",
      "['-0.39', '+0.42', '-1.61', '-1.00', '+1.70', '+1.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -324.11\n",
      "['-0.66', '+0.10', '-1.95', '-0.55', '+2.84', '+3.78', '+0.00', '+0.00']\n",
      "step 77 total_reward -567.25\n",
      "['+0.00', '+0.93', '+0.19', '-0.23', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.78\n",
      "['+0.03', '+0.78', '-0.00', '-0.78', '+0.41', '+0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.12\n",
      "['-0.04', '+0.56', '-0.86', '-0.75', '+1.18', '+0.77', '+0.00', '+0.00']\n",
      "step 40 total_reward -171.46\n",
      "['-0.28', '+0.28', '-1.39', '-1.31', '+2.21', '+1.36', '+0.00', '+0.00']\n",
      "step 60 total_reward -338.23\n",
      "['-0.49', '-0.01', '-2.07', '-0.85', '+2.99', '+3.29', '+0.00', '+0.00']\n",
      "step 73 total_reward -551.48\n",
      "['-0.01', '+0.93', '-0.53', '-0.32', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.28\n",
      "['-0.13', '+0.75', '-0.78', '-0.82', '+0.57', '+0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -93.15\n",
      "['-0.37', '+0.51', '-1.36', '-1.02', '+1.55', '+1.33', '+0.00', '+0.00']\n",
      "step 40 total_reward -238.79\n",
      "['-0.64', '+0.12', '-1.23', '-1.63', '+3.39', '+2.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -459.72\n",
      "['-0.72', '-0.04', '-1.83', '-0.82', '+3.98', '-1.66', '+0.00', '+0.00']\n",
      "step 66 total_reward -631.20\n",
      "['+0.01', '+0.93', '+0.75', '-0.43', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.51\n",
      "['+0.15', '+0.72', '+0.69', '-0.95', '+0.17', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.05\n",
      "['+0.31', '+0.35', '+0.87', '-1.47', '-0.11', '-0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.42\n",
      "['+0.43', '+0.02', '+0.03', '+0.02', '-1.53', '-0.14', '+1.00', '+0.00']\n",
      "step 57 total_reward -183.54\n",
      "['+0.01', '+0.93', '+0.64', '-0.24', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.21\n",
      "['+0.12', '+0.78', '+0.54', '-0.77', '+0.24', '+0.29', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.12\n",
      "['+0.25', '+0.47', '+0.73', '-1.27', '+0.10', '-0.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.64\n",
      "['+0.41', '+0.01', '+0.71', '-1.35', '-0.82', '-1.36', '+1.00', '+0.00']\n",
      "step 60 total_reward -104.40\n",
      "['+0.42', '-0.01', '+0.61', '-0.00', '-0.85', '-0.10', '+1.00', '+0.00']\n",
      "step 62 total_reward -211.05\n",
      "['+0.01', '+0.95', '+0.30', '+0.44', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.17\n",
      "['+0.07', '+1.00', '+0.30', '-0.09', '-0.07', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward +11.57\n",
      "['+0.12', '+0.89', '+0.21', '-0.62', '-0.05', '+0.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -10.44\n",
      "['+0.17', '+0.62', '+0.35', '-1.14', '+0.04', '-0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -38.68\n",
      "['+0.26', '+0.20', '+0.56', '-1.67', '-0.62', '-1.02', '+0.00', '+0.00']\n",
      "step 80 total_reward -121.80\n",
      "['+0.29', '+0.06', '-0.33', '-0.07', '-1.57', '-0.58', '+1.00', '+0.00']\n",
      "step 87 total_reward -227.58\n",
      "['+0.01', '+0.93', '+0.29', '-0.47', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.01\n",
      "['+0.04', '+0.72', '+0.04', '-0.72', '+0.37', '+0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.91\n",
      "['-0.02', '+0.54', '-0.82', '-0.51', '+0.96', '+0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -107.74\n",
      "['-0.27', '+0.35', '-1.33', '-0.94', '+1.79', '+1.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -248.83\n",
      "['-0.51', '+0.07', '-1.95', '-0.51', '+2.83', '+2.67', '+0.00', '+0.00']\n",
      "step 76 total_reward -500.33\n",
      "['+0.01', '+0.93', '+0.59', '-0.27', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.15\n",
      "['+0.12', '+0.77', '+0.49', '-0.80', '+0.26', '+0.31', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.46\n",
      "['+0.23', '+0.45', '+0.69', '-1.30', '+0.13', '-0.52', '+0.00', '+0.00']\n",
      "step 40 total_reward -52.31\n",
      "['+0.38', '-0.03', '+0.83', '-1.85', '-0.78', '-1.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -161.09\n",
      "['+0.41', '-0.10', '+1.33', '-0.91', '-0.80', '+4.18', '+1.00', '+0.00']\n",
      "step 63 total_reward -260.83\n",
      "['-0.01', '+0.94', '-0.30', '+0.06', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.99\n",
      "['-0.08', '+0.88', '-0.48', '-0.50', '+0.52', '+0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -85.10\n",
      "['-0.22', '+0.65', '-1.30', '-0.99', '+1.71', '+1.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -282.06\n",
      "['-0.50', '+0.26', '-1.24', '-1.60', '+3.44', '+2.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -482.36\n",
      "['-0.62', '+0.04', '-1.61', '-0.38', '+4.49', '+5.24', '+1.00', '+0.00']\n",
      "step 69 total_reward -687.56\n",
      "['+0.00', '+0.95', '+0.10', '+0.15', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.33\n",
      "['+0.01', '+0.90', '-0.09', '-0.40', '+0.41', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.73\n",
      "['-0.04', '+0.71', '-0.64', '-0.90', '+1.56', '+1.32', '+0.00', '+0.00']\n",
      "step 40 total_reward -225.24\n",
      "['-0.28', '+0.33', '-1.27', '-1.62', '+2.99', '+1.75', '+0.00', '+0.00']\n",
      "step 60 total_reward -439.45\n",
      "['-0.47', '-0.07', '-1.00', '-0.64', '+4.19', '+7.84', '+1.00', '+0.00']\n",
      "step 74 total_reward -687.99\n",
      "['+0.01', '+0.94', '+0.42', '-0.12', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.50\n",
      "['+0.08', '+0.82', '+0.23', '-0.67', '+0.34', '+0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.16\n",
      "['+0.12', '+0.56', '+0.27', '-1.06', '+0.72', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -100.95\n",
      "['+0.19', '+0.17', '+0.34', '-1.37', '+0.25', '-0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -56.95\n",
      "['+0.21', '-0.02', '-0.00', '-0.00', '+0.09', '+0.00', '+0.00', '+1.00']\n",
      "step 70 total_reward -120.13\n",
      "['-0.02', '+0.95', '-0.79', '+0.31', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.49\n",
      "['-0.18', '+0.96', '-0.84', '-0.23', '+0.39', '+0.38', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.84\n",
      "['-0.35', '+0.80', '-1.04', '-0.81', '+1.09', '+1.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -149.09\n",
      "['-0.68', '+0.51', '-1.79', '-1.31', '+2.25', '+1.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -354.89\n",
      "['-1.00', '+0.08', '-1.70', '-1.81', '+3.87', '+1.89', '+0.00', '+0.00']\n",
      "step 78 total_reward -646.95\n",
      "['+0.01', '+0.94', '+0.42', '-0.18', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.12\n",
      "['+0.08', '+0.80', '+0.22', '-0.73', '+0.34', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.74\n",
      "['+0.14', '+0.50', '+0.38', '-1.20', '+0.68', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -106.71\n",
      "['+0.22', '+0.09', '+0.34', '-1.43', '+0.20', '-0.55', '+0.00', '+0.00']\n",
      "step 60 total_reward -53.78\n",
      "['+0.23', '-0.02', '+0.06', '-0.88', '+0.16', '+4.99', '+1.00', '+1.00']\n",
      "step 65 total_reward -132.74\n",
      "['+0.00', '+0.93', '+0.07', '-0.24', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.78\n",
      "['-0.00', '+0.78', '-0.12', '-0.78', '+0.43', '+0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.44\n",
      "['-0.09', '+0.58', '-0.94', '-0.72', '+1.27', '+0.86', '+0.00', '+0.00']\n",
      "step 40 total_reward -193.38\n",
      "['-0.34', '+0.29', '-1.29', '-1.31', '+2.44', '+1.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -365.10\n",
      "['-0.52', '+0.03', '-1.01', '-0.01', '+3.06', '+0.42', '+0.00', '+0.00']\n",
      "step 72 total_reward -588.47\n",
      "['-0.00', '+0.93', '-0.23', '-0.29', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.49\n",
      "['-0.07', '+0.76', '-0.43', '-0.79', '+0.50', '+0.81', '+0.00', '+0.00']\n",
      "step 20 total_reward -88.75\n",
      "['-0.25', '+0.55', '-1.32', '-0.77', '+1.33', '+1.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -224.23\n",
      "['-0.51', '+0.23', '-1.24', '-1.41', '+2.81', '+1.89', '+0.00', '+0.00']\n",
      "step 60 total_reward -403.35\n",
      "['-0.71', '-0.09', '-1.72', '-0.53', '+3.84', '-1.89', '+0.00', '+0.00']\n",
      "step 74 total_reward -656.43\n",
      "['-0.00', '+0.95', '-0.19', '+0.40', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.06\n",
      "['-0.04', '+0.99', '-0.22', '-0.14', '+0.19', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.34\n",
      "['-0.09', '+0.86', '-0.33', '-0.70', '+0.55', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -79.05\n",
      "['-0.22', '+0.60', '-1.24', '-0.99', '+1.50', '+0.99', '+0.00', '+0.00']\n",
      "step 60 total_reward -236.99\n",
      "['-0.51', '+0.22', '-1.39', '-1.59', '+2.84', '+1.66', '+0.00', '+0.00']\n",
      "step 80 total_reward -417.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.63', '+0.02', '-0.14', '+0.05', '+3.45', '+0.49', '+0.00', '+0.00']\n",
      "step 88 total_reward -593.24\n",
      "['+0.01', '+0.93', '+0.44', '-0.42', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.74\n",
      "['+0.08', '+0.72', '+0.38', '-0.95', '+0.21', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.99\n",
      "['+0.18', '+0.35', '+0.57', '-1.47', '-0.04', '-0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.87\n",
      "['+0.26', '+0.03', '-0.11', '-0.03', '-0.56', '+0.34', '+1.00', '+0.00']\n",
      "step 54 total_reward -194.53\n",
      "['+0.01', '+0.95', '+0.66', '+0.37', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.18\n",
      "['+0.15', '+0.98', '+0.66', '-0.16', '-0.12', '-0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -5.59\n",
      "['+0.27', '+0.85', '+0.58', '-0.68', '-0.07', '+0.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -12.55\n",
      "['+0.40', '+0.56', '+0.77', '-1.23', '-0.22', '-0.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -63.39\n",
      "['+0.54', '+0.20', '-0.16', '+0.01', '-1.61', '-1.04', '+1.00', '+0.00']\n",
      "step 78 total_reward -243.58\n",
      "['-0.01', '+0.93', '-0.62', '-0.24', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.20\n",
      "['-0.15', '+0.78', '-0.81', '-0.80', '+0.61', '+0.98', '+0.00', '+0.00']\n",
      "step 20 total_reward -95.76\n",
      "['-0.38', '+0.51', '-1.31', '-1.17', '+1.73', '+1.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -257.44\n",
      "['-0.65', '+0.07', '-1.25', '-1.76', '+3.60', '+1.92', '+0.00', '+0.00']\n",
      "step 60 total_reward -486.68\n",
      "['-0.66', '+0.06', '-1.29', '-0.39', '+3.54', '-3.29', '+0.00', '+0.00']\n",
      "step 61 total_reward -586.68\n",
      "['+0.01', '+0.93', '+0.41', '-0.36', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.69\n",
      "['+0.07', '+0.74', '+0.24', '-0.84', '+0.30', '+0.48', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.92\n",
      "['+0.12', '+0.44', '+0.31', '-1.21', '+0.46', '-0.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -70.84\n",
      "['+0.18', '+0.05', '+0.27', '-1.32', '-0.02', '-0.41', '+0.00', '+0.00']\n",
      "step 60 total_reward -13.95\n",
      "['+0.20', '-0.03', '+0.20', '-0.82', '-0.01', '+4.51', '+1.00', '+1.00']\n",
      "step 64 total_reward -108.07\n",
      "['-0.01', '+0.93', '-0.63', '-0.37', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.63\n",
      "['-0.15', '+0.74', '-0.90', '-0.81', '+0.58', '+0.85', '+0.00', '+0.00']\n",
      "step 20 total_reward -90.24\n",
      "['-0.41', '+0.48', '-1.41', '-1.13', '+1.55', '+1.37', '+0.00', '+0.00']\n",
      "step 40 total_reward -237.91\n",
      "['-0.69', '+0.06', '-1.29', '-1.74', '+3.43', '+2.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -467.71\n",
      "['-0.81', '-0.16', '-2.05', '-0.66', '+4.27', '+1.14', '+1.00', '+0.00']\n",
      "step 68 total_reward -670.31\n",
      "['+0.01', '+0.92', '+0.39', '-0.57', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.92\n",
      "['+0.07', '+0.67', '+0.32', '-1.03', '+0.25', '+0.20', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.45\n",
      "['+0.16', '+0.28', '+0.53', '-1.54', '+0.00', '-0.63', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.68\n",
      "['+0.22', '-0.00', '-0.01', '-0.39', '-0.19', '+4.38', '+1.00', '+0.00']\n",
      "step 53 total_reward -156.45\n",
      "['-0.02', '+0.95', '-0.81', '+0.20', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.40\n",
      "['-0.19', '+0.92', '-1.00', '-0.37', '+0.67', '+1.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -90.60\n",
      "['-0.41', '+0.72', '-1.39', '-1.08', '+2.19', '+1.77', '+0.00', '+0.00']\n",
      "step 40 total_reward -301.80\n",
      "['-0.71', '+0.29', '-1.35', '-1.66', '+4.28', '+2.49', '+0.00', '+0.00']\n",
      "step 60 total_reward -544.59\n",
      "['-0.84', '+0.08', '-1.62', '-0.17', '+4.68', '+0.87', '+0.00', '+0.00']\n",
      "step 69 total_reward -681.75\n",
      "['+0.00', '+0.95', '+0.07', '+0.20', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.44\n",
      "['+0.00', '+0.92', '-0.12', '-0.35', '+0.41', '+0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.40\n",
      "['-0.04', '+0.73', '-0.45', '-0.96', '+1.64', '+1.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -226.87\n",
      "['-0.25', '+0.32', '-1.10', '-1.75', '+3.21', '+1.83', '+0.00', '+0.00']\n",
      "step 60 total_reward -455.75\n",
      "['-0.36', '+0.04', '-0.83', '-0.30', '+4.26', '+6.19', '+1.00', '+0.00']\n",
      "step 70 total_reward -665.15\n",
      "['+0.01', '+0.93', '+0.40', '-0.35', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.71\n",
      "['+0.07', '+0.74', '+0.30', '-0.89', '+0.29', '+0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.91\n",
      "['+0.15', '+0.40', '+0.50', '-1.38', '+0.17', '-0.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -60.93\n",
      "['+0.25', '-0.00', '+0.22', '-0.33', '-0.33', '+3.87', '+1.00', '+0.00']\n",
      "step 59 total_reward -166.35\n",
      "['-0.01', '+0.93', '-0.45', '-0.46', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.47\n",
      "['-0.12', '+0.72', '-0.72', '-0.79', '+0.50', '+0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.36\n",
      "['-0.35', '+0.52', '-1.52', '-0.80', '+1.16', '+0.79', '+0.00', '+0.00']\n",
      "step 40 total_reward -201.48\n",
      "['-0.66', '+0.18', '-1.49', '-1.46', '+2.46', '+1.73', '+0.00', '+0.00']\n",
      "step 60 total_reward -375.00\n",
      "['-0.76', '+0.04', '-0.78', '+0.12', '+2.92', '-0.00', '+0.00', '+0.00']\n",
      "step 67 total_reward -546.84\n",
      "['-0.00', '+0.95', '-0.03', '+0.38', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.01\n",
      "['+0.01', '+0.98', '+0.16', '-0.17', '-0.41', '-0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.72\n",
      "['+0.05', '+0.84', '+0.26', '-0.81', '-1.63', '-1.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -198.48\n",
      "['+0.10', '+0.51', '+0.10', '-1.39', '-3.83', '-2.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -440.42\n",
      "['+0.10', '+0.03', '-0.03', '-1.53', '-6.74', '-3.02', '+1.00', '+0.00']\n",
      "step 80 total_reward -696.86\n",
      "['+0.09', '-0.00', '-0.29', '-0.07', '-6.89', '+1.04', '+1.00', '+0.00']\n",
      "step 82 total_reward -805.40\n",
      "['-0.01', '+0.93', '-0.59', '-0.32', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.31\n",
      "['-0.14', '+0.75', '-0.82', '-0.85', '+0.61', '+0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -96.76\n",
      "['-0.36', '+0.48', '-1.17', '-1.20', '+1.75', '+1.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -246.36\n",
      "['-0.60', '+0.03', '-1.16', '-1.79', '+3.66', '+1.82', '+0.00', '+0.00']\n",
      "step 60 total_reward -484.41\n",
      "['-0.66', '-0.10', '-1.20', '-0.13', '+4.22', '+4.30', '+1.00', '+0.00']\n",
      "step 65 total_reward -635.16\n",
      "['+0.00', '+0.95', '+0.25', '+0.33', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.01\n",
      "['+0.05', '+0.96', '+0.18', '-0.21', '+0.17', '+0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -3.09\n",
      "['+0.07', '+0.81', '+0.02', '-0.79', '+0.88', '+0.97', '+0.00', '+0.00']\n",
      "step 40 total_reward -110.51\n",
      "['+0.08', '+0.52', '+0.10', '-1.20', '+1.35', '+0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -170.37\n",
      "['+0.08', '+0.10', '-0.41', '-1.45', '+0.92', '-0.56', '+0.00', '+0.00']\n",
      "step 80 total_reward -121.11\n",
      "['+0.05', '+0.00', '-0.24', '-0.02', '+0.78', '-0.77', '+0.00', '+1.00']\n",
      "step 85 total_reward -200.44\n",
      "['+0.01', '+0.93', '+0.75', '-0.27', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.09\n",
      "['+0.15', '+0.77', '+0.71', '-0.80', '+0.13', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.43\n",
      "['+0.31', '+0.45', '+0.91', '-1.33', '-0.28', '-0.79', '+0.00', '+0.00']\n",
      "step 40 total_reward -70.96\n",
      "['+0.44', '+0.12', '-0.18', '+0.24', '-1.30', '-2.19', '+1.00', '+0.00']\n",
      "step 55 total_reward -268.31\n",
      "['-0.01', '+0.95', '-0.31', '+0.35', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.30\n",
      "['-0.07', '+0.97', '-0.35', '-0.18', '+0.28', '+0.29', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.49\n",
      "['-0.15', '+0.83', '-0.51', '-0.78', '+0.93', '+1.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -128.02\n",
      "['-0.34', '+0.52', '-1.43', '-1.43', '+2.22', '+1.42', '+0.00', '+0.00']\n",
      "step 60 total_reward -347.73\n",
      "['-0.63', '+0.02', '-1.04', '+0.04', '+4.00', '+2.99', '+1.00', '+0.00']\n",
      "step 80 total_reward -650.26\n",
      "['+0.01', '+0.95', '+0.34', '+0.25', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.69\n",
      "['+0.06', '+0.94', '+0.15', '-0.29', '+0.37', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.74\n",
      "['+0.09', '+0.77', '+0.15', '-0.79', '+1.20', '+0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -139.69\n",
      "['+0.12', '+0.47', '+0.21', '-1.20', '+1.20', '-0.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -152.07\n",
      "['+0.15', '+0.06', '-0.03', '-1.43', '+0.35', '-0.96', '+0.00', '+0.00']\n",
      "step 80 total_reward -59.38\n",
      "['+0.14', '-0.03', '+0.00', '-0.91', '+0.03', '-5.49', '+1.00', '+1.00']\n",
      "step 84 total_reward -123.74\n",
      "['+0.01', '+0.95', '+0.44', '+0.28', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.50\n",
      "['+0.08', '+0.95', '+0.24', '-0.27', '+0.35', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.12', '+0.78', '+0.17', '-0.82', '+1.41', '+0.97', '+0.00', '+0.00']\n",
      "step 40 total_reward -155.60\n",
      "['+0.15', '+0.48', '+0.15', '-1.22', '+1.84', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -209.93\n",
      "['+0.15', '+0.04', '-0.41', '-1.68', '+1.35', '-0.66', '+0.00', '+1.00']\n",
      "step 80 total_reward -168.24\n",
      "['+0.12', '+0.01', '-0.56', '+0.12', '+2.13', '+1.78', '+0.00', '+0.00']\n",
      "step 84 total_reward -269.32\n",
      "['+0.01', '+0.96', '+0.33', '+0.47', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.07\n",
      "['+0.07', '+1.01', '+0.33', '-0.06', '-0.08', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward +11.47\n",
      "['+0.14', '+0.91', '+0.28', '-0.59', '-0.13', '+0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -15.64\n",
      "['+0.20', '+0.65', '+0.41', '-1.13', '-0.17', '-0.41', '+0.00', '+0.00']\n",
      "step 60 total_reward -51.05\n",
      "['+0.30', '+0.22', '+0.59', '-1.73', '-1.02', '-1.29', '+0.00', '+0.00']\n",
      "step 80 total_reward -168.51\n",
      "['+0.32', '+0.08', '-0.40', '-0.29', '-1.41', '+1.46', '+1.00', '+0.00']\n",
      "step 86 total_reward -304.68\n",
      "['+0.01', '+0.93', '+0.36', '-0.30', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.72\n",
      "['+0.07', '+0.76', '+0.31', '-0.83', '+0.23', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.53\n",
      "['+0.15', '+0.43', '+0.51', '-1.34', '-0.05', '-0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -55.22\n",
      "['+0.25', '+0.06', '+0.26', '-0.19', '-1.54', '-3.85', '+0.00', '+0.00']\n",
      "step 60 total_reward -74.07\n",
      "['+0.28', '+0.03', '+0.10', '+0.07', '-3.17', '-0.43', '+0.00', '+0.00']\n",
      "step 69 total_reward -341.71\n",
      "['+0.00', '+0.95', '+0.20', '+0.29', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.17\n",
      "['+0.04', '+0.95', '+0.18', '-0.24', '+0.06', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward +0.58\n",
      "['+0.06', '+0.79', '+0.07', '-0.78', '+0.52', '+0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.88\n",
      "['+0.09', '+0.49', '+0.23', '-1.24', '+0.56', '-0.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -100.11\n",
      "['+0.14', '+0.07', '+0.27', '-1.45', '-0.14', '-0.82', '+0.00', '+0.00']\n",
      "step 80 total_reward -47.99\n",
      "['+0.15', '-0.02', '-0.03', '-0.69', '-0.20', '+4.70', '+1.00', '+0.00']\n",
      "step 84 total_reward -139.45\n",
      "['-0.00', '+0.93', '-0.25', '-0.31', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.82\n",
      "['-0.07', '+0.76', '-0.46', '-0.77', '+0.49', '+0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -85.62\n",
      "['-0.26', '+0.55', '-1.39', '-0.77', '+1.29', '+0.92', '+0.00', '+0.00']\n",
      "step 40 total_reward -225.29\n",
      "['-0.54', '+0.23', '-1.34', '-1.41', '+2.73', '+1.81', '+0.00', '+0.00']\n",
      "step 60 total_reward -402.10\n",
      "['-0.70', '-0.02', '-1.48', '-0.60', '+3.39', '-3.94', '+0.00', '+0.00']\n",
      "step 71 total_reward -616.70\n",
      "['-0.01', '+0.94', '-0.30', '-0.15', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.05\n",
      "['-0.08', '+0.81', '-0.48', '-0.71', '+0.50', '+0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -91.92\n",
      "['-0.25', '+0.58', '-1.31', '-0.88', '+1.55', '+1.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -255.68\n",
      "['-0.52', '+0.23', '-1.20', '-1.50', '+3.28', '+2.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -456.60\n",
      "['-0.65', '-0.02', '-1.89', '-0.54', '+4.19', '+0.64', '+0.00', '+0.00']\n",
      "step 70 total_reward -671.92\n",
      "['+0.01', '+0.93', '+0.66', '-0.44', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.51\n",
      "['+0.14', '+0.71', '+0.73', '-0.97', '-0.08', '-0.37', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.68\n",
      "['+0.30', '+0.34', '+0.90', '-1.55', '-0.87', '-1.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -140.24\n",
      "['+0.41', '+0.01', '-0.33', '-0.14', '-1.77', '+1.18', '+1.00', '+0.00']\n",
      "step 53 total_reward -362.16\n",
      "['-0.00', '+0.94', '-0.13', '+0.09', '+0.00', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.29\n",
      "['-0.05', '+0.88', '-0.34', '-0.47', '+0.54', '+0.95', '+0.00', '+0.00']\n",
      "step 20 total_reward -90.76\n",
      "['-0.15', '+0.67', '-0.96', '-0.99', '+1.89', '+1.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -288.60\n",
      "['-0.40', '+0.26', '-1.12', '-1.65', '+3.69', '+2.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -510.31\n",
      "['-0.51', '-0.03', '+0.29', '-0.01', '+3.90', '-0.88', '+1.00', '+0.00']\n",
      "step 72 total_reward -566.68\n",
      "['+0.01', '+0.94', '+0.41', '+0.02', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.00\n",
      "['+0.07', '+0.86', '+0.22', '-0.53', '+0.36', '+0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.15\n",
      "['+0.13', '+0.63', '+0.35', '-0.99', '+0.72', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -107.05\n",
      "['+0.22', '+0.26', '+0.53', '-1.46', '+0.22', '-0.91', '+0.00', '+0.00']\n",
      "step 60 total_reward -75.38\n",
      "['+0.28', '-0.02', '+0.50', '-0.68', '-0.22', '+4.99', '+1.00', '+0.00']\n",
      "step 73 total_reward -187.31\n",
      "['-0.01', '+0.94', '-0.43', '-0.18', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.44\n",
      "['-0.11', '+0.79', '-0.63', '-0.75', '+0.58', '+0.95', '+0.00', '+0.00']\n",
      "step 20 total_reward -96.90\n",
      "['-0.33', '+0.56', '-1.40', '-0.99', '+1.66', '+1.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -266.83\n",
      "['-0.61', '+0.17', '-1.27', '-1.59', '+3.46', '+2.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -478.60\n",
      "['-0.70', '-0.01', '+0.00', '-0.00', '+4.11', '-0.00', '+1.00', '+0.00']\n",
      "step 68 total_reward -663.82\n",
      "['+0.01', '+0.95', '+0.49', '+0.27', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.42\n",
      "['+0.09', '+0.95', '+0.31', '-0.27', '+0.29', '+0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.58\n",
      "['+0.15', '+0.78', '+0.27', '-0.79', '+1.19', '+0.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -129.90\n",
      "['+0.21', '+0.48', '+0.31', '-1.20', '+1.37', '-0.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -161.25\n",
      "['+0.26', '+0.06', '+0.12', '-1.53', '+0.62', '-0.96', '+0.00', '+0.00']\n",
      "step 80 total_reward -92.01\n",
      "['+0.27', '-0.02', '+0.64', '-0.38', '+0.32', '-4.00', '+0.00', '+1.00']\n",
      "step 84 total_reward -142.28\n",
      "['-0.01', '+0.94', '-0.59', '-0.15', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.92\n",
      "['-0.15', '+0.81', '-0.79', '-0.72', '+0.61', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -95.85\n",
      "['-0.35', '+0.52', '-1.18', '-1.25', '+1.92', '+1.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -275.83\n",
      "['-0.59', '+0.06', '-1.11', '-1.83', '+3.97', '+2.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -518.71\n",
      "['-0.65', '-0.09', '-1.33', '-0.89', '+4.52', '+7.62', '+1.00', '+0.00']\n",
      "step 65 total_reward -674.52\n",
      "['+0.01', '+0.95', '+0.66', '+0.30', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.22\n",
      "['+0.14', '+0.95', '+0.57', '-0.24', '+0.07', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward +4.38\n",
      "['+0.24', '+0.80', '+0.42', '-0.79', '+0.77', '+0.88', '+0.00', '+0.00']\n",
      "step 40 total_reward -81.07\n",
      "['+0.33', '+0.50', '+0.52', '-1.22', '+1.18', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -141.29\n",
      "['+0.44', '+0.09', '+1.21', '-0.46', '+0.68', '+0.71', '+0.00', '+1.00']\n",
      "step 79 total_reward -204.91\n",
      "['+0.01', '+0.95', '+0.61', '+0.17', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.94\n",
      "['+0.12', '+0.91', '+0.41', '-0.37', '+0.31', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.65\n",
      "['+0.20', '+0.72', '+0.41', '-0.87', '+1.13', '+0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.85\n",
      "['+0.29', '+0.39', '+0.49', '-1.28', '+1.09', '-0.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -135.92\n",
      "['+0.39', '-0.06', '+0.71', '-1.65', '+0.13', '-1.31', '+0.00', '+1.00']\n",
      "step 80 total_reward -64.85\n",
      "['+0.40', '-0.09', '+0.00', '-0.00', '+0.01', '-0.00', '+1.00', '+1.00']\n",
      "step 82 total_reward -146.69\n",
      "['+0.00', '+0.93', '+0.24', '-0.26', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.81\n",
      "['+0.04', '+0.77', '+0.14', '-0.79', '+0.34', '+0.38', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.45\n",
      "['+0.08', '+0.46', '+0.30', '-1.26', '+0.28', '-0.43', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.52\n",
      "['+0.15', '+0.05', '+0.45', '-1.33', '-0.41', '-0.76', '+0.00', '+0.00']\n",
      "step 60 total_reward -74.40\n",
      "['+0.11', '-0.01', '-0.49', '-0.23', '-0.26', '+1.78', '+1.00', '+0.00']\n",
      "step 72 total_reward -70.64\n",
      "['+0.01', '+0.95', '+0.37', '+0.32', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.00\n",
      "['+0.08', '+0.96', '+0.35', '-0.22', '+0.06', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward +2.69\n",
      "['+0.13', '+0.81', '+0.26', '-0.75', '+0.43', '+0.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -59.62\n",
      "['+0.20', '+0.51', '+0.45', '-1.22', '+0.35', '-0.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -75.71\n",
      "['+0.30', '+0.07', '+0.64', '-1.74', '-0.54', '-1.23', '+0.00', '+0.00']\n",
      "step 80 total_reward -126.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.33', '-0.03', '+0.74', '-0.41', '-0.60', '+3.89', '+1.00', '+0.00']\n",
      "step 84 total_reward -229.94\n",
      "['-0.01', '+0.95', '-0.45', '+0.43', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.51\n",
      "['-0.08', '+1.00', '-0.26', '-0.11', '-0.32', '-0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.35\n",
      "['-0.13', '+0.88', '-0.14', '-0.74', '-1.46', '-1.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -150.55\n",
      "['-0.10', '+0.54', '+0.38', '-1.75', '-3.12', '-1.67', '+0.00', '+0.00']\n",
      "step 60 total_reward -391.23\n",
      "['-0.05', '-0.01', '+0.52', '+0.23', '-4.59', '-3.64', '+0.00', '+1.00']\n",
      "step 76 total_reward -650.32\n",
      "['-0.02', '+0.95', '-0.77', '+0.32', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.59\n",
      "['-0.18', '+0.96', '-0.92', '-0.24', '+0.58', '+0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -72.98\n",
      "['-0.37', '+0.80', '-1.00', '-0.89', '+1.95', '+1.83', '+0.00', '+0.00']\n",
      "step 40 total_reward -239.43\n",
      "['-0.65', '+0.40', '-1.28', '-1.66', '+3.89', '+2.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -500.00\n",
      "['-0.85', '-0.06', '-0.58', '-1.05', '+5.93', '-6.24', '+1.00', '+1.00']\n",
      "step 76 total_reward -819.15\n",
      "['+0.01', '+0.94', '+0.61', '-0.20', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.23\n",
      "['+0.12', '+0.79', '+0.53', '-0.73', '+0.22', '+0.20', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.33\n",
      "['+0.24', '+0.49', '+0.72', '-1.24', '+0.01', '-0.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.04\n",
      "['+0.40', '+0.04', '+0.89', '-1.64', '-0.97', '-1.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -167.12\n",
      "['+0.41', '+0.02', '+1.41', '-0.50', '-0.94', '+2.18', '+1.00', '+0.00']\n",
      "step 61 total_reward -267.12\n",
      "['+0.00', '+0.95', '+0.18', '+0.45', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.90\n",
      "['+0.05', '+1.00', '+0.37', '-0.11', '-0.51', '-0.89', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.66\n",
      "['+0.14', '+0.88', '+0.45', '-0.75', '-1.86', '-1.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -219.16\n",
      "['+0.23', '+0.57', '+0.29', '-1.29', '-4.12', '-2.66', '+0.00', '+0.00']\n",
      "step 60 total_reward -462.98\n",
      "['+0.28', '+0.10', '+0.41', '-1.79', '-7.22', '-3.47', '+0.00', '+0.00']\n",
      "step 80 total_reward -792.86\n",
      "['+0.29', '+0.01', '-0.16', '-0.16', '-7.86', '+1.15', '+1.00', '+0.00']\n",
      "step 84 total_reward -945.60\n",
      "['+0.01', '+0.95', '+0.67', '+0.32', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.94\n",
      "['+0.14', '+0.96', '+0.61', '-0.21', '+0.04', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward +5.29\n",
      "['+0.25', '+0.81', '+0.46', '-0.77', '+0.57', '+0.77', '+0.00', '+0.00']\n",
      "step 40 total_reward -59.56\n",
      "['+0.36', '+0.52', '+0.60', '-1.21', '+0.86', '-0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -112.65\n",
      "['+0.49', '+0.08', '+0.82', '-1.58', '+0.26', '-1.11', '+0.00', '+1.00']\n",
      "step 80 total_reward -73.71\n",
      "['+0.50', '+0.06', '+1.33', '-0.79', '+0.14', '-4.97', '+0.00', '+1.00']\n",
      "step 81 total_reward -173.71\n",
      "['+0.02', '+0.93', '+0.79', '-0.29', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.07\n",
      "['+0.16', '+0.76', '+0.72', '-0.82', '+0.17', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.83\n",
      "['+0.32', '+0.44', '+0.91', '-1.34', '-0.11', '-0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.53\n",
      "['+0.49', '+0.01', '+0.10', '+0.13', '-1.51', '-1.02', '+1.00', '+0.00']\n",
      "step 59 total_reward -226.16\n",
      "['+0.02', '+0.94', '+0.79', '+0.06', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.52\n",
      "['+0.16', '+0.88', '+0.59', '-0.48', '+0.26', '+0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.35\n",
      "['+0.28', '+0.66', '+0.66', '-0.96', '+0.85', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -98.59\n",
      "['+0.42', '+0.30', '+0.80', '-1.40', '+0.58', '-0.72', '+0.00', '+0.00']\n",
      "step 60 total_reward -96.74\n",
      "['+0.54', '-0.01', '+0.97', '-0.05', '-0.08', '-0.00', '+1.00', '+1.00']\n",
      "step 74 total_reward -154.68\n",
      "['-0.01', '+0.92', '-0.74', '-0.55', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.68\n",
      "['-0.18', '+0.69', '-1.08', '-0.86', '+0.61', '+0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -88.27\n",
      "['-0.44', '+0.39', '-1.37', '-1.31', '+1.63', '+1.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -231.98\n",
      "['-0.67', '+0.04', '-1.66', '-0.68', '+3.21', '+5.88', '+0.00', '+0.00']\n",
      "step 56 total_reward -497.86\n",
      "['-0.01', '+0.94', '-0.28', '+0.01', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.29\n",
      "['-0.08', '+0.86', '-0.48', '-0.55', '+0.53', '+0.92', '+0.00', '+0.00']\n",
      "step 20 total_reward -91.40\n",
      "['-0.23', '+0.63', '-1.32', '-0.96', '+1.73', '+1.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -285.53\n",
      "['-0.51', '+0.25', '-1.29', '-1.58', '+3.41', '+2.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -486.26\n",
      "['-0.75', '-0.19', '-1.58', '-0.16', '+4.55', '-0.62', '+1.00', '+0.00']\n",
      "step 77 total_reward -704.74\n",
      "['-0.00', '+0.95', '-0.13', '+0.33', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.09\n",
      "['-0.03', '+0.97', '-0.17', '-0.20', '+0.21', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.31\n",
      "['-0.08', '+0.82', '-0.35', '-0.80', '+0.89', '+1.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -127.11\n",
      "['-0.26', '+0.53', '-1.36', '-1.28', '+2.09', '+1.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -328.27\n",
      "['-0.54', '+0.05', '-1.34', '-1.86', '+3.72', '+1.51', '+0.00', '+0.00']\n",
      "step 80 total_reward -529.25\n",
      "['-0.57', '-0.00', '-0.99', '+0.04', '+3.87', '+2.96', '+0.00', '+0.00']\n",
      "step 82 total_reward -640.64\n",
      "['+0.02', '+0.95', '+0.76', '+0.36', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.77\n",
      "['+0.16', '+0.98', '+0.72', '-0.17', '-0.02', '+0.00', '+0.00', '+0.00']\n",
      "step 20 total_reward +5.90\n",
      "['+0.30', '+0.84', '+0.59', '-0.71', '+0.22', '+0.52', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.20\n",
      "['+0.43', '+0.55', '+0.78', '-1.20', '+0.32', '-0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -64.88\n",
      "['+0.60', '+0.11', '+0.96', '-1.73', '-0.38', '-1.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -117.89\n",
      "['+0.69', '-0.11', '+1.10', '-0.32', '-0.64', '+2.82', '+0.00', '+0.00']\n",
      "step 89 total_reward -248.33\n",
      "['-0.01', '+0.95', '-0.60', '+0.33', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.06\n",
      "['-0.14', '+0.97', '-0.68', '-0.21', '+0.42', '+0.51', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.94\n",
      "['-0.28', '+0.81', '-0.81', '-0.84', '+1.41', '+1.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -181.77\n",
      "['-0.53', '+0.46', '-1.33', '-1.54', '+3.01', '+1.83', '+0.00', '+0.00']\n",
      "step 60 total_reward -414.90\n",
      "['-0.79', '-0.05', '-1.31', '-0.32', '+4.57', '-2.17', '+1.00', '+0.00']\n",
      "step 80 total_reward -610.89\n",
      "['+0.00', '+0.93', '+0.02', '-0.49', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.88\n",
      "['-0.01', '+0.71', '-0.17', '-0.79', '+0.39', '+0.58', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.02\n",
      "['-0.11', '+0.50', '-0.96', '-0.66', '+0.97', '+0.58', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.31\n",
      "['-0.32', '+0.22', '-1.17', '-1.27', '+2.03', '+1.36', '+0.00', '+0.00']\n",
      "step 60 total_reward -284.52\n",
      "['-0.42', '+0.06', '-0.75', '-0.56', '+2.56', '+6.09', '+0.00', '+0.00']\n",
      "step 68 total_reward -441.62\n",
      "['+0.00', '+0.95', '+0.16', '+0.32', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.24\n",
      "['+0.03', '+0.96', '+0.04', '-0.22', '+0.26', '+0.49', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.93\n",
      "['+0.02', '+0.81', '-0.12', '-0.84', '+1.20', '+1.40', '+0.00', '+0.00']\n",
      "step 40 total_reward -153.73\n",
      "['-0.11', '+0.45', '-1.05', '-1.70', '+2.72', '+1.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -390.48\n",
      "['-0.27', '+0.01', '-0.93', '-0.01', '+3.95', '+2.34', '+0.00', '+0.00']\n",
      "step 75 total_reward -629.29\n",
      "['+0.01', '+0.94', '+0.28', '-0.17', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.44\n",
      "['+0.05', '+0.80', '+0.09', '-0.72', '+0.36', '+0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.99\n",
      "['-0.01', '+0.60', '-0.83', '-0.67', '+1.17', '+0.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -162.97\n",
      "['-0.27', '+0.35', '-1.36', '-1.15', '+2.19', '+1.42', '+0.00', '+0.00']\n",
      "step 60 total_reward -323.81\n",
      "['-0.58', '-0.10', '-2.08', '-0.53', '+3.51', '-0.00', '+0.00', '+0.00']\n",
      "step 80 total_reward -630.27\n",
      "['-0.00', '+0.93', '-0.17', '-0.34', '+0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.25\n",
      "['-0.02', '+0.74', '+0.02', '-0.89', '-0.42', '-0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.87\n",
      "['+0.05', '+0.50', '+0.62', '-0.95', '-1.26', '-1.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -164.45\n",
      "['+0.18', '+0.12', '+0.69', '-1.71', '-2.83', '-1.86', '+0.00', '+0.00']\n",
      "step 60 total_reward -365.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.22', '-0.01', '+1.03', '-0.24', '-3.36', '-4.54', '+0.00', '+0.00']\n",
      "step 65 total_reward -512.45\n",
      "['+0.01', '+0.93', '+0.48', '-0.49', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.53\n",
      "['+0.10', '+0.70', '+0.47', '-1.02', '+0.14', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.92\n",
      "['+0.21', '+0.31', '+0.67', '-1.56', '-0.33', '-0.84', '+0.00', '+0.00']\n",
      "step 40 total_reward -79.53\n",
      "['+0.32', '-0.03', '+1.12', '-0.19', '-1.03', '-0.03', '+1.00', '+0.00']\n",
      "step 55 total_reward -214.40\n",
      "['+0.00', '+0.94', '+0.09', '-0.18', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.45\n",
      "['+0.00', '+0.80', '-0.10', '-0.73', '+0.44', '+0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -85.65\n",
      "['-0.11', '+0.60', '-1.16', '-0.69', '+1.28', '+0.85', '+0.00', '+0.00']\n",
      "step 40 total_reward -218.38\n",
      "['-0.40', '+0.32', '-1.43', '-1.26', '+2.43', '+1.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -381.10\n",
      "['-0.61', '+0.03', '-0.77', '-0.03', '+3.42', '-2.09', '+0.00', '+0.00']\n",
      "step 74 total_reward -617.66\n",
      "['-0.00', '+0.93', '-0.10', '-0.29', '+0.00', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.77\n",
      "['-0.04', '+0.76', '-0.31', '-0.81', '+0.50', '+0.85', '+0.00', '+0.00']\n",
      "step 20 total_reward -92.11\n",
      "['-0.20', '+0.53', '-1.33', '-0.83', '+1.32', '+0.86', '+0.00', '+0.00']\n",
      "step 40 total_reward -231.12\n",
      "['-0.46', '+0.19', '-1.30', '-1.46', '+2.67', '+1.58', '+0.00', '+0.00']\n",
      "step 60 total_reward -398.34\n",
      "['-0.58', '+0.01', '-1.30', '-0.45', '+3.18', '-4.01', '+0.00', '+0.00']\n",
      "step 68 total_reward -570.66\n",
      "['-0.00', '+0.94', '-0.21', '-0.09', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.93\n",
      "['-0.06', '+0.82', '-0.40', '-0.65', '+0.50', '+0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -94.24\n",
      "['-0.22', '+0.62', '-1.32', '-0.75', '+1.46', '+1.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -255.58\n",
      "['-0.54', '+0.32', '-1.59', '-1.35', '+2.76', '+1.72', '+0.00', '+0.00']\n",
      "step 60 total_reward -440.11\n",
      "['-0.73', '+0.08', '-1.86', '-0.74', '+3.54', '-3.28', '+0.00', '+0.00']\n",
      "step 71 total_reward -657.59\n",
      "['-0.01', '+0.93', '-0.50', '-0.40', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.52\n",
      "['-0.13', '+0.73', '-0.80', '-0.79', '+0.57', '+0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.28\n",
      "['-0.36', '+0.49', '-1.33', '-1.05', '+1.44', '+1.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -223.43\n",
      "['-0.63', '+0.09', '-1.33', '-1.64', '+3.02', '+1.56', '+0.00', '+0.00']\n",
      "step 60 total_reward -427.52\n",
      "['-0.65', '+0.06', '-0.61', '+0.07', '+2.97', '-0.00', '+0.00', '+0.00']\n",
      "step 62 total_reward -538.45\n",
      "['+0.01', '+0.95', '+0.26', '+0.29', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.12\n",
      "['+0.05', '+0.95', '+0.15', '-0.25', '+0.22', '+0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.47\n",
      "['+0.07', '+0.79', '+0.06', '-0.80', '+1.01', '+0.80', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.68\n",
      "['+0.09', '+0.49', '+0.13', '-1.21', '+1.30', '-0.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -166.09\n",
      "['+0.09', '+0.08', '-0.38', '-1.36', '+0.76', '-0.58', '+0.00', '+0.00']\n",
      "step 80 total_reward -97.19\n",
      "['+0.07', '-0.00', '-0.18', '-0.17', '+0.50', '-2.46', '+0.00', '+1.00']\n",
      "step 86 total_reward -116.45\n",
      "['+0.01', '+0.93', '+0.26', '-0.34', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.97\n",
      "['+0.05', '+0.75', '+0.21', '-0.86', '+0.26', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.93\n",
      "['+0.11', '+0.41', '+0.42', '-1.37', '-0.03', '-0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.34\n",
      "['+0.20', '+0.00', '+0.38', '-0.38', '-1.16', '-7.00', '+1.00', '+0.00']\n",
      "step 59 total_reward -232.92\n",
      "['+0.01', '+0.93', '+0.27', '-0.53', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.57\n",
      "['+0.06', '+0.68', '+0.37', '-1.07', '-0.20', '-0.52', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.52\n",
      "['+0.15', '+0.28', '+0.52', '-1.67', '-1.14', '-1.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -171.97\n",
      "['+0.19', '+0.02', '-0.52', '-0.17', '-1.90', '+0.35', '+1.00', '+0.00']\n",
      "step 50 total_reward -359.70\n",
      "['-0.00', '+0.95', '-0.13', '+0.34', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.27\n",
      "['-0.04', '+0.97', '-0.20', '-0.20', '+0.25', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.68\n",
      "['-0.09', '+0.82', '-0.37', '-0.80', '+1.01', '+1.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -138.78\n",
      "['-0.26', '+0.52', '-1.18', '-1.33', '+2.42', '+1.63', '+0.00', '+0.00']\n",
      "step 60 total_reward -349.42\n",
      "['-0.50', '+0.03', '-1.12', '-1.89', '+4.33', '+1.84', '+0.00', '+0.00']\n",
      "step 80 total_reward -574.41\n",
      "['-0.51', '-0.00', '-0.24', '+0.03', '+4.36', '+0.00', '+1.00', '+0.00']\n",
      "step 82 total_reward -676.97\n",
      "['-0.01', '+0.95', '-0.73', '+0.15', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.65\n",
      "['-0.14', '+0.91', '-0.52', '-0.40', '-0.31', '-0.73', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.26\n",
      "['-0.23', '+0.72', '-0.10', '-0.71', '-1.15', '-0.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -94.03\n",
      "['-0.11', '+0.46', '+1.19', '-1.18', '-2.00', '-0.78', '+0.00', '+0.00']\n",
      "step 60 total_reward -252.44\n",
      "['+0.19', '-0.00', '+1.62', '-0.81', '-3.11', '-7.14', '+0.00', '+0.00']\n",
      "step 80 total_reward -484.89\n",
      "['+0.01', '+0.93', '+0.60', '-0.48', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.43\n",
      "['+0.12', '+0.70', '+0.57', '-1.01', '+0.17', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.68\n",
      "['+0.25', '+0.31', '+0.77', '-1.54', '-0.22', '-0.77', '+0.00', '+0.00']\n",
      "step 40 total_reward -64.42\n",
      "['+0.32', '+0.07', '-0.35', '+0.17', '-1.28', '-2.11', '+1.00', '+0.00']\n",
      "step 51 total_reward -180.31\n",
      "['+0.01', '+0.94', '+0.45', '-0.04', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.46\n",
      "['+0.08', '+0.85', '+0.26', '-0.58', '+0.33', '+0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.57\n",
      "['+0.15', '+0.60', '+0.38', '-1.05', '+0.78', '+0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -111.52\n",
      "['+0.23', '+0.21', '+0.50', '-1.44', '+0.32', '-0.85', '+0.00', '+0.00']\n",
      "step 60 total_reward -77.80\n",
      "['+0.28', '-0.01', '+0.14', '-0.77', '-0.01', '+5.13', '+1.00', '+1.00']\n",
      "step 70 total_reward -149.76\n",
      "['-0.00', '+0.94', '-0.24', '+0.10', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.29\n",
      "['-0.07', '+0.89', '-0.43', '-0.46', '+0.50', '+0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.23\n",
      "['-0.18', '+0.67', '-0.93', '-1.02', '+1.77', '+1.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -265.58\n",
      "['-0.38', '+0.27', '-0.87', '-1.61', '+3.62', '+2.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -475.29\n",
      "['-0.45', '+0.07', '+0.29', '+0.03', '+4.02', '-0.94', '+1.00', '+0.00']\n",
      "step 69 total_reward -550.89\n",
      "['-0.01', '+0.92', '-0.68', '-0.54', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.51\n",
      "['-0.17', '+0.69', '-1.02', '-0.83', '+0.57', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.52\n",
      "['-0.42', '+0.41', '-1.32', '-1.24', '+1.52', '+1.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -217.33\n",
      "['-0.66', '+0.02', '-1.07', '+0.06', '+3.06', '+0.00', '+0.00', '+0.00']\n",
      "step 58 total_reward -497.34\n",
      "['+0.01', '+0.95', '+0.36', '+0.18', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.51\n",
      "['+0.06', '+0.92', '+0.17', '-0.37', '+0.36', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.71\n",
      "['+0.10', '+0.73', '+0.17', '-0.87', '+1.20', '+0.52', '+0.00', '+0.00']\n",
      "step 40 total_reward -144.97\n",
      "['+0.14', '+0.40', '+0.23', '-1.27', '+1.18', '-0.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -154.09\n",
      "['+0.14', '-0.01', '+0.01', '-0.68', '+0.29', '-5.49', '+1.00', '+1.00']\n",
      "step 80 total_reward -152.67\n",
      "['+0.00', '+0.94', '+0.06', '-0.09', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.08\n",
      "['-0.00', '+0.83', '-0.13', '-0.64', '+0.43', '+0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -88.06\n",
      "['-0.08', '+0.59', '-0.99', '-0.88', '+1.44', '+1.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -237.66\n",
      "['-0.36', '+0.25', '-1.40', '-1.55', '+2.79', '+1.69', '+0.00', '+0.00']\n",
      "step 60 total_reward -435.21\n",
      "['-0.59', '-0.13', '-1.75', '-0.48', '+3.71', '-2.91', '+1.00', '+0.00']\n",
      "step 75 total_reward -683.93\n",
      "['-0.02', '+0.95', '-0.80', '+0.21', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.63\n",
      "['-0.19', '+0.92', '-0.99', '-0.36', '+0.65', '+1.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -87.63\n",
      "['-0.41', '+0.73', '-1.40', '-1.05', '+2.09', '+1.67', '+0.00', '+0.00']\n",
      "step 40 total_reward -291.87\n",
      "['-0.74', '+0.30', '-1.52', '-1.67', '+4.06', '+2.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -537.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.82', '+0.15', '+0.13', '+0.03', '+4.03', '-0.51', '+1.00', '+0.00']\n",
      "step 68 total_reward -501.57\n",
      "['+0.01', '+0.93', '+0.73', '-0.24', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.07\n",
      "['+0.15', '+0.78', '+0.66', '-0.77', '+0.19', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.93\n",
      "['+0.29', '+0.47', '+0.85', '-1.29', '-0.08', '-0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.35\n",
      "['+0.46', '+0.04', '+0.37', '-0.12', '-1.32', '-6.25', '+1.00', '+0.00']\n",
      "step 59 total_reward -269.70\n",
      "['-0.02', '+0.94', '-0.80', '-0.05', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.20\n",
      "['-0.19', '+0.84', '-0.98', '-0.61', '+0.63', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -91.98\n",
      "['-0.44', '+0.59', '-1.74', '-1.14', '+1.93', '+1.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -305.50\n",
      "['-0.79', '+0.16', '-1.59', '-1.70', '+3.79', '+2.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -523.63\n",
      "['-0.81', '+0.13', '+0.00', '-0.08', '+3.90', '+0.62', '+1.00', '+0.00']\n",
      "step 62 total_reward -627.83\n",
      "['+0.01', '+0.95', '+0.42', '+0.43', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.47\n",
      "['+0.09', '+1.00', '+0.39', '-0.10', '+0.05', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward +12.46\n",
      "['+0.16', '+0.89', '+0.29', '-0.64', '+0.22', '+0.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -25.18\n",
      "['+0.23', '+0.61', '+0.41', '-1.13', '+0.52', '-0.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -80.47\n",
      "['+0.32', '+0.20', '+0.58', '-1.62', '+0.00', '-0.86', '+0.00', '+0.00']\n",
      "step 80 total_reward -53.82\n",
      "['+0.39', '-0.08', '+1.24', '-0.24', '-0.28', '+0.00', '+1.00', '+1.00']\n",
      "step 91 total_reward -201.39\n",
      "['+0.00', '+0.93', '+0.14', '-0.25', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.39\n",
      "['+0.01', '+0.77', '-0.05', '-0.80', '+0.41', '+0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -78.54\n",
      "['-0.07', '+0.54', '-0.90', '-0.84', '+1.31', '+0.90', '+0.00', '+0.00']\n",
      "step 40 total_reward -193.82\n",
      "['-0.31', '+0.21', '-1.33', '-1.50', '+2.51', '+1.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -375.82\n",
      "['-0.42', '+0.04', '-1.24', '-0.25', '+3.20', '+5.47', '+0.00', '+0.00']\n",
      "step 68 total_reward -540.56\n",
      "['-0.00', '+0.93', '-0.23', '-0.44', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.37\n",
      "['-0.07', '+0.72', '-0.45', '-0.79', '+0.43', '+0.55', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.16\n",
      "['-0.24', '+0.52', '-1.23', '-0.71', '+1.00', '+0.73', '+0.00', '+0.00']\n",
      "step 40 total_reward -164.28\n",
      "['-0.49', '+0.21', '-1.25', '-1.35', '+2.22', '+1.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -325.38\n",
      "['-0.55', '+0.12', '-0.93', '-0.55', '+2.66', '+6.29', '+0.00', '+0.00']\n",
      "step 65 total_reward -463.90\n",
      "['+0.01', '+0.93', '+0.39', '-0.28', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.56\n",
      "['+0.07', '+0.76', '+0.32', '-0.81', '+0.26', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -48.46\n",
      "['+0.15', '+0.44', '+0.51', '-1.31', '+0.05', '-0.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -52.56\n",
      "['+0.27', '-0.01', '+0.70', '-1.66', '-0.84', '-0.96', '+1.00', '+0.00']\n",
      "step 60 total_reward -142.23\n",
      "['+0.29', '-0.05', '+1.11', '-0.74', '-0.70', '+3.81', '+1.00', '+0.00']\n",
      "step 62 total_reward -241.77\n",
      "['+0.01', '+0.95', '+0.36', '+0.39', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.72\n",
      "['+0.07', '+0.98', '+0.24', '-0.15', '+0.22', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward +1.07\n",
      "['+0.10', '+0.85', '+0.09', '-0.75', '+1.06', '+1.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.36\n",
      "['+0.13', '+0.57', '+0.11', '-1.16', '+1.79', '+0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -204.92\n",
      "['+0.13', '+0.15', '-0.28', '-1.60', '+1.58', '-0.42', '+0.00', '+0.00']\n",
      "step 80 total_reward -194.04\n",
      "['+0.10', '-0.01', '-0.87', '-0.21', '+1.74', '+5.58', '+0.00', '+1.00']\n",
      "step 87 total_reward -285.94\n",
      "['+0.01', '+0.95', '+0.71', '+0.22', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.59\n",
      "['+0.14', '+0.93', '+0.51', '-0.32', '+0.28', '+0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.55\n",
      "['+0.23', '+0.75', '+0.45', '-0.85', '+1.24', '+0.82', '+0.00', '+0.00']\n",
      "step 40 total_reward -128.89\n",
      "['+0.33', '+0.43', '+0.47', '-1.26', '+1.57', '-0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -176.26\n",
      "['+0.40', '+0.08', '-0.43', '-0.09', '+1.27', '+2.31', '+0.00', '+1.00']\n",
      "step 77 total_reward -242.07\n",
      "['+0.01', '+0.96', '+0.62', '+0.48', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.14\n",
      "['+0.13', '+1.02', '+0.58', '-0.05', '+0.02', '+0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward +13.83\n",
      "['+0.24', '+0.92', '+0.48', '-0.59', '+0.17', '+0.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -12.08\n",
      "['+0.34', '+0.66', '+0.53', '-1.09', '+0.62', '+0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -82.57\n",
      "['+0.46', '+0.26', '+0.70', '-1.55', '+0.30', '-0.72', '+0.00', '+0.00']\n",
      "step 80 total_reward -78.65\n",
      "['+0.55', '-0.07', '+0.58', '-0.90', '-0.15', '+6.14', '+1.00', '+0.00']\n",
      "step 93 total_reward -195.61\n",
      "['-0.01', '+0.94', '-0.53', '-0.15', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.97\n",
      "['-0.13', '+0.80', '-0.71', '-0.72', '+0.55', '+0.89', '+0.00', '+0.00']\n",
      "step 20 total_reward -88.93\n",
      "['-0.33', '+0.56', '-1.28', '-1.05', '+1.68', '+1.40', '+0.00', '+0.00']\n",
      "step 40 total_reward -253.13\n",
      "['-0.59', '+0.15', '-1.19', '-1.64', '+3.53', '+2.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -472.35\n",
      "['-0.67', '+0.00', '-1.42', '-0.15', '+4.13', '+3.20', '+0.00', '+0.00']\n",
      "step 66 total_reward -637.58\n",
      "['+0.01', '+0.92', '+0.64', '-0.58', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.49\n",
      "['+0.13', '+0.67', '+0.66', '-1.11', '+0.05', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.61\n",
      "['+0.28', '+0.25', '+0.84', '-1.66', '-0.51', '-0.94', '+0.00', '+0.00']\n",
      "step 40 total_reward -96.97\n",
      "['+0.39', '-0.06', '+1.52', '-0.32', '-1.23', '-1.56', '+1.00', '+0.00']\n",
      "step 52 total_reward -280.76\n",
      "['-0.01', '+0.93', '-0.57', '-0.45', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.52\n",
      "['-0.14', '+0.71', '-0.84', '-0.85', '+0.59', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -88.73\n",
      "['-0.36', '+0.43', '-1.16', '-1.26', '+1.68', '+1.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -235.28\n",
      "['-0.56', '+0.05', '-0.17', '-0.25', '+3.14', '-3.50', '+0.00', '+0.00']\n",
      "step 57 total_reward -516.99\n",
      "['+0.01', '+0.95', '+0.52', '+0.41', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.55\n",
      "['+0.11', '+0.99', '+0.47', '-0.12', '+0.10', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward +5.67\n",
      "['+0.19', '+0.87', '+0.33', '-0.68', '+0.52', '+0.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -52.62\n",
      "['+0.28', '+0.60', '+0.49', '-1.12', '+0.78', '-0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -102.81\n",
      "['+0.39', '+0.19', '+0.66', '-1.59', '+0.09', '-1.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -61.54\n",
      "['+0.44', '-0.05', '-0.26', '-0.15', '-0.37', '+2.03', '+1.00', '+0.00']\n",
      "step 90 total_reward -194.89\n",
      "['+0.00', '+0.93', '+0.10', '-0.42', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.61\n",
      "['+0.01', '+0.72', '-0.13', '-0.82', '+0.40', '+0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.95\n",
      "['-0.10', '+0.54', '-0.99', '-0.63', '+1.06', '+0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -150.59\n",
      "['-0.33', '+0.28', '-1.17', '-1.22', '+2.11', '+1.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -297.85\n",
      "['-0.53', '-0.03', '-0.92', '+0.08', '+3.01', '-0.00', '+0.00', '+0.00']\n",
      "step 75 total_reward -529.21\n",
      "['+0.00', '+0.93', '+0.09', '-0.48', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.51\n",
      "['+0.00', '+0.71', '-0.11', '-0.84', '+0.40', '+0.64', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.82\n",
      "['-0.10', '+0.51', '-1.01', '-0.59', '+1.06', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -142.43\n",
      "['-0.32', '+0.24', '-1.28', '-1.30', '+2.20', '+1.49', '+0.00', '+0.00']\n",
      "step 60 total_reward -311.43\n",
      "['-0.53', '-0.09', '-1.59', '-0.90', '+3.18', '+5.86', '+0.00', '+0.00']\n",
      "step 75 total_reward -537.27\n",
      "['-0.00', '+0.93', '-0.25', '-0.28', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.52\n",
      "['-0.07', '+0.77', '-0.50', '-0.74', '+0.51', '+0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -88.58\n",
      "['-0.26', '+0.56', '-1.46', '-0.77', '+1.32', '+0.83', '+0.00', '+0.00']\n",
      "step 40 total_reward -236.71\n",
      "['-0.55', '+0.24', '-1.39', '-1.42', '+2.66', '+1.80', '+0.00', '+0.00']\n",
      "step 60 total_reward -403.17\n",
      "['-0.81', '-0.20', '-1.94', '-0.60', '+4.56', '+4.48', '+1.00', '+0.00']\n",
      "step 78 total_reward -725.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.01', '+0.95', '-0.58', '+0.19', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.30\n",
      "['-0.14', '+0.92', '-0.76', '-0.37', '+0.60', '+0.95', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.05\n",
      "['-0.32', '+0.72', '-1.39', '-1.01', '+1.97', '+1.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -295.04\n",
      "['-0.67', '+0.30', '-1.63', '-1.73', '+3.66', '+2.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -527.18\n",
      "['-0.84', '+0.01', '-1.99', '-1.14', '+4.77', '+7.04', '+1.00', '+0.00']\n",
      "step 70 total_reward -747.78\n",
      "['+0.00', '+0.93', '+0.22', '-0.37', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.03\n",
      "['+0.03', '+0.74', '+0.04', '-0.82', '+0.37', '+0.62', '+0.00', '+0.00']\n",
      "step 20 total_reward -59.39\n",
      "['-0.02', '+0.53', '-0.69', '-0.70', '+1.00', '+0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -122.78\n",
      "['-0.24', '+0.26', '-1.37', '-1.20', '+1.85', '+1.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -277.21\n",
      "['-0.44', '+0.01', '-1.79', '-0.44', '+2.32', '-1.57', '+0.00', '+0.00']\n",
      "step 73 total_reward -463.72\n",
      "['+0.01', '+0.93', '+0.30', '-0.41', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.42\n",
      "['+0.08', '+0.72', '+0.49', '-0.96', '-0.52', '-0.88', '+0.00', '+0.00']\n",
      "step 20 total_reward -91.73\n",
      "['+0.19', '+0.34', '+0.56', '-1.61', '-1.88', '-1.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -257.25\n",
      "['+0.26', '+0.01', '-0.07', '-0.68', '-3.00', '+5.47', '+0.00', '+0.00']\n",
      "step 53 total_reward -501.85\n",
      "['+0.02', '+0.93', '+0.76', '-0.22', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.05\n",
      "['+0.15', '+0.78', '+0.65', '-0.75', '+0.22', '+0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.17\n",
      "['+0.30', '+0.48', '+0.83', '-1.26', '+0.12', '-0.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.27\n",
      "['+0.48', '+0.02', '+0.99', '-1.81', '-0.75', '-1.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -156.29\n",
      "['+0.50', '-0.04', '+0.14', '-0.41', '-1.27', '-8.62', '+1.00', '+0.00']\n",
      "step 63 total_reward -264.18\n",
      "['+0.01', '+0.93', '+0.63', '-0.51', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.58\n",
      "['+0.13', '+0.69', '+0.59', '-1.04', '+0.17', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.09\n",
      "['+0.26', '+0.30', '+0.79', '-1.56', '-0.19', '-0.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -60.49\n",
      "['+0.37', '-0.07', '+1.28', '-0.79', '-0.81', '+4.23', '+1.00', '+0.00']\n",
      "step 54 total_reward -246.10\n",
      "['+0.01', '+0.95', '+0.61', '+0.20', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.89\n",
      "['+0.12', '+0.92', '+0.42', '-0.34', '+0.28', '+0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.06\n",
      "['+0.20', '+0.74', '+0.43', '-0.84', '+1.02', '+0.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -114.09\n",
      "['+0.30', '+0.42', '+0.53', '-1.26', '+0.94', '-0.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -123.61\n",
      "['+0.40', '+0.02', '+0.08', '+0.01', '+0.11', '-0.00', '+1.00', '+1.00']\n",
      "step 79 total_reward -152.54\n",
      "['+0.00', '+0.95', '+0.13', '+0.41', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.04\n",
      "['+0.03', '+0.99', '+0.11', '-0.12', '+0.08', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward +16.55\n",
      "['+0.04', '+0.87', '-0.00', '-0.67', '+0.32', '+0.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.38\n",
      "['+0.02', '+0.62', '-0.41', '-0.89', '+0.89', '+0.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -111.12\n",
      "['-0.17', '+0.32', '-1.30', '-1.19', '+1.36', '+0.68', '+0.00', '+0.00']\n",
      "step 80 total_reward -215.22\n",
      "['-0.41', '+0.03', '-1.64', '-0.14', '+1.90', '+0.06', '+0.00', '+1.00']\n",
      "step 95 total_reward -390.51\n",
      "['-0.01', '+0.93', '-0.46', '-0.45', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.33\n",
      "['-0.12', '+0.72', '-0.77', '-0.78', '+0.54', '+0.73', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.24\n",
      "['-0.32', '+0.47', '-1.10', '-1.10', '+1.49', '+1.32', '+0.00', '+0.00']\n",
      "step 40 total_reward -210.12\n",
      "['-0.55', '+0.06', '-1.10', '-1.68', '+3.16', '+1.64', '+0.00', '+0.00']\n",
      "step 60 total_reward -421.03\n",
      "['-0.54', '+0.05', '+0.27', '-0.26', '+3.09', '-2.36', '+0.00', '+0.00']\n",
      "step 61 total_reward -521.03\n",
      "['+0.01', '+0.93', '+0.61', '-0.20', '-0.02', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.13\n",
      "['+0.12', '+0.79', '+0.60', '-0.73', '+0.04', '-0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.22\n",
      "['+0.26', '+0.49', '+0.79', '-1.29', '-0.49', '-0.90', '+0.00', '+0.00']\n",
      "step 40 total_reward -98.70\n",
      "['+0.40', '+0.08', '-0.54', '-0.32', '-1.57', '+2.18', '+1.00', '+0.00']\n",
      "step 58 total_reward -347.22\n",
      "['+0.00', '+0.93', '+0.22', '-0.28', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.54\n",
      "['+0.06', '+0.76', '+0.42', '-0.84', '-0.51', '-0.91', '+0.00', '+0.00']\n",
      "step 20 total_reward -95.71\n",
      "['+0.15', '+0.42', '+0.49', '-1.49', '-1.88', '-1.82', '+0.00', '+0.00']\n",
      "step 40 total_reward -264.05\n",
      "['+0.24', '+0.00', '+0.31', '-0.31', '-3.44', '+3.50', '+0.00', '+0.00']\n",
      "step 57 total_reward -562.05\n",
      "['+0.01', '+0.95', '+0.36', '+0.25', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.65\n",
      "['+0.06', '+0.94', '+0.17', '-0.30', '+0.35', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.93\n",
      "['+0.09', '+0.76', '+0.09', '-0.86', '+1.43', '+1.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -166.31\n",
      "['+0.11', '+0.45', '+0.05', '-1.26', '+1.96', '+0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -228.58\n",
      "['+0.06', '+0.00', '-0.60', '-0.01', '+1.90', '-0.12', '+0.00', '+1.00']\n",
      "step 80 total_reward -321.08\n",
      "['-0.02', '+0.95', '-0.77', '+0.44', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.14\n",
      "['-0.17', '+1.00', '-0.78', '-0.09', '+0.29', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.61\n",
      "['-0.32', '+0.89', '-0.84', '-0.65', '+0.61', '+0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.75\n",
      "['-0.53', '+0.63', '-1.39', '-1.08', '+1.48', '+1.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -225.33\n",
      "['-0.81', '+0.21', '-1.28', '-1.71', '+3.04', '+1.97', '+0.00', '+0.00']\n",
      "step 80 total_reward -420.50\n",
      "['-0.95', '-0.04', '-1.71', '-0.40', '+3.89', '-1.19', '+0.00', '+0.00']\n",
      "step 90 total_reward -639.03\n",
      "['-0.01', '+0.94', '-0.70', '-0.16', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.95\n",
      "['-0.17', '+0.80', '-0.89', '-0.72', '+0.61', '+0.98', '+0.00', '+0.00']\n",
      "step 20 total_reward -92.58\n",
      "['-0.43', '+0.56', '-1.72', '-0.99', '+1.76', '+1.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -284.67\n",
      "['-0.77', '+0.18', '-1.57', '-1.58', '+3.53', '+2.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -495.01\n",
      "['-1.01', '-0.20', '-1.53', '-0.97', '+4.50', '-6.50', '+0.00', '+0.00']\n",
      "step 75 total_reward -725.82\n",
      "['-0.00', '+0.95', '-0.22', '+0.43', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.78\n",
      "['-0.05', '+1.00', '-0.26', '-0.10', '+0.24', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.13\n",
      "['-0.11', '+0.88', '-0.34', '-0.66', '+0.59', '+0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -78.13\n",
      "['-0.21', '+0.61', '-0.86', '-1.17', '+1.62', '+1.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -229.24\n",
      "['-0.39', '+0.16', '-0.81', '-1.81', '+3.35', '+1.87', '+0.00', '+0.00']\n",
      "step 80 total_reward -434.00\n",
      "['-0.43', '+0.04', '+0.09', '-0.47', '+3.94', '+5.74', '+1.00', '+0.00']\n",
      "step 85 total_reward -581.99\n",
      "['+0.00', '+0.93', '+0.15', '-0.46', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.41\n",
      "['+0.02', '+0.71', '-0.11', '-0.74', '+0.36', '+0.56', '+0.00', '+0.00']\n",
      "step 20 total_reward -44.22\n",
      "['-0.08', '+0.53', '-0.95', '-0.50', '+0.92', '+0.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -120.90\n",
      "['-0.33', '+0.33', '-1.30', '-1.01', '+1.76', '+1.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -256.99\n",
      "['-0.60', '-0.05', '-1.35', '-1.53', '+3.03', '+1.10', '+0.00', '+0.00']\n",
      "step 80 total_reward -436.16\n",
      "['-0.63', '-0.08', '-1.32', '-0.03', '+3.14', '-0.27', '+0.00', '+0.00']\n",
      "step 82 total_reward -545.30\n",
      "['+0.00', '+0.92', '+0.18', '-0.58', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['+0.02', '+0.68', '-0.09', '-0.86', '+0.31', '+0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.60\n",
      "['-0.07', '+0.49', '-0.85', '-0.49', '+0.73', '+0.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.32\n",
      "['-0.28', '+0.28', '-1.34', '-0.96', '+1.49', '+1.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -212.47\n",
      "['-0.59', '-0.06', '-1.56', '-1.41', '+2.27', '+0.58', '+0.00', '+0.00']\n",
      "step 80 total_reward -357.17\n",
      "['-0.62', '-0.09', '-1.93', '-0.53', '+2.37', '+1.13', '+0.00', '+1.00']\n",
      "step 82 total_reward -463.67\n",
      "['-0.00', '+0.95', '-0.04', '+0.38', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.01', '+0.98', '-0.07', '-0.15', '+0.16', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward +4.86\n",
      "['-0.04', '+0.85', '-0.22', '-0.72', '+0.61', '+0.85', '+0.00', '+0.00']\n",
      "step 40 total_reward -86.91\n",
      "['-0.15', '+0.59', '-1.18', '-1.04', '+1.66', '+1.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -254.03\n",
      "['-0.45', '+0.18', '-1.45', '-1.67', '+3.05', '+1.56', '+0.00', '+0.00']\n",
      "step 80 total_reward -445.67\n",
      "['-0.53', '+0.06', '-1.85', '-0.33', '+3.40', '-0.00', '+0.00', '+0.00']\n",
      "step 85 total_reward -588.74\n",
      "['+0.01', '+0.93', '+0.25', '-0.25', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.56\n",
      "['+0.04', '+0.77', '+0.07', '-0.80', '+0.37', '+0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -66.15\n",
      "['-0.02', '+0.55', '-0.86', '-0.73', '+1.06', '+0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -151.09\n",
      "['-0.26', '+0.28', '-1.30', '-1.24', '+1.98', '+1.32', '+0.00', '+0.00']\n",
      "step 60 total_reward -296.70\n",
      "['-0.50', '-0.07', '-1.51', '-0.74', '+2.95', '+6.63', '+0.00', '+0.00']\n",
      "step 76 total_reward -536.34\n",
      "['-0.02', '+0.93', '-0.81', '-0.34', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.27\n",
      "['-0.19', '+0.74', '-1.00', '-0.91', '+0.64', '+1.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -96.31\n",
      "['-0.43', '+0.43', '-1.24', '-1.35', '+1.94', '+1.73', '+0.00', '+0.00']\n",
      "step 40 total_reward -261.06\n",
      "['-0.63', '+0.07', '-0.99', '-0.43', '+3.28', '-4.24', '+0.00', '+0.00']\n",
      "step 56 total_reward -544.03\n",
      "['+0.01', '+0.95', '+0.66', '+0.19', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.76\n",
      "['+0.13', '+0.92', '+0.46', '-0.36', '+0.31', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.87\n",
      "['+0.21', '+0.73', '+0.44', '-0.87', '+1.17', '+0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -126.53\n",
      "['+0.31', '+0.41', '+0.49', '-1.27', '+1.27', '-0.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -151.16\n",
      "['+0.40', '-0.00', '+0.63', '-0.57', '+0.26', '-5.87', '+0.00', '+1.00']\n",
      "step 79 total_reward -187.99\n",
      "['+0.01', '+0.94', '+0.41', '-0.15', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.09\n",
      "['+0.07', '+0.81', '+0.29', '-0.69', '+0.32', '+0.44', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.49\n",
      "['+0.15', '+0.52', '+0.48', '-1.17', '+0.31', '-0.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -74.80\n",
      "['+0.26', '+0.11', '+0.66', '-1.46', '-0.47', '-1.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -100.88\n",
      "['+0.30', '-0.05', '+1.04', '-0.83', '-0.77', '+3.77', '+1.00', '+0.00']\n",
      "step 67 total_reward -237.44\n",
      "['-0.02', '+0.94', '-0.80', '-0.20', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.01\n",
      "['-0.19', '+0.79', '-0.99', '-0.77', '+0.66', '+1.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -97.24\n",
      "['-0.45', '+0.52', '-1.45', '-1.20', '+1.91', '+1.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -274.92\n",
      "['-0.74', '+0.07', '-1.31', '-1.77', '+3.94', '+2.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -515.90\n",
      "['-0.81', '-0.08', '-1.84', '-0.77', '+4.53', '+6.14', '+0.00', '+0.00']\n",
      "step 65 total_reward -675.53\n",
      "['+0.00', '+0.93', '+0.23', '-0.26', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.78\n",
      "['+0.03', '+0.77', '+0.03', '-0.81', '+0.41', '+0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -71.69\n",
      "['-0.02', '+0.53', '-0.76', '-0.87', '+1.33', '+0.96', '+0.00', '+0.00']\n",
      "step 40 total_reward -180.30\n",
      "['-0.26', '+0.18', '-1.42', '-1.60', '+2.47', '+1.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -373.59\n",
      "['-0.41', '-0.07', '-1.95', '-0.75', '+3.09', '+4.91', '+0.00', '+0.00']\n",
      "step 70 total_reward -545.92\n",
      "['+0.00', '+0.95', '+0.12', '+0.14', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.58\n",
      "['+0.01', '+0.90', '-0.08', '-0.42', '+0.43', '+0.81', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.91\n",
      "['-0.04', '+0.70', '-0.70', '-0.93', '+1.67', '+1.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -240.84\n",
      "['-0.28', '+0.31', '-1.21', '-1.63', '+3.23', '+1.85', '+0.00', '+0.00']\n",
      "step 60 total_reward -459.84\n",
      "['-0.42', '+0.02', '-1.35', '-0.29', '+4.32', '+5.50', '+1.00', '+0.00']\n",
      "step 71 total_reward -677.47\n",
      "['+0.01', '+0.95', '+0.37', '+0.27', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.57\n",
      "['+0.07', '+0.95', '+0.22', '-0.27', '+0.25', '+0.54', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.27\n",
      "['+0.11', '+0.78', '+0.16', '-0.80', '+1.09', '+0.71', '+0.00', '+0.00']\n",
      "step 40 total_reward -127.61\n",
      "['+0.15', '+0.48', '+0.22', '-1.21', '+1.30', '-0.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -161.73\n",
      "['+0.17', '+0.06', '-0.13', '-1.45', '+0.61', '-0.84', '+0.00', '+0.00']\n",
      "step 80 total_reward -85.32\n",
      "['+0.16', '-0.02', '+0.04', '-0.65', '+0.20', '-5.57', '+0.00', '+1.00']\n",
      "step 84 total_reward -151.49\n",
      "['+0.01', '+0.93', '+0.30', '-0.25', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.54\n",
      "['+0.05', '+0.77', '+0.17', '-0.79', '+0.34', '+0.49', '+0.00', '+0.00']\n",
      "step 20 total_reward -59.96\n",
      "['+0.10', '+0.46', '+0.36', '-1.26', '+0.38', '-0.37', '+0.00', '+0.00']\n",
      "step 40 total_reward -85.81\n",
      "['+0.18', '+0.03', '+0.47', '-1.52', '-0.26', '-0.71', '+1.00', '+0.00']\n",
      "step 60 total_reward -64.99\n",
      "['+0.19', '-0.02', '-0.35', '-0.71', '+0.21', '+5.78', '+1.00', '+1.00']\n",
      "step 63 total_reward -94.91\n",
      "['+0.01', '+0.93', '+0.75', '-0.28', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.35\n",
      "['+0.15', '+0.76', '+0.65', '-0.82', '+0.23', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.09\n",
      "['+0.30', '+0.44', '+0.84', '-1.32', '+0.09', '-0.52', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.49\n",
      "['+0.48', '-0.02', '+1.15', '-0.56', '-0.58', '+4.21', '+1.00', '+0.00']\n",
      "step 60 total_reward -233.24\n",
      "['+0.00', '+0.95', '+0.06', '+0.22', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.42\n",
      "['-0.00', '+0.93', '-0.14', '-0.33', '+0.44', '+0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.52\n",
      "['-0.05', '+0.75', '-0.60', '-0.90', '+1.65', '+1.38', '+0.00', '+0.00']\n",
      "step 40 total_reward -230.84\n",
      "['-0.28', '+0.35', '-1.23', '-1.69', '+3.08', '+1.68', '+0.00', '+0.00']\n",
      "step 60 total_reward -448.77\n",
      "['-0.39', '+0.11', '-0.36', '-0.59', '+3.99', '+7.25', '+0.00', '+0.00']\n",
      "step 69 total_reward -637.91\n",
      "['+0.00', '+0.95', '+0.05', '+0.24', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.36\n",
      "['+0.01', '+0.94', '+0.01', '-0.29', '+0.05', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -6.20\n",
      "['+0.00', '+0.77', '-0.04', '-0.82', '+0.53', '+0.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -90.21\n",
      "['-0.10', '+0.57', '-1.02', '-0.59', '+0.87', '+0.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -147.70\n",
      "['-0.36', '+0.34', '-1.34', '-1.08', '+1.44', '+0.99', '+0.00', '+0.00']\n",
      "step 80 total_reward -252.13\n",
      "['-0.56', '+0.06', '-1.50', '-0.34', '+2.15', '-2.19', '+0.00', '+1.00']\n",
      "step 95 total_reward -460.60\n",
      "['+0.01', '+0.93', '+0.75', '-0.25', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.11\n",
      "['+0.15', '+0.77', '+0.64', '-0.78', '+0.23', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.00\n",
      "['+0.30', '+0.46', '+0.84', '-1.29', '+0.05', '-0.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.77\n",
      "['+0.48', '-0.01', '+0.98', '-1.85', '-0.90', '-1.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -174.90\n",
      "['+0.54', '-0.08', '+0.73', '-0.43', '-0.24', '+3.15', '+0.00', '+0.00']\n",
      "step 68 total_reward -109.05\n",
      "['+0.01', '+0.93', '+0.50', '-0.48', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.56\n",
      "['+0.09', '+0.70', '+0.41', '-1.01', '+0.25', '+0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.58\n",
      "['+0.20', '+0.32', '+0.60', '-1.51', '+0.08', '-0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.68\n",
      "['+0.29', '-0.06', '+1.05', '-0.23', '-0.31', '-0.00', '+1.00', '+1.00']\n",
      "step 56 total_reward -167.21\n",
      "['-0.01', '+0.93', '-0.39', '-0.24', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.64\n",
      "['-0.10', '+0.78', '-0.60', '-0.75', '+0.55', '+0.88', '+0.00', '+0.00']\n",
      "step 20 total_reward -92.96\n",
      "['-0.30', '+0.55', '-1.26', '-0.93', '+1.48', '+1.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -235.03\n",
      "['-0.55', '+0.18', '-1.21', '-1.55', '+3.12', '+1.82', '+0.00', '+0.00']\n",
      "step 60 total_reward -434.36\n",
      "['-0.65', '+0.01', '-1.42', '-0.50', '+3.66', '-3.26', '+0.00', '+0.00']\n",
      "step 67 total_reward -605.28\n",
      "['-0.00', '+0.93', '-0.06', '-0.28', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.62\n",
      "['-0.03', '+0.77', '-0.29', '-0.77', '+0.47', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -87.70\n",
      "['-0.18', '+0.58', '-1.31', '-0.69', '+1.28', '+0.80', '+0.00', '+0.00']\n",
      "step 40 total_reward -224.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.44', '+0.28', '-1.26', '-1.34', '+2.61', '+1.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -385.94\n",
      "['-0.64', '-0.05', '-0.75', '+0.09', '+3.85', '+2.30', '+1.00', '+0.00']\n",
      "step 75 total_reward -641.04\n",
      "['-0.01', '+0.92', '-0.31', '-0.56', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.46\n",
      "['-0.09', '+0.70', '-0.67', '-0.70', '+0.44', '+0.50', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.26\n",
      "['-0.30', '+0.53', '-1.47', '-0.58', '+0.97', '+0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -168.89\n",
      "['-0.60', '+0.26', '-1.49', '-1.23', '+2.08', '+1.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -320.41\n",
      "['-0.74', '+0.08', '-0.30', '+0.08', '+2.75', '-0.00', '+0.00', '+0.00']\n",
      "step 70 total_reward -518.00\n",
      "['+0.01', '+0.95', '+0.67', '+0.21', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.12\n",
      "['+0.13', '+0.93', '+0.48', '-0.34', '+0.30', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.71\n",
      "['+0.22', '+0.74', '+0.47', '-0.84', '+1.14', '+0.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -120.73\n",
      "['+0.32', '+0.43', '+0.53', '-1.25', '+1.19', '-0.44', '+0.00', '+0.00']\n",
      "step 60 total_reward -141.93\n",
      "['+0.43', '-0.02', '+0.61', '-1.71', '+0.30', '-1.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -89.09\n",
      "['+0.45', '-0.09', '+0.00', '+0.00', '+0.26', '-0.00', '+1.00', '+1.00']\n",
      "step 83 total_reward -159.89\n",
      "['-0.00', '+0.95', '-0.13', '+0.24', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.44\n",
      "['-0.01', '+0.93', '-0.03', '-0.30', '-0.38', '-0.42', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.42\n",
      "['-0.02', '+0.76', '+0.10', '-0.82', '-0.79', '-0.71', '+0.00', '+0.00']\n",
      "step 40 total_reward -115.56\n",
      "['+0.09', '+0.51', '+0.82', '-1.08', '-1.59', '-1.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -228.04\n",
      "['+0.26', '+0.09', '+0.74', '-1.76', '-3.15', '-1.90', '+0.00', '+0.00']\n",
      "step 80 total_reward -416.40\n",
      "['+0.28', '+0.02', '+0.22', '-0.31', '-3.35', '+3.77', '+0.00', '+0.00']\n",
      "step 83 total_reward -540.46\n",
      "['+0.01', '+0.95', '+0.52', '+0.14', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.86\n",
      "['+0.10', '+0.90', '+0.32', '-0.41', '+0.34', '+0.73', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.53\n",
      "['+0.17', '+0.70', '+0.38', '-0.89', '+0.99', '+0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.63\n",
      "['+0.25', '+0.37', '+0.50', '-1.31', '+0.74', '-0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -110.36\n",
      "['+0.34', '-0.03', '+0.39', '-0.00', '-0.02', '+0.00', '+1.00', '+1.00']\n",
      "step 78 total_reward -157.47\n",
      "['-0.02', '+0.93', '-0.80', '-0.30', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.41\n",
      "['-0.19', '+0.76', '-0.99', '-0.87', '+0.66', '+1.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -98.18\n",
      "['-0.48', '+0.50', '-1.64', '-1.12', '+1.81', '+1.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -275.14\n",
      "['-0.81', '+0.08', '-1.49', '-1.69', '+3.74', '+2.27', '+0.00', '+0.00']\n",
      "step 60 total_reward -507.20\n",
      "['-0.95', '-0.16', '-1.39', '-0.28', '+4.60', '+1.88', '+1.00', '+0.00']\n",
      "step 69 total_reward -717.65\n",
      "['-0.01', '+0.93', '-0.33', '-0.23', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.22\n",
      "['-0.09', '+0.78', '-0.53', '-0.79', '+0.55', '+0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -96.67\n",
      "['-0.28', '+0.55', '-1.29', '-0.97', '+1.57', '+1.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -252.02\n",
      "['-0.54', '+0.17', '-1.23', '-1.57', '+3.27', '+1.87', '+0.00', '+0.00']\n",
      "step 60 total_reward -455.79\n",
      "['-0.64', '+0.00', '-1.26', '-0.09', '+3.83', '-0.12', '+0.00', '+0.00']\n",
      "step 67 total_reward -628.93\n",
      "['-0.00', '+0.94', '-0.22', '-0.10', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.90\n",
      "['-0.06', '+0.82', '-0.41', '-0.66', '+0.48', '+0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -92.00\n",
      "['-0.21', '+0.60', '-1.16', '-0.91', '+1.58', '+1.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -255.99\n",
      "['-0.45', '+0.23', '-1.04', '-1.52', '+3.35', '+2.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -458.02\n",
      "['-0.56', '+0.00', '-0.26', '+0.03', '+4.21', '-0.20', '+1.00', '+0.00']\n",
      "step 70 total_reward -664.33\n",
      "['-0.02', '+0.95', '-0.78', '+0.26', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.67\n",
      "['-0.19', '+0.94', '-0.97', '-0.31', '+0.65', '+1.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -86.01\n",
      "['-0.39', '+0.76', '-1.20', '-1.00', '+2.13', '+1.78', '+0.00', '+0.00']\n",
      "step 40 total_reward -279.11\n",
      "['-0.68', '+0.33', '-1.33', '-1.69', '+4.11', '+2.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -528.93\n",
      "['-0.89', '-0.15', '+0.69', '-0.71', '+6.04', '-3.51', '+0.00', '+1.00']\n",
      "step 77 total_reward -873.56\n",
      "['-0.01', '+0.94', '-0.44', '+0.09', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.99\n",
      "['-0.11', '+0.89', '-0.64', '-0.47', '+0.55', '+0.94', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.67\n",
      "['-0.26', '+0.66', '-1.06', '-1.07', '+1.97', '+1.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -280.52\n",
      "['-0.49', '+0.24', '-0.98', '-1.64', '+4.09', '+2.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -517.92\n",
      "['-0.60', '-0.06', '-0.89', '-0.05', '+5.44', '+3.52', '+1.00', '+0.00']\n",
      "step 71 total_reward -759.21\n",
      "['-0.00', '+0.95', '-0.06', '+0.29', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.61\n",
      "['-0.03', '+0.95', '-0.26', '-0.26', '+0.47', '+0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.91\n",
      "['-0.09', '+0.79', '-0.34', '-0.90', '+1.78', '+1.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -227.23\n",
      "['-0.24', '+0.37', '-0.79', '-1.77', '+3.62', '+2.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -478.28\n",
      "['-0.35', '-0.05', '-1.43', '-0.51', '+4.72', '+1.23', '+1.00', '+0.00']\n",
      "step 74 total_reward -728.28\n",
      "['+0.01', '+0.93', '+0.30', '-0.31', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.80\n",
      "['+0.05', '+0.75', '+0.15', '-0.85', '+0.35', '+0.57', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.74\n",
      "['+0.07', '+0.48', '+0.11', '-1.11', '+0.69', '-0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -95.75\n",
      "['+0.08', '+0.11', '-0.10', '-1.19', '+0.29', '-0.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -32.36\n",
      "['+0.06', '-0.03', '-0.13', '-0.72', '+0.04', '-4.04', '+1.00', '+1.00']\n",
      "step 68 total_reward -90.78\n",
      "['-0.00', '+0.93', '-0.10', '-0.51', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.25\n",
      "['-0.04', '+0.70', '-0.40', '-0.81', '+0.43', '+0.57', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.44\n",
      "['-0.19', '+0.48', '-1.06', '-0.85', '+0.98', '+0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -149.49\n",
      "['-0.42', '+0.13', '-1.27', '-1.44', '+2.07', '+1.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -308.92\n",
      "['-0.52', '-0.01', '-1.48', '-0.22', '+2.34', '-0.62', '+0.00', '+1.00']\n",
      "step 67 total_reward -454.08\n",
      "['-0.01', '+0.95', '-0.51', '+0.32', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.64\n",
      "['-0.12', '+0.96', '-0.60', '-0.22', '+0.41', '+0.53', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.11\n",
      "['-0.25', '+0.81', '-0.74', '-0.85', '+1.39', '+1.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -180.57\n",
      "['-0.48', '+0.44', '-1.30', '-1.65', '+3.01', '+1.82', '+0.00', '+0.00']\n",
      "step 60 total_reward -423.86\n",
      "['-0.65', '+0.05', '+0.51', '+0.01', '+3.91', '-1.53', '+1.00', '+0.00']\n",
      "step 75 total_reward -545.01\n",
      "['-0.01', '+0.93', '-0.29', '-0.40', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.62\n",
      "['-0.08', '+0.73', '-0.53', '-0.83', '+0.49', '+0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.36\n",
      "['-0.26', '+0.49', '-1.13', '-0.97', '+1.32', '+1.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -200.71\n",
      "['-0.49', '+0.11', '-1.14', '-1.57', '+2.84', '+1.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -393.31\n",
      "['-0.53', '+0.03', '-0.29', '-0.49', '+2.87', '-4.89', '+0.00', '+0.00']\n",
      "step 64 total_reward -524.58\n",
      "['+0.00', '+0.95', '+0.09', '+0.14', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.45\n",
      "['+0.00', '+0.90', '-0.11', '-0.42', '+0.44', '+0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.09\n",
      "['-0.04', '+0.69', '-0.60', '-1.00', '+1.67', '+1.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -242.99\n",
      "['-0.25', '+0.29', '-1.02', '-1.68', '+3.23', '+1.88', '+0.00', '+0.00']\n",
      "step 60 total_reward -450.53\n",
      "['-0.36', '+0.01', '-1.13', '-0.08', '+4.15', '+3.41', '+1.00', '+0.00']\n",
      "step 70 total_reward -657.52\n",
      "['+0.01', '+0.93', '+0.64', '-0.47', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.27\n",
      "['+0.13', '+0.70', '+0.63', '-1.00', '+0.08', '-0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.27', '+0.32', '+0.82', '-1.55', '-0.42', '-0.86', '+0.00', '+0.00']\n",
      "step 40 total_reward -86.97\n",
      "['+0.43', '-0.02', '+0.64', '+0.22', '-1.93', '-2.16', '+0.00', '+0.00']\n",
      "step 57 total_reward -264.88\n",
      "['+0.01', '+0.93', '+0.40', '-0.38', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.67\n",
      "['+0.08', '+0.73', '+0.35', '-0.91', '+0.21', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -44.46\n",
      "['+0.16', '+0.38', '+0.53', '-1.42', '-0.05', '-0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -50.99\n",
      "['+0.27', '-0.01', '+0.64', '-0.36', '-0.58', '+3.85', '+1.00', '+0.00']\n",
      "step 57 total_reward -202.38\n",
      "['+0.00', '+0.94', '+0.20', '+0.03', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.39\n",
      "['+0.03', '+0.87', '+0.00', '-0.52', '+0.40', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.83\n",
      "['-0.03', '+0.66', '-0.79', '-0.83', '+1.47', '+1.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -216.61\n",
      "['-0.29', '+0.32', '-1.41', '-1.48', '+2.71', '+1.55', '+0.00', '+0.00']\n",
      "step 60 total_reward -410.34\n",
      "['-0.48', '+0.01', '-2.01', '-0.48', '+3.49', '-0.00', '+0.00', '+0.00']\n",
      "step 72 total_reward -625.85\n",
      "['+0.01', '+0.94', '+0.38', '-0.17', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.02\n",
      "['+0.10', '+0.80', '+0.57', '-0.73', '-0.55', '-0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -94.91\n",
      "['+0.22', '+0.49', '+0.64', '-1.38', '-1.96', '-1.87', '+0.00', '+0.00']\n",
      "step 40 total_reward -269.49\n",
      "['+0.35', '-0.01', '+0.48', '-1.91', '-4.29', '-2.72', '+0.00', '+0.00']\n",
      "step 60 total_reward -529.21\n",
      "['+0.37', '-0.06', '+1.45', '-0.67', '-4.61', '-5.28', '+0.00', '+0.00']\n",
      "step 62 total_reward -646.09\n",
      "['+0.01', '+0.95', '+0.34', '+0.34', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.99\n",
      "['+0.06', '+0.97', '+0.14', '-0.21', '+0.38', '+0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.14\n",
      "['+0.08', '+0.82', '+0.03', '-0.84', '+1.61', '+1.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -182.53\n",
      "['+0.07', '+0.51', '-0.08', '-1.29', '+2.85', '+0.84', '+0.00', '+0.00']\n",
      "step 60 total_reward -321.25\n",
      "['+0.04', '-0.00', '+0.14', '-0.79', '+3.24', '-7.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -533.67\n",
      "['-0.02', '+0.93', '-0.80', '-0.31', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.28\n",
      "['-0.19', '+0.75', '-1.04', '-0.80', '+0.62', '+0.89', '+0.00', '+0.00']\n",
      "step 20 total_reward -93.94\n",
      "['-0.50', '+0.53', '-1.93', '-0.94', '+1.57', '+1.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -271.60\n",
      "['-0.88', '+0.15', '-1.83', '-1.55', '+3.18', '+1.90', '+0.00', '+0.00']\n",
      "step 60 total_reward -476.20\n",
      "['-0.96', '+0.06', '-1.82', '-0.45', '+3.55', '-3.38', '+0.00', '+0.00']\n",
      "step 64 total_reward -614.56\n",
      "['+0.00', '+0.93', '+0.19', '-0.45', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.46\n",
      "['+0.02', '+0.72', '-0.07', '-0.79', '+0.41', '+0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.67\n",
      "['-0.08', '+0.54', '-0.97', '-0.56', '+1.09', '+0.71', '+0.00', '+0.00']\n",
      "step 40 total_reward -141.95\n",
      "['-0.32', '+0.30', '-1.18', '-1.13', '+2.13', '+1.51', '+0.00', '+0.00']\n",
      "step 60 total_reward -289.39\n",
      "['-0.60', '-0.13', '-0.23', '-0.01', '+3.38', '-1.37', '+0.00', '+0.00']\n",
      "step 80 total_reward -580.93\n",
      "['+0.01', '+0.93', '+0.68', '-0.33', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.30\n",
      "['+0.14', '+0.75', '+0.61', '-0.86', '+0.19', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.56\n",
      "['+0.28', '+0.41', '+0.81', '-1.38', '-0.09', '-0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.53\n",
      "['+0.42', '+0.00', '+0.40', '-0.66', '-1.02', '-6.92', '+1.00', '+0.00']\n",
      "step 57 total_reward -258.81\n",
      "['+0.01', '+0.93', '+0.34', '-0.39', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.79\n",
      "['+0.09', '+0.73', '+0.54', '-0.95', '-0.54', '-0.94', '+0.00', '+0.00']\n",
      "step 20 total_reward -94.39\n",
      "['+0.20', '+0.36', '+0.61', '-1.60', '-1.97', '-1.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -267.39\n",
      "['+0.29', '-0.01', '+0.40', '-0.60', '-3.27', '+5.03', '+0.00', '+0.00']\n",
      "step 54 total_reward -530.91\n",
      "['-0.01', '+0.95', '-0.32', '+0.25', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.00\n",
      "['-0.09', '+0.94', '-0.51', '-0.31', '+0.52', '+0.89', '+0.00', '+0.00']\n",
      "step 20 total_reward -70.43\n",
      "['-0.20', '+0.76', '-0.58', '-0.96', '+1.92', '+1.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -247.50\n",
      "['-0.40', '+0.34', '-0.90', '-1.68', '+3.95', '+2.36', '+0.00', '+0.00']\n",
      "step 60 total_reward -504.59\n",
      "['-0.52', '-0.00', '-1.67', '-0.52', '+4.89', '-0.19', '+1.00', '+0.00']\n",
      "step 73 total_reward -695.56\n",
      "['-0.02', '+0.94', '-0.81', '+0.11', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.62\n",
      "['-0.19', '+0.89', '-0.99', '-0.46', '+0.65', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.86\n",
      "['-0.44', '+0.67', '-1.81', '-1.07', '+1.99', '+1.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -317.11\n",
      "['-0.82', '+0.26', '-1.76', '-1.65', '+3.77', '+2.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -533.77\n",
      "['-0.98', '+0.03', '-1.40', '-0.28', '+4.48', '-5.68', '+1.00', '+0.00']\n",
      "step 69 total_reward -737.47\n",
      "['-0.01', '+0.95', '-0.57', '+0.16', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.03\n",
      "['-0.14', '+0.91', '-0.76', '-0.40', '+0.59', '+0.95', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.40\n",
      "['-0.31', '+0.71', '-1.22', '-1.01', '+1.99', '+1.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -283.18\n",
      "['-0.62', '+0.27', '-1.42', '-1.74', '+3.79', '+2.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -522.19\n",
      "['-0.71', '+0.02', '+0.40', '-0.07', '+4.21', '-1.23', '+0.00', '+0.00']\n",
      "step 80 total_reward -383.98\n",
      "['-0.62', '-0.08', '+0.40', '-0.60', '+2.99', '-1.22', '+0.00', '+0.00']\n",
      "step 100 total_reward -284.81\n",
      "['-0.61', '-0.11', '+0.26', '-0.02', '+2.83', '+0.38', '+0.00', '+0.00']\n",
      "step 103 total_reward -376.49\n",
      "['-0.01', '+0.94', '-0.44', '-0.03', '+0.01', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.93\n",
      "['-0.09', '+0.85', '-0.35', '-0.56', '-0.12', '-0.31', '+0.00', '+0.00']\n",
      "step 20 total_reward -23.76\n",
      "['-0.15', '+0.68', '-0.13', '-0.45', '-0.36', '-0.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -18.06\n",
      "['-0.13', '+0.59', '+0.42', '-0.19', '-0.62', '-0.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -39.57\n",
      "['+0.04', '+0.56', '+1.12', '-0.19', '-0.91', '-0.46', '+0.00', '+0.00']\n",
      "step 80 total_reward -136.58\n",
      "['+0.27', '+0.41', '+1.17', '-0.85', '-1.89', '-1.46', '+0.00', '+0.00']\n",
      "step 100 total_reward -259.63\n",
      "['+0.51', '+0.06', '+1.01', '-1.42', '-3.80', '-2.26', '+0.00', '+0.00']\n",
      "step 120 total_reward -482.36\n",
      "['+0.57', '-0.06', '+0.87', '-0.10', '-4.38', '-0.45', '+0.00', '+0.00']\n",
      "step 126 total_reward -644.64\n",
      "['+0.00', '+0.95', '+0.19', '+0.35', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.20\n",
      "['+0.03', '+0.97', '+0.13', '-0.18', '+0.22', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.06\n",
      "['+0.04', '+0.83', '-0.02', '-0.76', '+0.89', '+0.93', '+0.00', '+0.00']\n",
      "step 40 total_reward -111.68\n",
      "['+0.02', '+0.55', '-0.31', '-1.13', '+1.43', '+0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -180.19\n",
      "['-0.16', '+0.14', '-1.33', '-1.58', '+1.54', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -252.57\n",
      "['-0.25', '+0.00', '-1.25', '+0.20', '+1.87', '+1.68', '+0.00', '+1.00']\n",
      "step 87 total_reward -330.06\n",
      "['+0.02', '+0.95', '+0.81', '+0.19', '-0.02', '-0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.58\n",
      "['+0.19', '+0.92', '+1.00', '-0.38', '-0.68', '-1.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -90.58\n",
      "['+0.40', '+0.72', '+1.04', '-1.02', '-2.20', '-1.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -270.30\n",
      "['+0.61', '+0.32', '+0.89', '-1.53', '-4.57', '-2.76', '+0.00', '+0.00']\n",
      "step 60 total_reward -525.37\n",
      "['+0.75', '-0.08', '+0.99', '-0.37', '-6.72', '+3.03', '+0.00', '+0.00']\n",
      "step 76 total_reward -877.68\n",
      "['+0.02', '+0.94', '+0.78', '-0.17', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.00\n",
      "['+0.15', '+0.80', '+0.57', '-0.71', '+0.27', '+0.67', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.74\n",
      "['+0.29', '+0.51', '+0.75', '-1.18', '+0.50', '-0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -76.14\n",
      "['+0.45', '+0.08', '+0.90', '-1.68', '-0.07', '-0.82', '+0.00', '+0.00']\n",
      "step 60 total_reward -71.20\n",
      "['+0.48', '+0.00', '+1.13', '-1.30', '-0.27', '-5.62', '+0.00', '+1.00']\n",
      "step 63 total_reward -175.25\n",
      "['-0.01', '+0.95', '-0.38', '+0.43', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.09', '+1.00', '-0.49', '-0.11', '+0.37', '+0.55', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.45\n",
      "['-0.19', '+0.88', '-0.60', '-0.72', '+1.25', '+1.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -155.90\n",
      "['-0.35', '+0.56', '-0.85', '-1.47', '+2.90', '+1.90', '+0.00', '+0.00']\n",
      "step 60 total_reward -374.69\n",
      "['-0.51', '+0.03', '-0.77', '-1.99', '+5.11', '+2.17', '+0.00', '+0.00']\n",
      "step 80 total_reward -625.01\n",
      "['-0.70', '-0.12', '-0.97', '-0.43', '+4.09', '-1.66', '+0.00', '+0.00']\n",
      "step 100 total_reward -435.44\n",
      "['-0.71', '-0.12', '-0.90', '+0.01', '+4.01', '-0.60', '+0.00', '+0.00']\n",
      "step 101 total_reward -535.44\n",
      "['+0.00', '+0.93', '+0.21', '-0.38', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.19\n",
      "['+0.03', '+0.73', '+0.01', '-0.85', '+0.39', '+0.63', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.05\n",
      "['-0.05', '+0.50', '-0.86', '-0.73', '+0.99', '+0.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.61\n",
      "['-0.28', '+0.23', '-1.51', '-1.25', '+1.88', '+1.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -294.85\n",
      "['-0.47', '-0.01', '-1.89', '-0.59', '+2.35', '-1.93', '+0.00', '+1.00']\n",
      "step 72 total_reward -470.10\n",
      "['+0.00', '+0.94', '+0.13', '-0.17', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.90\n",
      "['+0.01', '+0.80', '-0.07', '-0.72', '+0.45', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.16\n",
      "['-0.07', '+0.57', '-0.97', '-0.84', '+1.33', '+0.88', '+0.00', '+0.00']\n",
      "step 40 total_reward -210.98\n",
      "['-0.36', '+0.24', '-1.59', '-1.46', '+2.45', '+1.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -398.30\n",
      "['-0.54', '-0.02', '-1.05', '+0.08', '+3.02', '-0.00', '+0.00', '+0.00']\n",
      "step 71 total_reward -587.17\n",
      "['-0.01', '+0.95', '-0.29', '+0.13', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.61\n",
      "['-0.08', '+0.90', '-0.50', '-0.43', '+0.56', '+0.96', '+0.00', '+0.00']\n",
      "step 20 total_reward -85.72\n",
      "['-0.20', '+0.68', '-0.96', '-1.07', '+1.93', '+1.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -283.66\n",
      "['-0.46', '+0.25', '-1.18', '-1.72', '+3.72', '+2.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -511.72\n",
      "['-0.58', '+0.00', '-1.00', '-0.08', '+4.32', '+1.37', '+0.00', '+0.00']\n",
      "step 70 total_reward -622.96\n",
      "['+0.01', '+0.93', '+0.28', '-0.53', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.34\n",
      "['+0.08', '+0.68', '+0.47', '-1.09', '-0.51', '-0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -87.23\n",
      "['+0.18', '+0.27', '+0.54', '-1.73', '-1.82', '-1.77', '+0.00', '+0.00']\n",
      "step 40 total_reward -245.76\n",
      "['+0.24', '+0.00', '+1.35', '-0.40', '-2.87', '-3.88', '+0.00', '+0.00']\n",
      "step 50 total_reward -452.08\n",
      "['-0.00', '+0.95', '-0.17', '+0.21', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.73\n",
      "['-0.05', '+0.93', '-0.36', '-0.34', '+0.48', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.69\n",
      "['-0.15', '+0.74', '-0.80', '-0.97', '+1.73', '+1.40', '+0.00', '+0.00']\n",
      "step 40 total_reward -252.94\n",
      "['-0.41', '+0.32', '-1.32', '-1.75', '+3.21', '+1.78', '+0.00', '+0.00']\n",
      "step 60 total_reward -475.47\n",
      "['-0.54', '+0.08', '-1.49', '-0.12', '+4.00', '+1.16', '+0.00', '+0.00']\n",
      "step 69 total_reward -667.66\n",
      "['+0.01', '+0.94', '+0.57', '+0.05', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.53\n",
      "['+0.11', '+0.87', '+0.37', '-0.50', '+0.30', '+0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.44\n",
      "['+0.18', '+0.64', '+0.38', '-0.99', '+1.07', '+0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -128.80\n",
      "['+0.27', '+0.28', '+0.47', '-1.40', '+1.02', '-0.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -137.61\n",
      "['+0.30', '+0.01', '+0.17', '-0.38', '+0.69', '-4.39', '+0.00', '+1.00']\n",
      "step 72 total_reward -204.42\n",
      "['-0.00', '+0.93', '-0.02', '-0.49', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.86\n",
      "['+0.01', '+0.70', '+0.17', '-1.04', '-0.46', '-0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.23\n",
      "['+0.05', '+0.30', '+0.27', '-1.65', '-1.72', '-1.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -231.91\n",
      "['+0.11', '+0.01', '+0.81', '-0.46', '-2.81', '-6.46', '+0.00', '+0.00']\n",
      "step 51 total_reward -457.19\n",
      "['+0.01', '+0.92', '+0.50', '-0.55', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.87\n",
      "['+0.10', '+0.67', '+0.51', '-1.08', '+0.08', '-0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.47\n",
      "['+0.22', '+0.26', '+0.70', '-1.64', '-0.44', '-0.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -90.48\n",
      "['+0.37', '-0.03', '+0.99', '-0.22', '-0.74', '+1.91', '+0.00', '+0.00']\n",
      "step 57 total_reward -160.23\n",
      "['-0.01', '+0.93', '-0.30', '-0.44', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.45\n",
      "['-0.09', '+0.72', '-0.63', '-0.81', '+0.51', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.25\n",
      "['-0.26', '+0.48', '-1.00', '-1.04', '+1.42', '+1.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -200.01\n",
      "['-0.47', '+0.08', '-1.01', '-1.63', '+3.10', '+1.64', '+0.00', '+0.00']\n",
      "step 60 total_reward -409.74\n",
      "['-0.53', '-0.08', '-0.48', '-0.06', '+3.55', '-1.71', '+0.00', '+0.00']\n",
      "step 66 total_reward -566.94\n",
      "['-0.01', '+0.95', '-0.72', '+0.30', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.17\n",
      "['-0.17', '+0.95', '-0.91', '-0.27', '+0.63', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.91\n",
      "['-0.36', '+0.78', '-1.12', '-0.94', '+2.12', '+1.90', '+0.00', '+0.00']\n",
      "step 40 total_reward -272.57\n",
      "['-0.65', '+0.37', '-1.29', '-1.64', '+4.19', '+2.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -533.39\n",
      "['-0.88', '-0.17', '-0.87', '-0.31', '+6.69', '-3.74', '+0.00', '+1.00']\n",
      "step 79 total_reward -925.45\n",
      "['-0.01', '+0.93', '-0.70', '-0.46', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.48\n",
      "['-0.17', '+0.71', '-0.97', '-0.85', '+0.60', '+0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -88.50\n",
      "['-0.40', '+0.42', '-1.24', '-1.29', '+1.75', '+1.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -239.72\n",
      "['-0.66', '-0.05', '-1.22', '-1.85', '+3.61', '+1.86', '+0.00', '+0.00']\n",
      "step 60 total_reward -477.48\n",
      "['-0.67', '-0.07', '-1.44', '-0.66', '+3.52', '-4.08', '+0.00', '+0.00']\n",
      "step 61 total_reward -577.48\n",
      "['+0.01', '+0.93', '+0.69', '-0.27', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.05\n",
      "['+0.14', '+0.77', '+0.61', '-0.80', '+0.21', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.13\n",
      "['+0.28', '+0.45', '+0.81', '-1.31', '-0.02', '-0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.22\n",
      "['+0.45', '-0.03', '+0.93', '-1.69', '-1.01', '-1.00', '+1.00', '+0.00']\n",
      "step 60 total_reward -160.90\n",
      "['+0.46', '-0.05', '+1.45', '-0.45', '-1.02', '+1.69', '+1.00', '+0.00']\n",
      "step 61 total_reward -260.90\n",
      "['+0.00', '+0.95', '+0.04', '+0.18', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.39\n",
      "['-0.01', '+0.92', '-0.15', '-0.37', '+0.45', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.19\n",
      "['-0.06', '+0.72', '-0.63', '-0.92', '+1.67', '+1.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -238.30\n",
      "['-0.31', '+0.32', '-1.33', '-1.72', '+3.21', '+1.81', '+0.00', '+0.00']\n",
      "step 60 total_reward -474.02\n",
      "['-0.47', '+0.02', '-1.37', '-0.14', '+4.27', '+4.55', '+1.00', '+0.00']\n",
      "step 71 total_reward -688.42\n",
      "['-0.01', '+0.94', '-0.61', '-0.12', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.20\n",
      "['-0.15', '+0.82', '-0.79', '-0.69', '+0.60', '+0.97', '+0.00', '+0.00']\n",
      "step 20 total_reward -93.75\n",
      "['-0.37', '+0.55', '-1.35', '-1.16', '+1.86', '+1.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -277.53\n",
      "['-0.64', '+0.11', '-1.22', '-1.74', '+3.86', '+2.24', '+0.00', '+0.00']\n",
      "step 60 total_reward -510.99\n",
      "['-0.68', '+0.04', '-1.78', '-0.39', '+4.16', '+1.27', '+0.00', '+0.00']\n",
      "step 63 total_reward -639.57\n",
      "['-0.00', '+0.93', '-0.04', '-0.24', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.53\n",
      "['-0.03', '+0.78', '-0.23', '-0.79', '+0.45', '+0.81', '+0.00', '+0.00']\n",
      "step 20 total_reward -90.42\n",
      "['-0.15', '+0.56', '-1.12', '-0.77', '+1.28', '+0.83', '+0.00', '+0.00']\n",
      "step 40 total_reward -212.87\n",
      "['-0.40', '+0.25', '-1.20', '-1.38', '+2.55', '+1.64', '+0.00', '+0.00']\n",
      "step 60 total_reward -377.25\n",
      "['-0.54', '+0.01', '-1.28', '-0.08', '+3.24', '+0.00', '+0.00', '+0.00']\n",
      "step 71 total_reward -577.24\n",
      "['+0.01', '+0.94', '+0.53', '+0.12', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.10\n",
      "['+0.10', '+0.90', '+0.33', '-0.43', '+0.34', '+0.73', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.69\n",
      "['+0.17', '+0.69', '+0.39', '-0.91', '+0.99', '+0.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -119.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.26', '+0.35', '+0.50', '-1.33', '+0.76', '-0.66', '+0.00', '+0.00']\n",
      "step 60 total_reward -112.96\n",
      "['+0.33', '+0.03', '+0.02', '-0.89', '+0.18', '+5.51', '+1.00', '+0.00']\n",
      "step 74 total_reward -164.37\n",
      "['+0.00', '+0.95', '+0.24', '+0.39', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.13\n",
      "['+0.04', '+0.99', '+0.09', '-0.15', '+0.32', '+0.62', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.82\n",
      "['+0.05', '+0.85', '-0.04', '-0.77', '+1.37', '+1.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -156.98\n",
      "['-0.02', '+0.50', '-0.62', '-1.75', '+2.99', '+1.62', '+0.00', '+0.00']\n",
      "step 60 total_reward -396.61\n",
      "['-0.11', '+0.00', '-0.66', '-0.29', '+4.34', '+5.62', '+1.00', '+0.00']\n",
      "step 75 total_reward -658.77\n",
      "['-0.01', '+0.93', '-0.49', '-0.38', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.57\n",
      "['-0.13', '+0.73', '-0.75', '-0.84', '+0.58', '+0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -92.72\n",
      "['-0.34', '+0.45', '-1.21', '-1.21', '+1.51', '+1.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -229.38\n",
      "['-0.59', '+0.01', '-1.23', '-1.78', '+3.09', '+1.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -434.77\n",
      "['-0.66', '-0.11', '-1.51', '-0.22', '+3.35', '-0.00', '+0.00', '+0.00']\n",
      "step 65 total_reward -579.58\n",
      "['+0.00', '+0.93', '+0.11', '-0.20', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.51\n",
      "['+0.01', '+0.79', '-0.09', '-0.75', '+0.42', '+0.81', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.92\n",
      "['-0.08', '+0.55', '-0.99', '-0.88', '+1.30', '+0.86', '+0.00', '+0.00']\n",
      "step 40 total_reward -208.93\n",
      "['-0.33', '+0.20', '-1.38', '-1.55', '+2.54', '+1.49', '+0.00', '+0.00']\n",
      "step 60 total_reward -394.21\n",
      "['-0.51', '-0.10', '-1.50', '-0.12', '+3.27', '-0.00', '+0.00', '+0.00']\n",
      "step 72 total_reward -599.74\n",
      "['+0.00', '+0.92', '+0.06', '-0.54', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.64\n",
      "['-0.00', '+0.69', '-0.16', '-0.82', '+0.37', '+0.52', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.89\n",
      "['-0.11', '+0.49', '-0.98', '-0.60', '+0.89', '+0.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -116.30\n",
      "['-0.32', '+0.22', '-1.34', '-1.22', '+1.89', '+1.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -273.09\n",
      "['-0.48', '+0.00', '-0.75', '-0.36', '+2.38', '+5.65', '+0.00', '+0.00']\n",
      "step 71 total_reward -440.43\n",
      "['-0.00', '+0.95', '-0.01', '+0.28', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.90\n",
      "['-0.02', '+0.95', '-0.20', '-0.28', '+0.46', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.84\n",
      "['-0.07', '+0.77', '-0.40', '-0.93', '+1.77', '+1.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -232.75\n",
      "['-0.25', '+0.36', '-0.96', '-1.79', '+3.55', '+2.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -483.40\n",
      "['-0.37', '-0.03', '-0.96', '-0.01', '+4.69', '+1.28', '+1.00', '+0.00']\n",
      "step 73 total_reward -722.37\n",
      "['+0.01', '+0.95', '+0.44', '+0.29', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.31\n",
      "['+0.08', '+0.95', '+0.27', '-0.26', '+0.27', '+0.62', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.77\n",
      "['+0.13', '+0.79', '+0.21', '-0.79', '+1.19', '+0.80', '+0.00', '+0.00']\n",
      "step 40 total_reward -132.17\n",
      "['+0.18', '+0.49', '+0.24', '-1.20', '+1.49', '-0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -175.16\n",
      "['+0.20', '+0.07', '-0.13', '-1.48', '+0.88', '-0.74', '+0.00', '+0.00']\n",
      "step 80 total_reward -112.34\n",
      "['+0.20', '+0.00', '+0.08', '-0.06', '+0.69', '-1.73', '+0.00', '+1.00']\n",
      "step 84 total_reward -195.42\n",
      "['+0.02', '+0.95', '+0.78', '+0.28', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.76\n",
      "['+0.16', '+0.95', '+0.68', '-0.25', '+0.08', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward +3.53\n",
      "['+0.29', '+0.79', '+0.63', '-0.78', '+0.65', '+0.43', '+0.00', '+0.00']\n",
      "step 40 total_reward -69.70\n",
      "['+0.43', '+0.49', '+0.77', '-1.23', '+0.63', '-0.42', '+0.00', '+0.00']\n",
      "step 60 total_reward -93.93\n",
      "['+0.59', '+0.04', '+0.92', '-1.72', '-0.20', '-1.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -96.18\n",
      "['+0.64', '-0.08', '+1.48', '-0.36', '-0.35', '+0.00', '+1.00', '+1.00']\n",
      "step 85 total_reward -205.57\n",
      "['+0.01', '+0.94', '+0.63', '+0.12', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.74\n",
      "['+0.12', '+0.90', '+0.44', '-0.43', '+0.28', '+0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.69\n",
      "['+0.21', '+0.69', '+0.48', '-0.92', '+0.96', '+0.33', '+0.00', '+0.00']\n",
      "step 40 total_reward -111.63\n",
      "['+0.32', '+0.35', '+0.60', '-1.34', '+0.78', '-0.63', '+0.00', '+0.00']\n",
      "step 60 total_reward -112.22\n",
      "['+0.44', '-0.08', '+0.76', '-0.08', '-0.08', '+0.30', '+1.00', '+1.00']\n",
      "step 79 total_reward -163.04\n",
      "['-0.01', '+0.94', '-0.64', '-0.10', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.30\n",
      "['-0.16', '+0.82', '-0.83', '-0.66', '+0.63', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -95.51\n",
      "['-0.37', '+0.56', '-1.33', '-1.15', '+1.90', '+1.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -278.54\n",
      "['-0.64', '+0.13', '-1.17', '-1.71', '+3.91', '+2.38', '+0.00', '+0.00']\n",
      "step 60 total_reward -509.59\n",
      "['-0.71', '-0.01', '-0.65', '+0.03', '+4.46', '+0.00', '+1.00', '+0.00']\n",
      "step 66 total_reward -674.98\n",
      "['+0.01', '+0.94', '+0.38', '-0.15', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.02\n",
      "['+0.07', '+0.81', '+0.19', '-0.70', '+0.36', '+0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.09\n",
      "['+0.13', '+0.52', '+0.36', '-1.16', '+0.64', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -105.30\n",
      "['+0.20', '+0.12', '+0.38', '-1.31', '+0.13', '-0.64', '+0.00', '+0.00']\n",
      "step 60 total_reward -41.25\n",
      "['+0.23', '-0.03', '+0.23', '-0.80', '+0.02', '+4.73', '+1.00', '+1.00']\n",
      "step 67 total_reward -129.67\n",
      "['-0.01', '+0.93', '-0.32', '-0.21', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.39\n",
      "['-0.09', '+0.79', '-0.52', '-0.77', '+0.54', '+0.91', '+0.00', '+0.00']\n",
      "step 20 total_reward -95.59\n",
      "['-0.29', '+0.57', '-1.44', '-0.86', '+1.53', '+1.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -258.97\n",
      "['-0.58', '+0.22', '-1.32', '-1.49', '+3.18', '+2.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -453.65\n",
      "['-0.69', '+0.05', '-1.39', '-0.08', '+3.93', '+0.89', '+0.00', '+0.00']\n",
      "step 68 total_reward -646.43\n",
      "['-0.01', '+0.92', '-0.68', '-0.58', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.68\n",
      "['-0.17', '+0.68', '-0.97', '-0.90', '+0.59', '+0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.26\n",
      "['-0.39', '+0.35', '-1.15', '-1.43', '+1.74', '+1.58', '+0.00', '+0.00']\n",
      "step 40 total_reward -231.93\n",
      "['-0.60', '-0.06', '-1.58', '-0.88', '+3.36', '+6.18', '+0.00', '+0.00']\n",
      "step 57 total_reward -505.84\n",
      "['+0.02', '+0.94', '+0.81', '+0.06', '-0.02', '-0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.59\n",
      "['+0.19', '+0.87', '+0.99', '-0.51', '-0.63', '-0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.32\n",
      "['+0.40', '+0.63', '+1.03', '-1.17', '-2.13', '-1.98', '+0.00', '+0.00']\n",
      "step 40 total_reward -268.93\n",
      "['+0.60', '+0.19', '+0.89', '-1.68', '-4.53', '-2.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -532.51\n",
      "['+0.66', '+0.03', '+0.40', '+0.05', '-5.29', '+1.85', '+0.00', '+0.00']\n",
      "step 67 total_reward -722.43\n",
      "['+0.01', '+0.92', '+0.36', '-0.58', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.88\n",
      "['+0.07', '+0.67', '+0.28', '-1.03', '+0.22', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.62\n",
      "['+0.14', '+0.28', '+0.47', '-1.55', '+0.01', '-0.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -34.56\n",
      "['+0.21', '-0.03', '+0.95', '-0.28', '-0.24', '+4.57', '+1.00', '+1.00']\n",
      "step 54 total_reward -156.47\n",
      "['+0.01', '+0.93', '+0.66', '-0.41', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.15\n",
      "['+0.14', '+0.72', '+0.70', '-0.94', '+0.00', '-0.24', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.23\n",
      "['+0.29', '+0.35', '+0.89', '-1.51', '-0.68', '-1.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.96\n",
      "['+0.42', '+0.02', '+1.18', '-0.19', '-2.00', '+0.06', '+1.00', '+0.00']\n",
      "step 54 total_reward -330.63\n",
      "['-0.01', '+0.94', '-0.73', '-0.15', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.14\n",
      "['-0.18', '+0.81', '-0.93', '-0.72', '+0.65', '+1.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -97.00\n",
      "['-0.44', '+0.56', '-1.67', '-1.05', '+1.89', '+1.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -293.04\n",
      "['-0.78', '+0.15', '-1.52', '-1.61', '+3.78', '+2.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -515.53\n",
      "['-0.91', '-0.08', '+0.07', '-0.20', '+4.87', '+3.64', '+1.00', '+0.00']\n",
      "step 69 total_reward -725.98\n",
      "['+0.01', '+0.93', '+0.31', '-0.31', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.05', '+0.75', '+0.11', '-0.79', '+0.36', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -55.10\n",
      "['+0.02', '+0.54', '-0.48', '-0.77', '+0.96', '+0.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -108.45\n",
      "['-0.18', '+0.29', '-1.44', '-1.06', '+1.55', '+0.75', '+0.00', '+0.00']\n",
      "step 60 total_reward -240.31\n",
      "['-0.53', '-0.09', '-2.18', '-0.54', '+2.48', '+1.55', '+0.00', '+0.00']\n",
      "step 80 total_reward -468.81\n",
      "['-0.01', '+0.94', '-0.68', '-0.07', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.79\n",
      "['-0.16', '+0.83', '-0.87', '-0.64', '+0.61', '+0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -92.03\n",
      "['-0.38', '+0.58', '-1.26', '-1.14', '+1.93', '+1.58', '+0.00', '+0.00']\n",
      "step 40 total_reward -272.76\n",
      "['-0.63', '+0.14', '-1.08', '-1.69', '+3.96', '+2.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -503.18\n",
      "['-0.70', '-0.01', '-0.60', '+0.09', '+4.26', '-0.86', '+1.00', '+0.00']\n",
      "step 67 total_reward -572.17\n",
      "['+0.01', '+0.95', '+0.44', '+0.38', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.68\n",
      "['+0.09', '+0.98', '+0.41', '-0.15', '+0.05', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward +8.70\n",
      "['+0.16', '+0.85', '+0.25', '-0.70', '+0.38', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.52\n",
      "['+0.23', '+0.57', '+0.39', '-1.16', '+0.70', '-0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -99.79\n",
      "['+0.32', '+0.15', '+0.58', '-1.63', '+0.16', '-0.97', '+0.00', '+0.00']\n",
      "step 80 total_reward -70.78\n",
      "['+0.36', '+0.00', '+0.51', '-0.92', '+0.17', '+6.66', '+1.00', '+1.00']\n",
      "step 86 total_reward -164.96\n",
      "['+0.02', '+0.92', '+0.77', '-0.57', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.23\n",
      "['+0.16', '+0.67', '+0.80', '-1.10', '-0.01', '-0.23', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.03\n",
      "['+0.34', '+0.25', '+0.99', '-1.67', '-0.69', '-1.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -116.59\n",
      "['+0.43', '+0.04', '+1.34', '-0.24', '-2.23', '-1.18', '+1.00', '+0.00']\n",
      "step 50 total_reward -290.38\n",
      "['-0.01', '+0.93', '-0.42', '-0.25', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.29\n",
      "['-0.11', '+0.78', '-0.67', '-0.70', '+0.53', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.01\n",
      "['-0.34', '+0.60', '-1.71', '-0.66', '+1.35', '+0.82', '+0.00', '+0.00']\n",
      "step 40 total_reward -252.71\n",
      "['-0.73', '+0.33', '-1.94', '-1.23', '+2.54', '+1.61', '+0.00', '+0.00']\n",
      "step 60 total_reward -431.98\n",
      "['-1.01', '+0.02', '-1.91', '-1.61', '+3.76', '+1.76', '+0.00', '+0.00']\n",
      "step 74 total_reward -682.38\n",
      "['+0.00', '+0.94', '+0.12', '-0.01', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.14\n",
      "['+0.01', '+0.85', '-0.08', '-0.56', '+0.44', '+0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -78.45\n",
      "['-0.05', '+0.64', '-0.84', '-0.83', '+1.39', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -217.95\n",
      "['-0.32', '+0.31', '-1.40', '-1.42', '+2.50', '+1.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -394.18\n",
      "['-0.51', '+0.01', '-0.19', '-0.00', '+3.37', '-1.06', '+0.00', '+0.00']\n",
      "step 73 total_reward -615.78\n",
      "total rewards [-567.2547877725481, -551.4783974089457, -631.2034458290071, -183.54405120484444, -211.04828348264428, -227.581124426133, -500.3304808459497, -260.8250655736589, -687.5623448972896, -687.9945764499663, -120.13137137011233, -646.9471596389183, -132.7362231001561, -588.4650201462695, -656.4335330874372, -593.2351450938753, -194.53475457796787, -243.58106599430653, -586.6764992679792, -108.07399654845081, -670.307623779265, -156.44618379827145, -681.7483677941294, -665.1541567936142, -166.35106410345153, -546.8360056082101, -805.3981072226122, -635.1568825190075, -200.4367480354561, -268.3053682895493, -650.2571033594209, -123.73890085275295, -269.3177339140476, -304.6801295426891, -341.70527774607586, -139.44936861611126, -616.6984131679579, -671.9211942468202, -362.1624052692809, -566.6802033987362, -187.30687469190548, -663.8191859593302, -142.28427003516552, -674.5179877509909, -204.9083230951134, -146.69383472272693, -70.63809707182278, -229.94274859136846, -650.3246627558809, -819.1522546169846, -267.11770341653846, -945.6038756185764, -173.70753474073348, -226.15756094165954, -154.67792193032128, -497.85651967040695, -704.7425220526641, -640.6377794351831, -248.33282548941835, -610.892192464603, -441.6216105269061, -629.2894094328454, -630.2681251690426, -512.4504901082512, -214.4022990767297, -617.6566968293489, -570.6602171714409, -657.5899012269593, -538.4495519946372, -116.45479841122467, -232.92387522137705, -359.69949865759435, -676.9713076910565, -484.890643474473, -180.31394509744374, -149.76385389481308, -550.8890779152716, -497.3431129742617, -152.67122015730007, -683.9321226542417, -501.56977041410687, -269.69900892399215, -627.8340026213544, -201.3918018115259, -540.555735406409, -463.9018415503909, -241.77315679148174, -285.9351860742851, -242.07026620344288, -195.61008022990805, -637.5805644848784, -280.75693218500226, -516.9897783150867, -194.89390421152706, -529.2077678640342, -537.274824955869, -725.3109812763134, -747.780916820123, -463.7197546257758, -501.85251831549675, -264.1793010458868, -246.09789420643276, -152.5357689811642, -390.51226277214056, -521.0286624218595, -347.2215548582878, -562.0531653705674, -321.08002019035507, -639.0253869894319, -725.8211673723341, -581.9859837007152, -545.298999886805, -463.6659627676193, -588.7369949132308, -536.344661516498, -544.031004559972, -187.9890778451288, -237.4378809536475, -675.528799229374, -545.9223549829107, -677.467546091466, -151.4854683377133, -94.90510015254985, -233.24156388697048, -637.908067837739, -460.6010931201741, -109.04542109716087, -167.21394211210315, -605.2845403461836, -641.0375778001062, -517.9971774084623, -159.88575615875067, -540.456079927986, -157.47175945134654, -717.6519823702937, -628.9281806878549, -664.3302010565716, -873.5634620631753, -759.2121684497247, -728.2818090321047, -90.78442806289017, -454.08022981847097, -545.0062719782089, -524.5780638969404, -657.5207878887136, -264.8787961145655, -202.377533955478, -625.8536974242077, -646.0897610204178, -533.6665200888796, -614.5613948607314, -580.9291674809986, -258.8082257028125, -530.9115021196211, -695.5626204794708, -737.4701325959314, -376.48796224355027, -644.6446997725009, -330.05930296632295, -877.6827142200345, -175.2471390422973, -535.436086607154, -470.0980198999522, -587.1690801418213, -622.9647247821312, -452.08282356383467, -667.6564790305473, -204.4196943619412, -457.18502946235367, -160.2295502544015, -566.9437546089566, -925.4523101910995, -577.4802542142835, -260.9046409026507, -688.417679668326, -639.5690819670372, -577.2427602663804, -164.3706411729064, -658.7688370345209, -579.5778538021407, -599.74013188046, -440.43202752459644, -722.3671520895665, -195.41630801375305, -205.57178515792188, -163.03647805309015, -674.9767795013853, -129.66699096962583, -646.4299933200141, -505.84477274361853, -722.4317739992666, -156.4743216105777, -330.6256106511182, -725.9827703132007, -468.81101148639937, -572.1715654647629, -164.96239082643632, -290.383826010608, -682.3792863076474, -615.7752181729295]\n",
      "average total reward -454.32385020153396\n"
     ]
    }
   ],
   "source": [
    "!python lunar_lander_ml_images_player_model3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_model_reward_comparisons[\"Model 3\"] = '-454.32385020153396'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-04-23 00:36:20.115182: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "['-0.00', '+0.93', '-0.10', '-0.36', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.70\n",
      "['-0.03', '+0.74', '-0.13', '-0.90', '+0.08', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.13\n",
      "['-0.06', '+0.47', '-0.25', '-0.73', '+0.21', '+0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -21.61\n",
      "['-0.15', '+0.31', '-0.65', '-0.41', '+0.34', '+0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -26.42\n",
      "['-0.34', '+0.25', '-1.25', '+0.03', '+0.47', '+0.14', '+0.00', '+0.00']\n",
      "step 80 total_reward -101.28\n",
      "['-0.65', '+0.30', '-1.79', '+0.10', '+0.64', '+0.36', '+0.00', '+0.00']\n",
      "step 100 total_reward -208.07\n",
      "['-1.00', '+0.24', '-1.88', '-0.48', '+1.37', '+0.97', '+0.00', '+0.00']\n",
      "step 119 total_reward -419.51\n",
      "['-0.01', '+0.93', '-0.66', '-0.44', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.48\n",
      "['-0.15', '+0.71', '-0.75', '-0.98', '+0.49', '+0.53', '+0.00', '+0.00']\n",
      "step 20 total_reward -73.76\n",
      "['-0.31', '+0.33', '-1.04', '-1.55', '+1.42', '+1.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -204.45\n",
      "['-0.53', '-0.05', '-1.91', '-0.44', '+2.27', '-1.93', '+0.00', '+1.00']\n",
      "step 55 total_reward -468.74\n",
      "['+0.01', '+0.93', '+0.66', '-0.41', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.60\n",
      "['+0.14', '+0.72', '+0.59', '-0.94', '+0.10', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -23.40\n",
      "['+0.25', '+0.46', '+0.60', '-0.99', '+0.20', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -20.63\n",
      "['+0.35', '+0.22', '+0.42', '-0.60', '+0.05', '-0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward +42.51\n",
      "['+0.41', '+0.07', '+0.36', '-0.03', '-0.10', '-1.10', '+1.00', '+0.00']\n",
      "step 75 total_reward -78.81\n",
      "['-0.01', '+0.93', '-0.55', '-0.34', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.21\n",
      "['-0.13', '+0.75', '-0.63', '-0.87', '+0.44', '+0.49', '+0.00', '+0.00']\n",
      "step 20 total_reward -71.87\n",
      "['-0.26', '+0.40', '-0.73', '-1.48', '+1.23', '+1.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -180.15\n",
      "['-0.51', '-0.09', '-1.48', '-0.07', '+2.43', '+1.61', '+0.00', '+1.00']\n",
      "step 60 total_reward -482.17\n",
      "['+0.00', '+0.94', '+0.15', '+0.01', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.96\n",
      "['+0.03', '+0.86', '+0.06', '-0.51', '-0.01', '+0.31', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.28\n",
      "['+0.04', '+0.63', '-0.06', '-0.83', '+0.30', '+0.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -69.28\n",
      "['+0.03', '+0.34', '-0.10', '-1.07', '+0.27', '-0.31', '+0.00', '+0.00']\n",
      "step 60 total_reward -63.34\n",
      "['-0.02', '+0.06', '-0.22', '-1.02', '-0.13', '-0.60', '+0.00', '+0.00']\n",
      "step 80 total_reward -22.90\n",
      "['-0.03', '-0.02', '-0.29', '-0.57', '-0.17', '+3.80', '+1.00', '+0.00']\n",
      "step 85 total_reward -127.05\n",
      "['-0.01', '+0.94', '-0.64', '-0.01', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.05\n",
      "['-0.15', '+0.85', '-0.81', '-0.58', '+0.60', '+0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.36\n",
      "['-0.31', '+0.60', '-0.84', '-1.18', '+1.69', '+1.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -224.91\n",
      "['-0.49', '+0.13', '-0.78', '-2.05', '+3.58', '+2.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -473.93\n",
      "['-0.50', '+0.11', '-0.78', '-0.33', '+3.64', '-3.51', '+0.00', '+0.00']\n",
      "step 61 total_reward -573.93\n",
      "['+0.01', '+0.94', '+0.62', '-0.10', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.40\n",
      "['+0.12', '+0.82', '+0.47', '-0.64', '+0.28', '+0.48', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.12\n",
      "['+0.21', '+0.56', '+0.45', '-1.04', '+0.64', '+0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -81.11\n",
      "['+0.31', '+0.20', '+0.33', '-1.22', '+0.33', '-0.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -42.19\n",
      "['+0.33', '+0.05', '-0.31', '-0.77', '+0.39', '+4.27', '+0.00', '+1.00']\n",
      "step 68 total_reward -131.22\n",
      "['+0.01', '+0.95', '+0.60', '+0.28', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.01\n",
      "['+0.12', '+0.95', '+0.41', '-0.27', '+0.30', '+0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.09\n",
      "['+0.19', '+0.79', '+0.34', '-0.83', '+1.28', '+1.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -134.89\n",
      "['+0.18', '+0.46', '-0.29', '-1.36', '+2.20', '+0.66', '+0.00', '+0.00']\n",
      "step 60 total_reward -248.14\n",
      "['+0.07', '+0.01', '-1.12', '-0.21', '+2.77', '+3.93', '+0.00', '+0.00']\n",
      "step 78 total_reward -444.48\n",
      "['-0.00', '+0.95', '-0.25', '+0.37', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.71\n",
      "['-0.06', '+0.98', '-0.30', '-0.17', '+0.26', '+0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.89\n",
      "['-0.13', '+0.84', '-0.44', '-0.75', '+0.97', '+0.99', '+0.00', '+0.00']\n",
      "step 40 total_reward -129.45\n",
      "['-0.22', '+0.53', '-0.42', '-1.35', '+2.08', '+1.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -267.44\n",
      "['-0.33', '-0.02', '-0.82', '-0.72', '+3.62', '-5.91', '+0.00', '+0.00']\n",
      "step 80 total_reward -611.71\n",
      "['+0.01', '+0.94', '+0.64', '+0.04', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.53\n",
      "['+0.12', '+0.87', '+0.45', '-0.50', '+0.29', '+0.62', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.19\n",
      "['+0.21', '+0.65', '+0.24', '-0.89', '+0.89', '+0.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -91.10\n",
      "['+0.24', '+0.34', '+0.05', '-1.20', '+1.01', '-0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -106.54\n",
      "['+0.17', '+0.02', '-0.26', '-0.21', '+0.83', '-1.00', '+0.00', '+1.00']\n",
      "step 80 total_reward +30.10\n",
      "['+0.14', '-0.03', '-0.22', '-0.30', '+0.05', '-1.82', '+1.00', '+0.00']\n",
      "step 90 total_reward -11.17\n",
      "['-0.01', '+0.94', '-0.26', '+0.05', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.16\n",
      "['-0.07', '+0.87', '-0.46', '-0.51', '+0.54', '+0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.61\n",
      "['-0.16', '+0.64', '-0.48', '-1.08', '+1.55', '+1.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -219.62\n",
      "['-0.26', '+0.22', '-0.47', '-1.89', '+3.22', '+1.84', '+0.00', '+0.00']\n",
      "step 60 total_reward -433.97\n",
      "['-0.31', '-0.01', '-0.76', '-0.23', '+3.79', '-2.09', '+0.00', '+0.00']\n",
      "step 68 total_reward -621.06\n",
      "['-0.01', '+0.93', '-0.46', '-0.40', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.70\n",
      "['-0.11', '+0.72', '-0.55', '-0.94', '+0.38', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -66.78\n",
      "['-0.23', '+0.35', '-0.76', '-1.52', '+1.17', '+1.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -175.55\n",
      "['-0.36', '+0.08', '+0.07', '-0.03', '+1.83', '-0.25', '+0.00', '+0.00']\n",
      "step 52 total_reward -376.58\n",
      "['-0.01', '+0.95', '-0.35', '+0.44', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.35\n",
      "['-0.08', '+1.00', '-0.41', '-0.10', '+0.27', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.56\n",
      "['-0.17', '+0.89', '-0.54', '-0.69', '+0.93', '+1.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.91\n",
      "['-0.29', '+0.59', '-0.53', '-1.34', '+2.46', '+1.99', '+0.00', '+0.00']\n",
      "step 60 total_reward -304.33\n",
      "['-0.37', '+0.07', '+0.01', '-2.13', '+4.78', '+2.36', '+0.00', '+0.00']\n",
      "step 80 total_reward -579.41\n",
      "['-0.38', '-0.06', '-0.81', '-0.06', '+4.68', '-1.68', '+1.00', '+0.00']\n",
      "step 85 total_reward -613.92\n",
      "['-0.00', '+0.93', '-0.18', '-0.27', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.20\n",
      "['-0.04', '+0.77', '-0.23', '-0.80', '+0.10', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.63\n",
      "['-0.10', '+0.52', '-0.39', '-0.75', '+0.29', '+0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.49\n",
      "['-0.21', '+0.24', '-0.91', '-0.95', '+0.84', '+0.63', '+0.00', '+0.00']\n",
      "step 60 total_reward -129.54\n",
      "['-0.35', '+0.05', '-0.44', '+0.25', '+1.50', '+1.76', '+0.00', '+1.00']\n",
      "step 74 total_reward -260.17\n",
      "['-0.01', '+0.95', '-0.37', '+0.39', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.01\n",
      "['-0.09', '+0.99', '-0.43', '-0.15', '+0.30', '+0.39', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.55\n",
      "['-0.18', '+0.85', '-0.57', '-0.75', '+1.06', '+1.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -138.82\n",
      "['-0.30', '+0.55', '-0.52', '-1.37', '+2.48', '+1.82', '+0.00', '+0.00']\n",
      "step 60 total_reward -308.26\n",
      "['-0.38', '-0.01', '+0.05', '-2.33', '+4.51', '+2.01', '+1.00', '+0.00']\n",
      "step 80 total_reward -566.56\n",
      "['-0.37', '-0.01', '-0.01', '-0.00', '+4.49', '+0.02', '+1.00', '+0.00']\n",
      "step 81 total_reward -666.56\n",
      "['-0.01', '+0.94', '-0.70', '-0.03', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.89\n",
      "['-0.17', '+0.85', '-0.86', '-0.59', '+0.58', '+0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -85.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.34', '+0.59', '-0.90', '-1.20', '+1.63', '+1.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -217.15\n",
      "['-0.53', '+0.12', '-0.94', '-2.04', '+3.46', '+1.99', '+0.00', '+0.00']\n",
      "step 60 total_reward -463.62\n",
      "['-0.60', '-0.10', '-1.50', '-0.25', '+4.15', '+2.28', '+0.00', '+0.00']\n",
      "step 67 total_reward -642.80\n",
      "['+0.01', '+0.94', '+0.75', '+0.01', '-0.02', '-0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.61\n",
      "['+0.18', '+0.86', '+0.94', '-0.56', '-0.62', '-1.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -91.02\n",
      "['+0.37', '+0.60', '+0.98', '-1.21', '-2.12', '-1.97', '+0.00', '+0.00']\n",
      "step 40 total_reward -270.42\n",
      "['+0.57', '+0.15', '+0.83', '-1.73', '-4.50', '-2.76', '+0.00', '+0.00']\n",
      "step 60 total_reward -533.10\n",
      "['+0.60', '+0.05', '-0.05', '-0.05', '-4.78', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -378.40\n",
      "['+0.58', '+0.02', '-0.21', '-0.14', '-4.60', '+0.54', '+0.00', '+1.00']\n",
      "step 100 total_reward -365.96\n",
      "['+0.52', '-0.04', '-0.48', '-0.08', '-3.84', '+0.62', '+0.00', '+0.00']\n",
      "step 118 total_reward -422.81\n",
      "['+0.01', '+0.93', '+0.30', '-0.52', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.97\n",
      "['+0.06', '+0.69', '+0.25', '-1.05', '+0.14', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.57\n",
      "['+0.08', '+0.41', '+0.05', '-0.83', '+0.21', '-0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward +0.86\n",
      "['+0.06', '+0.22', '-0.28', '-0.43', '+0.19', '-0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward +48.26\n",
      "['-0.03', '+0.14', '-0.52', '-0.10', '+0.09', '-0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward +57.95\n",
      "['-0.14', '+0.16', '-0.53', '+0.20', '+0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 100 total_reward +50.02\n",
      "['-0.24', '+0.28', '-0.47', '+0.65', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 120 total_reward +5.03\n",
      "['-0.34', '+0.51', '-0.54', '+0.67', '+0.07', '+0.31', '+0.00', '+0.00']\n",
      "step 140 total_reward -36.34\n",
      "['-0.46', '+0.63', '-0.73', '+0.09', '+0.81', '+1.15', '+0.00', '+0.00']\n",
      "step 160 total_reward -114.31\n",
      "['-0.61', '+0.57', '-0.73', '-0.56', '+2.46', '+2.12', '+0.00', '+0.00']\n",
      "step 180 total_reward -304.20\n",
      "['-0.76', '+0.31', '-0.61', '-1.04', '+5.00', '+2.94', '+0.00', '+0.00']\n",
      "step 200 total_reward -585.34\n",
      "['-0.87', '-0.08', '-0.64', '-1.57', '+8.14', '+3.13', '+0.00', '+0.00']\n",
      "step 220 total_reward -953.84\n",
      "['-0.89', '-0.14', '-0.67', '+0.02', '+8.60', '+2.00', '+0.00', '+0.00']\n",
      "step 223 total_reward -1091.54\n",
      "['-0.02', '+0.92', '-0.80', '-0.58', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.62\n",
      "['-0.18', '+0.66', '-0.86', '-1.12', '+0.43', '+0.48', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.51\n",
      "['-0.38', '+0.25', '-1.34', '-1.57', '+1.31', '+1.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -194.00\n",
      "['-0.49', '+0.07', '-0.58', '-0.11', '+1.54', '-2.97', '+0.00', '+1.00']\n",
      "step 48 total_reward -356.86\n",
      "['-0.01', '+0.93', '-0.42', '-0.31', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.60\n",
      "['-0.10', '+0.75', '-0.52', '-0.85', '+0.44', '+0.51', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.05\n",
      "['-0.21', '+0.41', '-0.62', '-1.46', '+1.23', '+1.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -184.35\n",
      "['-0.41', '-0.03', '-1.82', '-0.76', '+2.19', '-3.36', '+0.00', '+1.00']\n",
      "step 58 total_reward -463.32\n",
      "['+0.01', '+0.93', '+0.68', '-0.34', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.45\n",
      "['+0.14', '+0.74', '+0.59', '-0.87', '+0.15', '+0.23', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.03\n",
      "['+0.26', '+0.43', '+0.72', '-1.26', '+0.06', '-0.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -32.12\n",
      "['+0.42', '+0.07', '+0.83', '-1.30', '-0.52', '-0.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -82.01\n",
      "['+0.44', '+0.02', '+0.94', '-0.68', '-0.46', '+3.89', '+1.00', '+0.00']\n",
      "step 63 total_reward -174.64\n",
      "['+0.00', '+0.95', '+0.18', '+0.39', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.07\n",
      "['+0.03', '+0.99', '+0.13', '-0.14', '+0.17', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward +5.24\n",
      "['+0.05', '+0.86', '-0.01', '-0.71', '+0.71', '+0.83', '+0.00', '+0.00']\n",
      "step 40 total_reward -87.93\n",
      "['+0.03', '+0.57', '-0.46', '-1.16', '+1.54', '+0.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -198.89\n",
      "['-0.18', '+0.12', '-1.55', '-1.99', '+2.30', '+0.66', '+0.00', '+0.00']\n",
      "step 80 total_reward -371.29\n",
      "['-0.26', '-0.02', '-2.16', '-0.96', '+2.30', '-3.41', '+0.00', '+1.00']\n",
      "step 85 total_reward -494.31\n",
      "['-0.00', '+0.94', '-0.07', '+0.03', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.56\n",
      "['-0.03', '+0.87', '-0.26', '-0.52', '+0.46', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.39\n",
      "['-0.08', '+0.63', '-0.54', '-0.98', '+1.29', '+0.82', '+0.00', '+0.00']\n",
      "step 40 total_reward -204.88\n",
      "['-0.23', '+0.26', '-0.86', '-1.69', '+2.51', '+1.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -376.83\n",
      "['-0.31', '+0.03', '-0.35', '-0.82', '+2.98', '-6.71', '+0.00', '+0.00']\n",
      "step 68 total_reward -570.25\n",
      "['+0.01', '+0.95', '+0.59', '+0.34', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.97\n",
      "['+0.12', '+0.97', '+0.54', '-0.19', '+0.07', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward +3.80\n",
      "['+0.22', '+0.83', '+0.45', '-0.73', '+0.44', '+0.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -50.41\n",
      "['+0.26', '+0.60', '-0.25', '-0.75', '+0.92', '+0.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -75.65\n",
      "['+0.17', '+0.33', '-0.85', '-1.00', '+1.12', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -122.40\n",
      "['-0.15', '+0.04', '-2.05', '-0.36', '+1.53', '+3.99', '+0.00', '+1.00']\n",
      "step 100 total_reward -213.78\n",
      "['-0.34', '-0.01', '-1.99', '-0.06', '+3.26', '+0.12', '+0.00', '+0.00']\n",
      "step 109 total_reward -499.10\n",
      "['+0.01', '+0.93', '+0.54', '-0.41', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.61\n",
      "['+0.11', '+0.72', '+0.46', '-0.94', '+0.15', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.70\n",
      "['+0.19', '+0.46', '+0.42', '-1.03', '+0.26', '-0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -29.29\n",
      "['+0.26', '+0.17', '+0.35', '-0.79', '-0.10', '-0.36', '+0.00', '+0.00']\n",
      "step 60 total_reward +24.60\n",
      "['+0.30', '+0.02', '+0.03', '-0.36', '-0.16', '+3.29', '+1.00', '+0.00']\n",
      "step 72 total_reward -87.03\n",
      "['+0.01', '+0.93', '+0.71', '-0.50', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.27\n",
      "['+0.15', '+0.69', '+0.64', '-0.93', '+0.07', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -10.69\n",
      "['+0.28', '+0.40', '+0.76', '-1.19', '-0.02', '-0.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -14.87\n",
      "['+0.45', '+0.07', '+0.33', '-0.54', '-0.47', '-1.03', '+1.00', '+0.00']\n",
      "step 60 total_reward +27.51\n",
      "['+0.46', '+0.04', '+0.03', '-0.23', '-0.18', '+2.64', '+1.00', '+0.00']\n",
      "step 65 total_reward -31.94\n",
      "['+0.01', '+0.94', '+0.59', '-0.08', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.17\n",
      "['+0.14', '+0.83', '+0.79', '-0.65', '-0.63', '-1.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -97.29\n",
      "['+0.31', '+0.54', '+0.83', '-1.31', '-2.16', '-2.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -281.93\n",
      "['+0.45', '+0.12', '-0.40', '-0.05', '-3.88', '+0.67', '+0.00', '+1.00']\n",
      "step 59 total_reward -516.71\n",
      "['-0.02', '+0.92', '-0.81', '-0.57', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.41\n",
      "['-0.18', '+0.67', '-0.86', '-1.10', '+0.41', '+0.42', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.25\n",
      "['-0.38', '+0.26', '-1.36', '-1.55', '+1.23', '+1.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -187.05\n",
      "['-0.52', '+0.06', '-1.15', '+0.10', '+1.75', '-0.11', '+0.00', '+1.00']\n",
      "step 49 total_reward -353.59\n",
      "['+0.01', '+0.93', '+0.70', '-0.36', '-0.02', '-0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.47\n",
      "['+0.14', '+0.74', '+0.63', '-0.89', '+0.02', '+0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.95\n",
      "['+0.28', '+0.42', '+0.78', '-1.27', '-0.18', '-0.52', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.16\n",
      "['+0.46', '+0.03', '+0.98', '-1.22', '-0.79', '-0.51', '+1.00', '+0.00']\n",
      "step 60 total_reward -105.70\n",
      "['+0.48', '+0.01', '+1.14', '-0.31', '-0.68', '+2.36', '+1.00', '+0.00']\n",
      "step 62 total_reward -211.43\n",
      "['+0.00', '+0.94', '+0.11', '-0.11', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.49\n",
      "['+0.01', '+0.82', '-0.02', '-0.65', '+0.38', '+0.56', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.99\n",
      "['-0.01', '+0.57', '-0.44', '-0.89', '+0.96', '+0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -146.33\n",
      "['-0.23', '+0.28', '-1.84', '-1.12', '+1.54', '+0.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -305.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.58', '-0.01', '-2.46', '-0.08', '+2.04', '+0.43', '+0.00', '+1.00']\n",
      "step 75 total_reward -544.77\n",
      "['-0.00', '+0.94', '-0.20', '-0.12', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.32\n",
      "['-0.06', '+0.82', '-0.36', '-0.67', '+0.49', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -91.50\n",
      "['-0.13', '+0.53', '-0.40', '-1.26', '+1.33', '+1.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -205.89\n",
      "['-0.23', '+0.05', '-0.63', '-2.00', '+2.88', '+1.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -408.70\n",
      "['-0.25', '-0.00', '-1.49', '-0.80', '+3.08', '+5.75', '+0.00', '+0.00']\n",
      "step 62 total_reward -519.50\n",
      "['+0.01', '+0.94', '+0.43', '+0.07', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.06\n",
      "['+0.10', '+0.88', '+0.45', '-0.45', '-0.36', '-0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -51.49\n",
      "['+0.19', '+0.66', '+0.50', '-1.01', '-0.63', '-0.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -107.72\n",
      "['+0.30', '+0.27', '+0.60', '-1.65', '-1.55', '-1.36', '+0.00', '+0.00']\n",
      "step 60 total_reward -234.52\n",
      "['+0.38', '-0.05', '+1.21', '-0.39', '-2.45', '+1.52', '+0.00', '+0.00']\n",
      "step 72 total_reward -445.62\n",
      "['+0.00', '+0.94', '+0.22', '-0.06', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.43\n",
      "['+0.03', '+0.84', '+0.06', '-0.61', '+0.38', '+0.61', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.75\n",
      "['+0.03', '+0.59', '-0.26', '-0.96', '+0.98', '+0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -142.02\n",
      "['-0.08', '+0.26', '-1.21', '-1.19', '+1.40', '+0.41', '+0.00', '+0.00']\n",
      "step 60 total_reward -226.74\n",
      "['-0.27', '+0.03', '-0.87', '-0.28', '+1.55', '-4.91', '+0.00', '+1.00']\n",
      "step 72 total_reward -397.68\n",
      "['-0.00', '+0.95', '-0.07', '+0.30', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.33\n",
      "['+0.00', '+0.95', '+0.13', '-0.26', '-0.43', '-0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.97\n",
      "['+0.03', '+0.79', '+0.21', '-0.84', '-1.65', '-1.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -204.32\n",
      "['+0.11', '+0.44', '+0.40', '-1.54', '-3.08', '-1.67', '+0.00', '+0.00']\n",
      "step 60 total_reward -387.14\n",
      "['+0.17', '-0.01', '+0.42', '+0.17', '-4.61', '-2.66', '+0.00', '+1.00']\n",
      "step 76 total_reward -648.13\n",
      "['-0.01', '+0.95', '-0.59', '+0.28', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.11\n",
      "['-0.15', '+0.95', '-0.78', '-0.28', '+0.61', '+0.97', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.22\n",
      "['-0.31', '+0.78', '-0.83', '-0.93', '+2.04', '+1.90', '+0.00', '+0.00']\n",
      "step 40 total_reward -252.35\n",
      "['-0.47', '+0.41', '-0.67', '-1.45', '+4.39', '+2.74', '+0.00', '+0.00']\n",
      "step 60 total_reward -502.38\n",
      "['-0.60', '-0.08', '-1.01', '-0.54', '+7.66', '+5.49', '+0.00', '+1.00']\n",
      "step 80 total_reward -772.68\n",
      "['-0.75', '-0.10', '-1.24', '+0.11', '+10.32', '+4.71', '+1.00', '+0.00']\n",
      "step 90 total_reward -1151.43\n",
      "['-0.00', '+0.93', '-0.07', '-0.33', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.62\n",
      "['-0.03', '+0.75', '-0.16', '-0.87', '+0.33', '+0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -71.79\n",
      "['-0.09', '+0.47', '-0.59', '-0.95', '+0.75', '+0.52', '+0.00', '+0.00']\n",
      "step 40 total_reward -113.85\n",
      "['-0.26', '+0.12', '-1.34', '-1.31', '+1.60', '+0.97', '+0.00', '+0.00']\n",
      "step 60 total_reward -259.27\n",
      "['-0.34', '+0.02', '-1.08', '-0.11', '+1.72', '-2.44', '+0.00', '+1.00']\n",
      "step 66 total_reward -392.11\n",
      "['+0.00', '+0.93', '+0.18', '-0.40', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.30\n",
      "['+0.03', '+0.73', '+0.12', '-0.93', '+0.21', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.23\n",
      "['+0.01', '+0.47', '-0.31', '-0.79', '+0.40', '+0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.65\n",
      "['-0.11', '+0.28', '-0.94', '-0.44', '+0.47', '+0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -56.39\n",
      "['-0.37', '+0.19', '-1.60', '-0.18', '+0.50', '+0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -133.98\n",
      "['-0.72', '+0.10', '-1.76', '-0.53', '+0.53', '+0.03', '+0.00', '+0.00']\n",
      "step 100 total_reward -192.14\n",
      "['-1.02', '-0.10', '-1.82', '-0.88', '+0.44', '+0.04', '+1.00', '+0.00']\n",
      "step 117 total_reward -317.64\n",
      "['-0.02', '+0.94', '-0.77', '+0.11', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.76\n",
      "['-0.18', '+0.89', '-0.96', '-0.46', '+0.65', '+1.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -91.04\n",
      "['-0.38', '+0.67', '-1.00', '-1.11', '+2.16', '+1.98', '+0.00', '+0.00']\n",
      "step 40 total_reward -271.54\n",
      "['-0.58', '+0.24', '-0.85', '-1.62', '+4.56', '+2.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -531.35\n",
      "['-0.64', '+0.11', '-1.39', '-0.25', '+4.82', '-1.26', '+0.00', '+0.00']\n",
      "step 66 total_reward -643.81\n",
      "['+0.01', '+0.94', '+0.59', '-0.10', '-0.02', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.01\n",
      "['+0.14', '+0.82', '+0.74', '-0.65', '-0.57', '-0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -86.56\n",
      "['+0.29', '+0.54', '+0.80', '-1.28', '-1.66', '-1.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -226.14\n",
      "['+0.47', '+0.09', '+0.75', '-0.07', '-3.17', '-0.30', '+0.00', '+0.00']\n",
      "step 59 total_reward -557.37\n",
      "['+0.00', '+0.94', '+0.04', '+0.09', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.61\n",
      "['-0.01', '+0.89', '-0.15', '-0.47', '+0.44', '+0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.66\n",
      "['-0.03', '+0.67', '-0.23', '-0.98', '+1.33', '+0.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -194.31\n",
      "['-0.18', '+0.30', '-0.99', '-1.61', '+2.41', '+1.32', '+0.00', '+0.00']\n",
      "step 60 total_reward -362.51\n",
      "['-0.31', '-0.02', '-1.63', '-1.20', '+3.17', '+7.84', '+0.00', '+0.00']\n",
      "step 71 total_reward -589.86\n",
      "['+0.01', '+0.92', '+0.31', '-0.57', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.89\n",
      "['+0.06', '+0.67', '+0.23', '-1.05', '+0.18', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.14\n",
      "['+0.11', '+0.32', '+0.31', '-1.32', '+0.12', '-0.38', '+0.00', '+0.00']\n",
      "step 40 total_reward -26.50\n",
      "['+0.16', '-0.02', '+0.31', '-1.18', '-0.24', '+0.00', '+1.00', '+0.00']\n",
      "step 60 total_reward -1.31\n",
      "['+0.17', '-0.03', '+0.42', '-0.68', '-0.03', '+3.81', '+1.00', '+1.00']\n",
      "step 61 total_reward -101.31\n",
      "['-0.01', '+0.95', '-0.51', '+0.16', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.35\n",
      "['-0.09', '+0.91', '-0.33', '-0.38', '-0.33', '-0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.31\n",
      "['-0.17', '+0.72', '-0.38', '-0.91', '-0.78', '-0.43', '+0.00', '+0.00']\n",
      "step 40 total_reward -100.74\n",
      "['-0.26', '+0.37', '-0.45', '-1.32', '-0.78', '+0.42', '+0.00', '+0.00']\n",
      "step 60 total_reward -114.91\n",
      "['-0.28', '+0.01', '+0.63', '-0.19', '-0.42', '+0.00', '+1.00', '+1.00']\n",
      "step 78 total_reward -154.21\n",
      "['+0.01', '+0.93', '+0.41', '-0.28', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.15\n",
      "['+0.08', '+0.77', '+0.34', '-0.81', '+0.20', '+0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.34\n",
      "['+0.14', '+0.51', '+0.28', '-0.95', '+0.44', '+0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -55.05\n",
      "['+0.19', '+0.20', '+0.09', '-0.90', '+0.21', '-0.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -1.64\n",
      "['+0.19', '-0.03', '-0.04', '-0.00', '-0.02', '+0.17', '+1.00', '+1.00']\n",
      "step 76 total_reward -77.20\n",
      "['+0.01', '+0.93', '+0.65', '-0.27', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.07\n",
      "['+0.13', '+0.77', '+0.57', '-0.80', '+0.15', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.16\n",
      "['+0.23', '+0.52', '+0.43', '-0.82', '+0.35', '+0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.42\n",
      "['+0.31', '+0.26', '+0.28', '-0.74', '+0.20', '-0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward +17.91\n",
      "['+0.35', '+0.02', '+0.19', '-0.99', '-0.00', '-0.20', '+0.00', '+0.00']\n",
      "step 80 total_reward +18.92\n",
      "['+0.36', '-0.02', '+0.15', '-0.54', '+0.05', '+3.52', '+1.00', '+1.00']\n",
      "step 83 total_reward -78.64\n",
      "['+0.01', '+0.93', '+0.43', '-0.24', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.29\n",
      "['+0.09', '+0.78', '+0.36', '-0.77', '+0.05', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.04\n",
      "['+0.16', '+0.49', '+0.43', '-1.13', '+0.06', '-0.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -39.37\n",
      "['+0.26', '+0.12', '+0.62', '-1.28', '-0.44', '-0.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -78.07\n",
      "['+0.30', '-0.01', '+0.54', '-0.40', '-0.52', '+3.90', '+1.00', '+0.00']\n",
      "step 67 total_reward -187.20\n",
      "['-0.00', '+0.95', '-0.18', '+0.30', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.81\n",
      "['-0.06', '+0.95', '-0.38', '-0.26', '+0.51', '+0.89', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.07\n",
      "['-0.14', '+0.79', '-0.45', '-0.85', '+1.79', '+1.43', '+0.00', '+0.00']\n",
      "step 40 total_reward -224.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.23', '+0.46', '-0.32', '-1.41', '+3.42', '+1.98', '+0.00', '+0.00']\n",
      "step 60 total_reward -408.15\n",
      "['-0.23', '+0.02', '+1.44', '-0.39', '+4.83', '-4.31', '+1.00', '+0.00']\n",
      "step 77 total_reward -695.15\n",
      "['-0.00', '+0.93', '-0.18', '-0.32', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.70\n",
      "['-0.05', '+0.75', '-0.26', '-0.86', '+0.33', '+0.38', '+0.00', '+0.00']\n",
      "step 20 total_reward -70.12\n",
      "['-0.13', '+0.48', '-0.67', '-0.90', '+0.74', '+0.52', '+0.00', '+0.00']\n",
      "step 40 total_reward -111.19\n",
      "['-0.35', '+0.17', '-1.71', '-1.14', '+1.44', '+0.71', '+0.00', '+0.00']\n",
      "step 60 total_reward -269.38\n",
      "['-0.44', '+0.09', '-1.81', '-0.74', '+1.68', '+4.48', '+0.00', '+1.00']\n",
      "step 65 total_reward -396.71\n",
      "['+0.01', '+0.94', '+0.56', '+0.10', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.81\n",
      "['+0.11', '+0.89', '+0.37', '-0.44', '+0.30', '+0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.37\n",
      "['+0.18', '+0.68', '+0.27', '-0.94', '+1.06', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -121.74\n",
      "['+0.21', '+0.34', '+0.05', '-1.33', '+1.39', '-0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -162.01\n",
      "['+0.12', '-0.00', '-0.76', '+0.32', '+1.82', '+2.56', '+0.00', '+1.00']\n",
      "step 77 total_reward -245.79\n",
      "['+0.01', '+0.94', '+0.34', '-0.07', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.69\n",
      "['+0.06', '+0.83', '+0.20', '-0.61', '+0.33', '+0.51', '+0.00', '+0.00']\n",
      "step 20 total_reward -51.51\n",
      "['+0.08', '+0.59', '-0.26', '-0.83', '+0.82', '+0.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -102.93\n",
      "['-0.07', '+0.34', '-1.28', '-0.96', '+1.30', '+0.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -204.04\n",
      "['-0.47', '-0.01', '-2.37', '-1.42', '+1.77', '+0.46', '+0.00', '+0.00']\n",
      "step 80 total_reward -384.25\n",
      "['-0.67', '-0.17', '-2.81', '-0.81', '+2.55', '+1.30', '+0.00', '+1.00']\n",
      "step 88 total_reward -582.51\n",
      "['+0.01', '+0.95', '+0.53', '+0.28', '-0.01', '-0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.91\n",
      "['+0.13', '+0.95', '+0.72', '-0.29', '-0.59', '-0.95', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.03\n",
      "['+0.28', '+0.77', '+0.78', '-0.94', '-2.02', '-1.90', '+0.00', '+0.00']\n",
      "step 40 total_reward -251.53\n",
      "['+0.45', '+0.39', '+0.67', '-1.51', '-4.32', '-2.68', '+0.00', '+0.00']\n",
      "step 60 total_reward -503.45\n",
      "['+0.54', '+0.00', '+0.24', '-0.57', '-6.50', '+4.57', '+1.00', '+0.00']\n",
      "step 75 total_reward -829.17\n",
      "['+0.01', '+0.95', '+0.59', '+0.20', '-0.02', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.13\n",
      "['+0.15', '+0.92', '+0.79', '-0.36', '-0.62', '-0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -85.44\n",
      "['+0.31', '+0.73', '+0.83', '-1.01', '-2.08', '-1.91', '+0.00', '+0.00']\n",
      "step 40 total_reward -261.99\n",
      "['+0.48', '+0.33', '+0.68', '-1.53', '-4.41', '-2.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -511.50\n",
      "['+0.56', '+0.01', '+0.23', '-0.82', '-5.95', '+4.94', '+1.00', '+1.00']\n",
      "step 73 total_reward -805.21\n",
      "['-0.01', '+0.93', '-0.34', '-0.50', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.05\n",
      "['-0.08', '+0.69', '-0.41', '-1.03', '+0.26', '+0.29', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.67\n",
      "['-0.20', '+0.37', '-0.72', '-1.29', '+0.71', '+0.75', '+0.00', '+0.00']\n",
      "step 40 total_reward -111.06\n",
      "['-0.43', '-0.03', '-1.29', '-1.50', '+1.31', '+0.32', '+0.00', '+1.00']\n",
      "step 60 total_reward -214.46\n",
      "['-0.44', '-0.04', '-0.74', '+0.05', '+1.47', '-0.85', '+0.00', '+1.00']\n",
      "step 61 total_reward -314.46\n",
      "['-0.01', '+0.95', '-0.72', '+0.17', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.70\n",
      "['-0.17', '+0.91', '-0.92', '-0.40', '+0.67', '+1.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -91.84\n",
      "['-0.36', '+0.70', '-0.95', '-1.05', '+2.22', '+2.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -275.73\n",
      "['-0.55', '+0.30', '-0.80', '-1.55', '+4.67', '+2.85', '+0.00', '+0.00']\n",
      "step 60 total_reward -537.65\n",
      "['-0.63', '-0.01', '-1.18', '-0.76', '+6.50', '+7.32', '+1.00', '+0.00']\n",
      "step 72 total_reward -822.74\n",
      "['+0.00', '+0.93', '+0.02', '-0.36', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.29\n",
      "['+0.00', '+0.74', '-0.02', '-0.89', '+0.03', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.15\n",
      "['+0.00', '+0.47', '+0.01', '-0.87', '+0.15', '+0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -26.70\n",
      "['-0.00', '+0.24', '-0.11', '-0.58', '+0.16', '+0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward +17.64\n",
      "['-0.04', '+0.09', '-0.13', '-0.64', '+0.06', '-0.37', '+0.00', '+0.00']\n",
      "step 80 total_reward +33.37\n",
      "['-0.05', '-0.02', '-0.15', '-0.50', '-0.08', '+2.99', '+1.00', '+1.00']\n",
      "step 90 total_reward -82.31\n",
      "['+0.01', '+0.93', '+0.55', '-0.27', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.20\n",
      "['+0.11', '+0.77', '+0.45', '-0.80', '+0.21', '+0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.62\n",
      "['+0.19', '+0.49', '+0.46', '-1.10', '+0.35', '-0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -53.46\n",
      "['+0.28', '+0.15', '+0.42', '-1.03', '-0.10', '-0.49', '+0.00', '+0.00']\n",
      "step 60 total_reward -2.71\n",
      "['+0.32', '-0.02', '+0.33', '-0.64', '-0.25', '+4.28', '+1.00', '+0.00']\n",
      "step 70 total_reward -128.15\n",
      "['-0.01', '+0.95', '-0.45', '+0.30', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.18\n",
      "['-0.11', '+0.96', '-0.64', '-0.26', '+0.56', '+0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -71.40\n",
      "['-0.25', '+0.79', '-0.71', '-0.91', '+1.99', '+1.91', '+0.00', '+0.00']\n",
      "step 40 total_reward -248.62\n",
      "['-0.39', '+0.43', '-0.55', '-1.44', '+4.33', '+2.73', '+0.00', '+0.00']\n",
      "step 60 total_reward -496.85\n",
      "['-0.43', '+0.05', '+0.63', '-0.31', '+6.51', '-2.18', '+0.00', '+1.00']\n",
      "step 76 total_reward -801.39\n",
      "['-0.01', '+0.94', '-0.28', '+0.01', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.89\n",
      "['-0.04', '+0.86', '-0.10', '-0.54', '-0.38', '-0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.16\n",
      "['-0.06', '+0.62', '+0.13', '-0.95', '-1.09', '-0.73', '+0.00', '+0.00']\n",
      "step 40 total_reward -145.07\n",
      "['+0.11', '+0.30', '+1.63', '-1.36', '-1.82', '-0.73', '+0.00', '+0.00']\n",
      "step 60 total_reward -309.58\n",
      "['+0.33', '+0.04', '+1.32', '+0.12', '-2.22', '-4.00', '+0.00', '+0.00']\n",
      "step 71 total_reward -509.38\n",
      "['+0.00', '+0.94', '+0.09', '+0.13', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.64\n",
      "['+0.00', '+0.90', '-0.12', '-0.43', '+0.46', '+0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -68.22\n",
      "['-0.02', '+0.69', '-0.15', '-0.98', '+1.49', '+1.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -205.30\n",
      "['-0.15', '+0.29', '-0.95', '-1.79', '+2.60', '+1.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -387.72\n",
      "['-0.25', '+0.01', '-0.63', '-0.91', '+3.14', '-7.46', '+0.00', '+0.00']\n",
      "step 69 total_reward -586.23\n",
      "['+0.01', '+0.94', '+0.38', '-0.14', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.12\n",
      "['+0.09', '+0.81', '+0.41', '-0.67', '-0.26', '-0.24', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.66\n",
      "['+0.18', '+0.57', '+0.65', '-0.88', '-0.54', '-0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -92.85\n",
      "['+0.35', '+0.23', '+1.27', '-1.33', '-1.38', '-0.97', '+0.00', '+0.00']\n",
      "step 60 total_reward -237.04\n",
      "['+0.44', '+0.12', '+1.02', '-0.15', '-1.56', '+3.48', '+1.00', '+0.00']\n",
      "step 66 total_reward -390.12\n",
      "['-0.01', '+0.93', '-0.74', '-0.52', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.36\n",
      "['-0.17', '+0.69', '-0.81', '-1.06', '+0.43', '+0.48', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.95\n",
      "['-0.35', '+0.29', '-1.29', '-1.57', '+1.34', '+1.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -201.56\n",
      "['-0.56', '-0.01', '-0.79', '+0.16', '+2.00', '+0.78', '+0.00', '+0.00']\n",
      "step 52 total_reward -437.78\n",
      "['-0.02', '+0.93', '-0.78', '-0.52', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.20\n",
      "['-0.18', '+0.69', '-0.85', '-1.06', '+0.42', '+0.50', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.51\n",
      "['-0.37', '+0.28', '-1.23', '-1.59', '+1.40', '+1.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -202.62\n",
      "['-0.57', '-0.01', '-1.70', '-0.20', '+2.04', '-2.48', '+0.00', '+1.00']\n",
      "step 52 total_reward -448.25\n",
      "['+0.01', '+0.93', '+0.49', '-0.44', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.69\n",
      "['+0.10', '+0.71', '+0.42', '-0.97', '+0.14', '+0.20', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.68\n",
      "['+0.17', '+0.42', '+0.41', '-1.18', '+0.16', '-0.31', '+0.00', '+0.00']\n",
      "step 40 total_reward -29.58\n",
      "['+0.26', '+0.09', '+0.46', '-1.11', '-0.20', '-0.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -15.13\n",
      "['+0.28', '+0.00', '+0.11', '-0.47', '-0.20', '+4.06', '+1.00', '+0.00']\n",
      "step 65 total_reward -113.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.00', '+0.93', '+0.21', '-0.23', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.79\n",
      "['+0.04', '+0.78', '+0.13', '-0.76', '+0.12', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.34\n",
      "['+0.06', '+0.51', '+0.09', '-0.98', '+0.29', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -59.03\n",
      "['+0.08', '+0.19', '+0.10', '-1.02', '-0.03', '-0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -8.71\n",
      "['+0.09', '-0.02', '+0.00', '-0.70', '-0.11', '+4.54', '+1.00', '+1.00']\n",
      "step 72 total_reward -131.34\n",
      "['+0.00', '+0.94', '+0.10', '+0.09', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.49\n",
      "['+0.01', '+0.89', '-0.09', '-0.46', '+0.44', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -70.16\n",
      "['-0.01', '+0.67', '-0.23', '-0.98', '+1.27', '+0.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -184.72\n",
      "['-0.20', '+0.31', '-1.56', '-1.59', '+2.11', '+0.82', '+0.00', '+0.00']\n",
      "step 60 total_reward -367.35\n",
      "['-0.47', '-0.08', '-1.79', '-0.63', '+2.93', '+8.32', '+0.00', '+0.00']\n",
      "step 74 total_reward -610.30\n",
      "['-0.02', '+0.93', '-0.78', '-0.52', '+0.02', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.37\n",
      "['-0.16', '+0.69', '-0.71', '-1.05', '-0.08', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.00\n",
      "['-0.32', '+0.30', '-0.89', '-1.36', '+0.19', '+0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.10\n",
      "['-0.44', '+0.05', '-0.25', '-0.14', '+0.30', '-2.82', '+0.00', '+1.00']\n",
      "step 54 total_reward -131.92\n",
      "['+0.01', '+0.93', '+0.60', '-0.52', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.46\n",
      "['+0.12', '+0.69', '+0.52', '-0.96', '+0.10', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.24\n",
      "['+0.23', '+0.37', '+0.66', '-1.30', '-0.01', '-0.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -20.48\n",
      "['+0.37', '+0.05', '+0.03', '-0.31', '-0.63', '-1.67', '+1.00', '+0.00']\n",
      "step 60 total_reward +44.61\n",
      "['+0.34', '-0.01', '-0.25', '-0.04', '+0.11', '-0.15', '+1.00', '+1.00']\n",
      "step 78 total_reward +3.21\n",
      "['+0.00', '+0.94', '+0.10', '-0.06', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.22\n",
      "['+0.01', '+0.84', '-0.06', '-0.61', '+0.42', '+0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -80.83\n",
      "['-0.01', '+0.59', '-0.41', '-0.91', '+1.08', '+0.71', '+0.00', '+0.00']\n",
      "step 40 total_reward -163.87\n",
      "['-0.18', '+0.25', '-1.12', '-1.45', '+1.91', '+1.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -304.95\n",
      "['-0.34', '-0.03', '-1.78', '-0.37', '+2.38', '-1.05', '+0.00', '+1.00']\n",
      "step 72 total_reward -504.61\n",
      "['+0.01', '+0.95', '+0.51', '+0.33', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.23\n",
      "['+0.10', '+0.97', '+0.35', '-0.21', '+0.25', '+0.56', '+0.00', '+0.00']\n",
      "step 20 total_reward -5.21\n",
      "['+0.16', '+0.82', '+0.25', '-0.78', '+1.16', '+1.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.13\n",
      "['+0.14', '+0.50', '-0.44', '-1.36', '+2.11', '+0.72', '+0.00', '+0.00']\n",
      "step 60 total_reward -251.74\n",
      "['-0.00', '+0.01', '-1.44', '-0.25', '+2.67', '+4.48', '+0.00', '+0.00']\n",
      "step 79 total_reward -464.29\n",
      "['+0.01', '+0.95', '+0.58', '+0.22', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.04\n",
      "['+0.11', '+0.93', '+0.38', '-0.32', '+0.33', '+0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.23\n",
      "['+0.19', '+0.75', '+0.35', '-0.87', '+1.19', '+0.87', '+0.00', '+0.00']\n",
      "step 40 total_reward -131.17\n",
      "['+0.15', '+0.44', '-0.38', '-1.27', '+1.90', '+0.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -213.78\n",
      "['-0.01', '+0.00', '-1.10', '-0.16', '+2.02', '-3.09', '+0.00', '+1.00']\n",
      "step 78 total_reward -408.71\n",
      "['+0.01', '+0.94', '+0.73', '-0.11', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.46\n",
      "['+0.14', '+0.82', '+0.60', '-0.65', '+0.24', '+0.41', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.96\n",
      "['+0.27', '+0.55', '+0.69', '-1.10', '+0.47', '-0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -69.65\n",
      "['+0.42', '+0.14', '+0.84', '-1.56', '-0.07', '-0.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -60.65\n",
      "['+0.53', '-0.19', '+1.16', '-1.29', '-0.49', '+4.85', '+1.00', '+1.00']\n",
      "step 73 total_reward -236.54\n",
      "['-0.00', '+0.95', '-0.12', '+0.24', '+0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.53\n",
      "['-0.01', '+0.93', '+0.08', '-0.32', '-0.44', '-0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.00\n",
      "['-0.00', '+0.76', '+0.05', '-0.84', '-1.14', '-0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -151.26\n",
      "['+0.07', '+0.44', '+0.63', '-1.37', '-1.85', '-0.97', '+0.00', '+0.00']\n",
      "step 60 total_reward -259.91\n",
      "['+0.23', '+0.01', '-0.36', '-0.12', '-2.68', '-0.00', '+0.00', '+0.00']\n",
      "step 77 total_reward -522.13\n",
      "['-0.01', '+0.94', '-0.35', '-0.11', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.82\n",
      "['-0.09', '+0.82', '-0.49', '-0.65', '+0.49', '+0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.41\n",
      "['-0.19', '+0.54', '-0.54', '-1.24', '+1.29', '+1.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -193.34\n",
      "['-0.34', '+0.06', '+0.28', '-0.09', '+2.71', '+0.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -522.16\n",
      "['+0.01', '+0.95', '+0.55', '+0.45', '-0.01', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.39\n",
      "['+0.13', '+1.00', '+0.70', '-0.11', '-0.48', '-0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.68\n",
      "['+0.28', '+0.88', '+0.78', '-0.74', '-1.65', '-1.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -198.84\n",
      "['+0.43', '+0.57', '+0.64', '-1.32', '-3.73', '-2.44', '+0.00', '+0.00']\n",
      "step 60 total_reward -425.73\n",
      "['+0.55', '+0.09', '+0.70', '-1.79', '-6.57', '-3.18', '+0.00', '+0.00']\n",
      "step 80 total_reward -740.58\n",
      "['+0.63', '-0.15', '+1.37', '-0.10', '-8.33', '-0.98', '+1.00', '+0.00']\n",
      "step 90 total_reward -999.92\n",
      "['+0.01', '+0.95', '+0.71', '+0.37', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.20\n",
      "['+0.15', '+0.98', '+0.67', '-0.16', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward +8.10\n",
      "['+0.28', '+0.84', '+0.66', '-0.70', '+0.08', '+0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -15.96\n",
      "['+0.40', '+0.64', '+0.45', '-0.54', '+0.14', '+0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward +13.86\n",
      "['+0.49', '+0.43', '+0.56', '-0.92', '-0.10', '-0.60', '+0.00', '+0.00']\n",
      "step 80 total_reward -11.96\n",
      "['+0.64', '+0.11', '+0.95', '-1.21', '-0.98', '-0.92', '+0.00', '+0.00']\n",
      "step 100 total_reward -146.41\n",
      "['+0.78', '-0.14', '+1.67', '-0.13', '-2.19', '-2.42', '+1.00', '+0.00']\n",
      "step 114 total_reward -357.85\n",
      "['+0.00', '+0.94', '+0.20', '+0.02', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.56\n",
      "['+0.03', '+0.86', '+0.03', '-0.53', '+0.43', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.76\n",
      "['+0.03', '+0.63', '-0.22', '-0.95', '+1.13', '+0.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -159.68\n",
      "['-0.16', '+0.29', '-1.61', '-1.39', '+1.83', '+0.72', '+0.00', '+0.00']\n",
      "step 60 total_reward -320.51\n",
      "['-0.40', '+0.00', '-2.06', '-0.16', '+2.20', '-1.47', '+0.00', '+0.00']\n",
      "step 72 total_reward -535.08\n",
      "['+0.00', '+0.94', '+0.24', '-0.02', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.01\n",
      "['+0.04', '+0.85', '+0.08', '-0.56', '+0.37', '+0.63', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.21\n",
      "['+0.05', '+0.61', '-0.18', '-0.91', '+1.01', '+0.67', '+0.00', '+0.00']\n",
      "step 40 total_reward -138.47\n",
      "['-0.10', '+0.30', '-1.35', '-1.23', '+1.60', '+0.58', '+0.00', '+0.00']\n",
      "step 60 total_reward -262.66\n",
      "['-0.44', '-0.07', '-1.31', '+0.22', '+2.30', '+4.05', '+0.00', '+1.00']\n",
      "step 78 total_reward -514.14\n",
      "['+0.01', '+0.95', '+0.43', '+0.17', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.02\n",
      "['+0.11', '+0.91', '+0.63', '-0.40', '-0.59', '-0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.78\n",
      "['+0.24', '+0.71', '+0.66', '-0.98', '-1.79', '-1.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -232.32\n",
      "['+0.38', '+0.32', '+0.60', '-1.61', '-3.57', '-2.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -440.21\n",
      "['+0.47', '-0.07', '+0.80', '-0.33', '-5.22', '+2.76', '+0.00', '+1.00']\n",
      "step 74 total_reward -710.53\n",
      "['-0.01', '+0.93', '-0.66', '-0.25', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.28\n",
      "['-0.16', '+0.77', '-0.77', '-0.79', '+0.51', '+0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.63\n",
      "['-0.31', '+0.45', '-0.85', '-1.41', '+1.41', '+1.32', '+0.00', '+0.00']\n",
      "step 40 total_reward -197.36\n",
      "['-0.56', '+0.00', '-1.77', '-0.29', '+2.85', '+5.48', '+0.00', '+0.00']\n",
      "step 58 total_reward -519.32\n",
      "['-0.01', '+0.93', '-0.27', '-0.27', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.39\n",
      "['-0.07', '+0.77', '-0.38', '-0.81', '+0.42', '+0.53', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.15', '+0.44', '-0.46', '-1.41', '+1.15', '+1.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -179.48\n",
      "['-0.31', '-0.03', '-1.68', '-0.50', '+2.34', '-1.88', '+0.00', '+1.00']\n",
      "step 59 total_reward -473.25\n",
      "['-0.00', '+0.94', '-0.21', '-0.06', '+0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.66\n",
      "['-0.03', '+0.84', '-0.11', '-0.60', '-0.33', '-0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.33\n",
      "['-0.04', '+0.60', '+0.23', '-0.82', '-0.74', '-0.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -105.26\n",
      "['+0.10', '+0.36', '+0.95', '-0.99', '-1.24', '-0.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -189.96\n",
      "['+0.38', '+0.06', '+0.94', '+0.23', '-2.02', '-2.77', '+0.00', '+0.00']\n",
      "step 78 total_reward -476.61\n",
      "['-0.01', '+0.95', '-0.42', '+0.26', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.41\n",
      "['-0.11', '+0.94', '-0.62', '-0.30', '+0.57', '+0.97', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.30\n",
      "['-0.24', '+0.77', '-0.67', '-0.90', '+1.90', '+1.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -239.00\n",
      "['-0.37', '+0.40', '-0.50', '-1.47', '+3.94', '+2.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -461.23\n",
      "['-0.39', '+0.04', '+0.28', '-0.86', '+5.85', '+7.00', '+1.00', '+0.00']\n",
      "step 74 total_reward -725.32\n",
      "['+0.00', '+0.96', '+0.05', '+0.48', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.49\n",
      "['+0.01', '+1.01', '+0.07', '-0.06', '-0.14', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward +20.47\n",
      "['+0.03', '+0.91', '+0.08', '-0.59', '-0.31', '-0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.38\n",
      "['+0.04', '+0.65', '+0.07', '-1.08', '-0.31', '+0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -60.95\n",
      "['+0.08', '+0.29', '+0.37', '-1.36', '-0.56', '-0.48', '+0.00', '+0.00']\n",
      "step 80 total_reward -84.98\n",
      "['+0.17', '+0.00', '+0.53', '+0.01', '-0.94', '-1.48', '+1.00', '+0.00']\n",
      "step 95 total_reward -214.69\n",
      "['+0.01', '+0.93', '+0.71', '-0.53', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.57\n",
      "['+0.15', '+0.69', '+0.66', '-0.95', '+0.08', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.14\n",
      "['+0.28', '+0.39', '+0.76', '-1.20', '+0.00', '-0.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -11.35\n",
      "['+0.44', '+0.05', '+0.88', '-1.24', '-0.42', '-0.42', '+0.00', '+0.00']\n",
      "step 60 total_reward -63.31\n",
      "['+0.48', '-0.02', '+0.57', '-0.29', '-0.38', '+3.57', '+1.00', '+0.00']\n",
      "step 64 total_reward -152.28\n",
      "['-0.00', '+0.93', '-0.06', '-0.53', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.21\n",
      "['-0.02', '+0.68', '-0.11', '-1.06', '+0.26', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -57.20\n",
      "['-0.09', '+0.41', '-0.63', '-0.81', '+0.51', '+0.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -57.67\n",
      "['-0.29', '+0.20', '-1.41', '-0.66', '+0.81', '+0.30', '+0.00', '+0.00']\n",
      "step 60 total_reward -139.85\n",
      "['-0.60', '+0.09', '-1.48', '-0.37', '+2.59', '+2.75', '+0.00', '+0.00']\n",
      "step 80 total_reward -341.26\n",
      "['-0.63', '+0.08', '-1.40', '-0.31', '+2.90', '+3.84', '+0.00', '+0.00']\n",
      "step 82 total_reward -456.99\n",
      "['-0.01', '+0.95', '-0.64', '+0.33', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.98\n",
      "['-0.13', '+0.97', '-0.46', '-0.21', '-0.25', '-0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.42\n",
      "['-0.22', '+0.82', '-0.44', '-0.74', '-0.99', '-0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -101.35\n",
      "['-0.31', '+0.53', '-0.48', '-1.18', '-1.37', '+0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -157.87\n",
      "['-0.35', '+0.12', '+0.18', '-1.53', '-1.02', '+0.28', '+1.00', '+0.00']\n",
      "step 80 total_reward -117.95\n",
      "['-0.34', '+0.06', '+0.65', '-0.68', '-0.77', '+4.36', '+1.00', '+0.00']\n",
      "step 83 total_reward -203.09\n",
      "['-0.01', '+0.95', '-0.60', '+0.29', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.73\n",
      "['-0.12', '+0.95', '-0.43', '-0.24', '-0.19', '-0.52', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.30\n",
      "['-0.20', '+0.80', '-0.44', '-0.77', '-0.74', '-0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -83.71\n",
      "['-0.30', '+0.50', '-0.54', '-1.22', '-0.87', '+0.31', '+0.00', '+0.00']\n",
      "step 60 total_reward -117.40\n",
      "['-0.37', '+0.10', '-0.18', '-1.37', '-0.31', '+0.45', '+0.00', '+0.00']\n",
      "step 80 total_reward -50.71\n",
      "['-0.38', '-0.00', '+0.08', '-0.01', '-0.20', '+0.04', '+1.00', '+1.00']\n",
      "step 85 total_reward -131.67\n",
      "['-0.00', '+0.93', '-0.18', '-0.32', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.57\n",
      "['-0.05', '+0.75', '-0.27', '-0.86', '+0.36', '+0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -73.46\n",
      "['-0.12', '+0.44', '-0.46', '-1.29', '+0.91', '+0.90', '+0.00', '+0.00']\n",
      "step 40 total_reward -147.21\n",
      "['-0.30', '-0.01', '-1.35', '-1.76', '+1.96', '+0.98', '+0.00', '+1.00']\n",
      "step 60 total_reward -314.98\n",
      "['-0.35', '-0.07', '-2.05', '-0.55', '+2.37', '+1.26', '+0.00', '+1.00']\n",
      "step 63 total_reward -436.51\n",
      "['+0.01', '+0.94', '+0.72', '+0.06', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.80\n",
      "['+0.14', '+0.88', '+0.53', '-0.48', '+0.28', '+0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.84\n",
      "['+0.24', '+0.67', '+0.16', '-0.76', '+0.93', '+0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -75.78\n",
      "['+0.15', '+0.42', '-0.64', '-0.98', '+1.49', '+0.27', '+0.00', '+0.00']\n",
      "step 60 total_reward -148.06\n",
      "['-0.14', '+0.05', '-2.07', '-1.55', '+1.73', '+0.25', '+0.00', '+0.00']\n",
      "step 80 total_reward -289.65\n",
      "['-0.20', '-0.00', '-2.28', '-0.27', '+2.06', '+1.49', '+0.00', '+1.00']\n",
      "step 83 total_reward -391.11\n",
      "['-0.00', '+0.95', '-0.11', '+0.47', '+0.00', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.24\n",
      "['-0.03', '+1.01', '-0.19', '-0.07', '+0.30', '+0.37', '+0.00', '+0.00']\n",
      "step 20 total_reward -6.36\n",
      "['-0.08', '+0.90', '-0.34', '-0.68', '+1.10', '+1.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -132.04\n",
      "['-0.15', '+0.62', '-0.31', '-1.26', '+2.51', '+1.61', '+0.00', '+0.00']\n",
      "step 60 total_reward -300.29\n",
      "['-0.19', '+0.12', '+0.25', '-2.09', '+4.46', '+2.04', '+0.00', '+0.00']\n",
      "step 80 total_reward -538.51\n",
      "['-0.19', '+0.00', '-0.13', '-0.04', '+4.38', '+0.44', '+0.00', '+0.00']\n",
      "step 85 total_reward -538.19\n",
      "['+0.00', '+0.93', '+0.09', '-0.37', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.62\n",
      "['+0.01', '+0.74', '+0.00', '-0.90', '+0.30', '+0.36', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.06\n",
      "['-0.03', '+0.47', '-0.46', '-0.82', '+0.68', '+0.40', '+0.00', '+0.00']\n",
      "step 40 total_reward -84.80\n",
      "['-0.23', '+0.25', '-1.56', '-0.72', '+1.06', '+0.38', '+0.00', '+0.00']\n",
      "step 60 total_reward -194.31\n",
      "['-0.64', '+0.03', '-1.65', '+0.13', '+1.62', '-0.70', '+0.00', '+1.00']\n",
      "step 80 total_reward -405.49\n",
      "['+0.01', '+0.93', '+0.46', '-0.38', '-0.01', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.68\n",
      "['+0.10', '+0.73', '+0.44', '-0.92', '-0.12', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.25\n",
      "['+0.21', '+0.44', '+0.64', '-1.20', '-0.38', '-0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -73.39\n",
      "['+0.37', '+0.09', '+0.27', '+0.16', '-1.66', '-1.10', '+1.00', '+0.00']\n",
      "step 60 total_reward -265.53\n",
      "['+0.01', '+0.93', '+0.42', '-0.35', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.50\n",
      "['+0.08', '+0.74', '+0.33', '-0.88', '+0.21', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.70\n",
      "['+0.13', '+0.46', '+0.27', '-1.08', '+0.37', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -51.28\n",
      "['+0.17', '+0.14', '+0.09', '-0.96', '+0.02', '-0.39', '+0.00', '+0.00']\n",
      "step 60 total_reward +20.70\n",
      "['+0.17', '-0.02', '-0.50', '-0.52', '+0.01', '+3.48', '+1.00', '+1.00']\n",
      "step 70 total_reward -88.51\n",
      "['-0.01', '+0.93', '-0.35', '-0.44', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.80\n",
      "['-0.08', '+0.71', '-0.37', '-0.97', '+0.10', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.13\n",
      "['-0.17', '+0.40', '-0.55', '-1.21', '+0.34', '+0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -64.53\n",
      "['-0.34', '+0.06', '-1.11', '-1.19', '+0.92', '+0.44', '+0.00', '+1.00']\n",
      "step 60 total_reward -137.81\n",
      "['-0.37', '+0.05', '-0.80', '+0.22', '+1.65', '+3.05', '+0.00', '+1.00']\n",
      "step 63 total_reward -234.26\n",
      "['+0.01', '+0.93', '+0.43', '-0.52', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.58\n",
      "['+0.09', '+0.69', '+0.40', '-1.05', '-0.04', '-0.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.58\n",
      "['+0.18', '+0.34', '+0.54', '-1.40', '-0.32', '-0.63', '+0.00', '+0.00']\n",
      "step 40 total_reward -64.16\n",
      "['+0.30', '-0.03', '+1.02', '-0.49', '-0.81', '+3.29', '+1.00', '+0.00']\n",
      "step 58 total_reward -209.07\n",
      "['-0.01', '+0.95', '-0.73', '+0.29', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.17', '+0.95', '-0.92', '-0.28', '+0.63', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.09\n",
      "['-0.37', '+0.78', '-0.97', '-0.94', '+2.16', '+2.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -263.40\n",
      "['-0.56', '+0.41', '-0.83', '-1.44', '+4.58', '+2.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -521.93\n",
      "['-0.68', '+0.00', '-0.57', '+0.04', '+7.15', '+1.60', '+0.00', '+1.00']\n",
      "step 77 total_reward -853.96\n",
      "['+0.01', '+0.96', '+0.31', '+0.48', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.72\n",
      "['+0.06', '+1.01', '+0.27', '-0.06', '+0.09', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward +16.14\n",
      "['+0.11', '+0.91', '+0.17', '-0.61', '+0.30', '+0.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -31.34\n",
      "['+0.14', '+0.65', '-0.04', '-1.05', '+0.92', '+0.56', '+0.00', '+0.00']\n",
      "step 60 total_reward -110.15\n",
      "['+0.12', '+0.28', '-0.29', '-1.38', '+1.03', '-0.20', '+0.00', '+0.00']\n",
      "step 80 total_reward -123.36\n",
      "['+0.00', '+0.05', '-0.68', '-0.11', '+1.89', '+2.14', '+0.00', '+0.00']\n",
      "step 100 total_reward -113.28\n",
      "['-0.10', '-0.00', '-0.62', '-0.05', '+3.24', '-0.34', '+0.00', '+0.00']\n",
      "step 114 total_reward -367.84\n",
      "['-0.00', '+0.94', '-0.01', '+0.07', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.20\n",
      "['-0.02', '+0.88', '-0.19', '-0.49', '+0.44', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.16\n",
      "['-0.05', '+0.65', '-0.26', '-1.00', '+1.23', '+0.79', '+0.00', '+0.00']\n",
      "step 40 total_reward -189.69\n",
      "['-0.18', '+0.28', '-0.79', '-1.61', '+2.26', '+1.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -339.01\n",
      "['-0.27', '+0.01', '-0.26', '+0.04', '+2.92', '+0.00', '+0.00', '+0.00']\n",
      "step 70 total_reward -543.46\n",
      "['+0.01', '+0.95', '+0.52', '+0.19', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.38\n",
      "['+0.10', '+0.92', '+0.32', '-0.35', '+0.35', '+0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.85\n",
      "['+0.16', '+0.73', '+0.28', '-0.89', '+1.28', '+0.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -144.50\n",
      "['+0.14', '+0.41', '-0.21', '-1.31', '+1.89', '+0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -215.26\n",
      "['+0.03', '-0.00', '-0.67', '-0.39', '+1.82', '-4.92', '+0.00', '+1.00']\n",
      "step 77 total_reward -367.78\n",
      "['-0.00', '+0.93', '-0.23', '-0.29', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.15\n",
      "['-0.06', '+0.76', '-0.31', '-0.83', '+0.26', '+0.35', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.34\n",
      "['-0.14', '+0.48', '-0.59', '-1.04', '+0.63', '+0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -105.95\n",
      "['-0.33', '+0.12', '-1.38', '-1.28', '+1.34', '+0.64', '+0.00', '+0.00']\n",
      "step 60 total_reward -234.93\n",
      "['-0.45', '-0.05', '-0.19', '-0.30', '+1.39', '-4.16', '+0.00', '+0.00']\n",
      "step 69 total_reward -371.94\n",
      "['-0.00', '+0.95', '-0.19', '+0.40', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.90\n",
      "['-0.06', '+0.99', '-0.39', '-0.16', '+0.52', '+0.89', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.87\n",
      "['-0.14', '+0.85', '-0.47', '-0.80', '+1.88', '+1.77', '+0.00', '+0.00']\n",
      "step 40 total_reward -227.33\n",
      "['-0.24', '+0.53', '-0.34', '-1.33', '+3.83', '+2.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -438.61\n",
      "['-0.27', '+0.07', '+0.10', '-1.59', '+6.37', '+2.47', '+0.00', '+0.00']\n",
      "step 80 total_reward -687.87\n",
      "['-0.26', '-0.05', '-0.31', '-0.92', '+6.77', '-4.46', '+0.00', '+1.00']\n",
      "step 85 total_reward -836.20\n",
      "['-0.00', '+0.95', '-0.10', '+0.36', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.48\n",
      "['-0.03', '+0.98', '-0.19', '-0.18', '+0.24', '+0.41', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.67\n",
      "['-0.08', '+0.83', '-0.33', '-0.77', '+1.08', '+1.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -142.20\n",
      "['-0.15', '+0.53', '-0.30', '-1.35', '+2.28', '+1.51', '+0.00', '+0.00']\n",
      "step 60 total_reward -287.90\n",
      "['-0.20', '+0.01', '+1.16', '-0.00', '+3.90', '-0.38', '+1.00', '+0.00']\n",
      "step 80 total_reward -602.98\n",
      "['+0.01', '+0.93', '+0.52', '-0.47', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.42\n",
      "['+0.10', '+0.70', '+0.44', '-0.93', '+0.17', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.07\n",
      "['+0.19', '+0.42', '+0.48', '-1.15', '+0.16', '-0.33', '+0.00', '+0.00']\n",
      "step 40 total_reward -26.58\n",
      "['+0.29', '+0.07', '+0.58', '-1.20', '-0.31', '-0.47', '+0.00', '+0.00']\n",
      "step 60 total_reward -36.68\n",
      "['+0.32', '-0.01', '+0.36', '-0.46', '-0.19', '+4.17', '+1.00', '+0.00']\n",
      "step 65 total_reward -137.92\n",
      "['+0.01', '+0.94', '+0.39', '-0.17', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.10\n",
      "['+0.07', '+0.80', '+0.25', '-0.71', '+0.32', '+0.49', '+0.00', '+0.00']\n",
      "step 20 total_reward -51.66\n",
      "['+0.09', '+0.57', '-0.06', '-0.80', '+0.80', '+0.31', '+0.00', '+0.00']\n",
      "step 40 total_reward -84.35\n",
      "['+0.07', '+0.28', '-0.42', '-0.96', '+0.73', '-0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -75.72\n",
      "['-0.09', '-0.01', '-0.84', '-0.41', '+0.42', '-3.63', '+0.00', '+1.00']\n",
      "step 80 total_reward -153.55\n",
      "['-0.01', '+0.95', '-0.28', '+0.14', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.33\n",
      "['-0.06', '+0.90', '-0.31', '-0.39', '+0.07', '+0.20', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.98\n",
      "['-0.12', '+0.70', '-0.34', '-0.93', '+0.41', '+0.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -85.46\n",
      "['-0.20', '+0.34', '-0.64', '-1.43', '+1.07', '+0.95', '+0.00', '+0.00']\n",
      "step 60 total_reward -178.30\n",
      "['-0.36', '+0.03', '-0.38', '+0.19', '+1.78', '+1.41', '+0.00', '+1.00']\n",
      "step 75 total_reward -383.41\n",
      "['+0.00', '+0.95', '+0.09', '+0.38', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.17\n",
      "['+0.00', '+0.98', '-0.07', '-0.17', '+0.39', '+0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.02\n",
      "['-0.02', '+0.84', '-0.18', '-0.77', '+1.47', '+1.39', '+0.00', '+0.00']\n",
      "step 40 total_reward -175.69\n",
      "['-0.07', '+0.53', '-0.42', '-1.55', '+2.86', '+1.38', '+0.00', '+0.00']\n",
      "step 60 total_reward -366.25\n",
      "['-0.13', '+0.00', '-0.30', '-0.47', '+4.41', '+6.74', '+1.00', '+0.00']\n",
      "step 78 total_reward -638.05\n",
      "['-0.01', '+0.95', '-0.31', '+0.25', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.22\n",
      "['-0.08', '+0.94', '-0.47', '-0.31', '+0.45', '+0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.62\n",
      "['-0.18', '+0.76', '-0.55', '-0.90', '+1.61', '+1.32', '+0.00', '+0.00']\n",
      "step 40 total_reward -210.53\n",
      "['-0.29', '+0.40', '-0.42', '-1.50', '+3.41', '+2.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -412.11\n",
      "['-0.32', '-0.05', '-0.64', '-0.25', '+5.03', '-1.27', '+1.00', '+0.00']\n",
      "step 75 total_reward -717.50\n",
      "['+0.01', '+0.94', '+0.44', '-0.04', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.63\n",
      "['+0.08', '+0.84', '+0.29', '-0.58', '+0.33', '+0.56', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.18\n",
      "['+0.11', '+0.63', '-0.21', '-0.71', '+0.90', '+0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -90.78\n",
      "['+0.05', '+0.36', '-0.61', '-1.03', '+1.18', '+0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -140.73\n",
      "['-0.19', '+0.02', '-1.73', '-1.25', '+1.23', '+0.14', '+0.00', '+1.00']\n",
      "step 80 total_reward -217.22\n",
      "['-0.28', '-0.03', '-1.95', '-0.37', '+2.44', '+2.63', '+0.00', '+1.00']\n",
      "step 85 total_reward -400.67\n",
      "['+0.00', '+0.93', '+0.11', '-0.33', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.59\n",
      "['+0.01', '+0.75', '+0.01', '-0.87', '+0.31', '+0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.84\n",
      "['-0.02', '+0.48', '-0.53', '-0.75', '+0.73', '+0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -91.01\n",
      "['-0.16', '+0.20', '-1.08', '-1.23', '+1.50', '+0.99', '+0.00', '+0.00']\n",
      "step 60 total_reward -220.12\n",
      "['-0.34', '-0.05', '-1.80', '-0.44', '+2.55', '+1.57', '+0.00', '+1.00']\n",
      "step 73 total_reward -452.07\n",
      "['-0.02', '+0.93', '-0.79', '-0.31', '+0.02', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.03\n",
      "['-0.17', '+0.75', '-0.79', '-0.85', '+0.08', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.31\n",
      "['-0.33', '+0.42', '-0.92', '-1.40', '+0.40', '+0.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -81.46\n",
      "['-0.53', '+0.05', '-0.17', '+0.11', '+1.61', '+0.72', '+0.00', '+1.00']\n",
      "step 59 total_reward -265.06\n",
      "['+0.00', '+0.94', '+0.22', '-0.15', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.34\n",
      "['+0.03', '+0.81', '+0.11', '-0.69', '+0.20', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.21\n",
      "['+0.04', '+0.55', '-0.09', '-0.88', '+0.53', '+0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -80.55\n",
      "['+0.01', '+0.26', '-0.46', '-0.90', '+0.41', '-0.24', '+0.00', '+0.00']\n",
      "step 60 total_reward -55.75\n",
      "['-0.11', '+0.01', '-0.59', '-0.97', '+0.10', '-0.51', '+0.00', '+0.00']\n",
      "step 80 total_reward -26.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.13', '-0.03', '-0.42', '+0.00', '+0.00', '-0.00', '+1.00', '+1.00']\n",
      "step 83 total_reward -105.14\n",
      "['+0.00', '+0.95', '+0.05', '+0.16', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.42\n",
      "['+0.02', '+0.91', '+0.07', '-0.36', '-0.22', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.99\n",
      "['+0.03', '+0.72', '+0.02', '-0.89', '-0.11', '+0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -58.69\n",
      "['+0.04', '+0.43', '+0.15', '-1.14', '-0.06', '-0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -53.36\n",
      "['+0.08', '+0.12', '+0.30', '-0.87', '-0.37', '-0.35', '+0.00', '+0.00']\n",
      "step 80 total_reward -38.92\n",
      "['+0.10', '-0.01', '+0.16', '-0.33', '-0.44', '+3.27', '+1.00', '+0.00']\n",
      "step 89 total_reward -149.28\n",
      "['-0.01', '+0.95', '-0.67', '+0.24', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['-0.16', '+0.93', '-0.86', '-0.33', '+0.63', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.65\n",
      "['-0.34', '+0.75', '-0.91', '-0.98', '+2.13', '+1.96', '+0.00', '+0.00']\n",
      "step 40 total_reward -264.05\n",
      "['-0.52', '+0.36', '-0.76', '-1.49', '+4.53', '+2.78', '+0.00', '+0.00']\n",
      "step 60 total_reward -519.80\n",
      "['-0.63', '-0.09', '-0.95', '-0.22', '+7.14', '-0.93', '+0.00', '+1.00']\n",
      "step 78 total_reward -896.67\n",
      "['+0.00', '+0.94', '+0.02', '-0.09', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.01\n",
      "['-0.01', '+0.83', '-0.12', '-0.63', '+0.43', '+0.62', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.12\n",
      "['-0.04', '+0.57', '-0.47', '-0.98', '+1.05', '+0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -172.73\n",
      "['-0.21', '+0.22', '-1.20', '-1.49', '+1.97', '+1.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -323.00\n",
      "['-0.30', '+0.06', '-0.59', '-0.45', '+2.49', '+6.29', '+0.00', '+0.00']\n",
      "step 67 total_reward -477.23\n",
      "['+0.00', '+0.94', '+0.24', '-0.01', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.19\n",
      "['+0.04', '+0.85', '+0.07', '-0.55', '+0.40', '+0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.03\n",
      "['+0.04', '+0.62', '-0.25', '-0.90', '+1.08', '+0.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -145.99\n",
      "['-0.14', '+0.31', '-1.69', '-1.28', '+1.76', '+0.69', '+0.00', '+0.00']\n",
      "step 60 total_reward -311.49\n",
      "['-0.43', '+0.02', '-2.87', '-0.82', '+2.17', '+0.00', '+0.00', '+1.00']\n",
      "step 73 total_reward -547.23\n",
      "['+0.01', '+0.95', '+0.30', '+0.38', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.84\n",
      "['+0.06', '+0.98', '+0.26', '-0.15', '+0.11', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward +6.80\n",
      "['+0.10', '+0.85', '+0.12', '-0.71', '+0.57', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -68.47\n",
      "['+0.11', '+0.57', '-0.24', '-1.08', '+1.26', '+0.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -151.47\n",
      "['+0.03', '+0.19', '-0.84', '-1.47', '+1.47', '+0.12', '+0.00', '+0.00']\n",
      "step 80 total_reward -196.32\n",
      "['-0.07', '+0.00', '-1.15', '+0.13', '+1.98', '+1.50', '+0.00', '+1.00']\n",
      "step 89 total_reward -328.08\n",
      "['-0.01', '+0.93', '-0.49', '-0.40', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.39\n",
      "['-0.12', '+0.72', '-0.57', '-0.94', '+0.42', '+0.46', '+0.00', '+0.00']\n",
      "step 20 total_reward -69.93\n",
      "['-0.24', '+0.37', '-0.75', '-1.46', '+1.18', '+1.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -172.52\n",
      "['-0.46', '+0.01', '-2.31', '-0.27', '+2.15', '+2.04', '+0.00', '+1.00']\n",
      "step 56 total_reward -444.05\n",
      "['-0.01', '+0.95', '-0.48', '+0.40', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.25\n",
      "['-0.11', '+0.99', '-0.52', '-0.13', '+0.28', '+0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.85\n",
      "['-0.22', '+0.86', '-0.65', '-0.72', '+0.85', '+0.97', '+0.00', '+0.00']\n",
      "step 40 total_reward -113.22\n",
      "['-0.35', '+0.56', '-0.65', '-1.36', '+2.22', '+1.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -281.42\n",
      "['-0.49', '+0.03', '-1.10', '-0.19', '+4.31', '+3.41', '+1.00', '+0.00']\n",
      "step 80 total_reward -645.77\n",
      "['+0.00', '+0.95', '+0.08', '+0.15', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.59\n",
      "['+0.00', '+0.91', '-0.11', '-0.40', '+0.42', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.35\n",
      "['-0.02', '+0.71', '-0.14', '-0.95', '+1.37', '+0.97', '+0.00', '+0.00']\n",
      "step 40 total_reward -190.42\n",
      "['-0.15', '+0.34', '-0.99', '-1.60', '+2.40', '+1.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -355.48\n",
      "['-0.30', '-0.02', '-1.81', '-1.10', '+3.15', '+7.34', '+0.00', '+0.00']\n",
      "step 72 total_reward -581.84\n",
      "['-0.01', '+0.93', '-0.63', '-0.46', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.36\n",
      "['-0.15', '+0.71', '-0.72', '-1.00', '+0.45', '+0.52', '+0.00', '+0.00']\n",
      "step 20 total_reward -69.63\n",
      "['-0.31', '+0.32', '-1.07', '-1.56', '+1.41', '+1.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -205.69\n",
      "['-0.47', '+0.05', '-1.87', '-0.61', '+1.93', '-3.30', '+0.00', '+1.00']\n",
      "step 51 total_reward -425.32\n",
      "['+0.00', '+0.93', '+0.05', '-0.26', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.54\n",
      "['-0.00', '+0.77', '-0.06', '-0.80', '+0.27', '+0.36', '+0.00', '+0.00']\n",
      "step 20 total_reward -66.34\n",
      "['-0.04', '+0.52', '-0.54', '-0.69', '+0.65', '+0.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -91.80\n",
      "['-0.25', '+0.33', '-1.55', '-0.60', '+1.05', '+0.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -206.55\n",
      "['-0.69', '+0.13', '-2.93', '-0.80', '+1.52', '+0.51', '+0.00', '+0.00']\n",
      "step 80 total_reward -425.16\n",
      "['-1.02', '-0.02', '-2.99', '-1.10', '+1.79', '+0.50', '+0.00', '+0.00']\n",
      "step 91 total_reward -592.89\n",
      "['+0.01', '+0.94', '+0.54', '+0.03', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.05\n",
      "['+0.10', '+0.87', '+0.34', '-0.51', '+0.33', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.99\n",
      "['+0.17', '+0.64', '+0.03', '-0.87', '+1.04', '+0.73', '+0.00', '+0.00']\n",
      "step 40 total_reward -109.79\n",
      "['+0.05', '+0.35', '-1.27', '-1.24', '+1.77', '+0.71', '+0.00', '+0.00']\n",
      "step 60 total_reward -247.51\n",
      "['-0.25', '-0.03', '-2.45', '-1.26', '+2.24', '-2.86', '+0.00', '+1.00']\n",
      "step 76 total_reward -490.31\n",
      "['+0.00', '+0.95', '+0.19', '+0.19', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.30\n",
      "['+0.03', '+0.92', '-0.00', '-0.36', '+0.39', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.81\n",
      "['+0.03', '+0.73', '-0.04', '-0.91', '+1.33', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -173.50\n",
      "['-0.10', '+0.37', '-1.19', '-1.64', '+2.29', '+0.97', '+0.00', '+0.00']\n",
      "step 60 total_reward -351.56\n",
      "['-0.29', '+0.00', '-1.73', '-1.04', '+3.03', '+9.21', '+0.00', '+0.00']\n",
      "step 72 total_reward -590.93\n",
      "['-0.00', '+0.94', '-0.04', '-0.06', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.20\n",
      "['+0.01', '+0.84', '+0.15', '-0.61', '-0.46', '-0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -92.79\n",
      "['+0.06', '+0.59', '+0.59', '-0.99', '-1.24', '-0.84', '+0.00', '+0.00']\n",
      "step 40 total_reward -200.95\n",
      "['+0.20', '+0.19', '+0.98', '-1.82', '-2.53', '-1.49', '+0.00', '+0.00']\n",
      "step 60 total_reward -392.90\n",
      "['+0.28', '-0.02', '+1.75', '-0.99', '-3.12', '-6.87', '+0.00', '+0.00']\n",
      "step 67 total_reward -572.30\n",
      "['-0.01', '+0.95', '-0.39', '+0.36', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.10\n",
      "['-0.07', '+0.98', '-0.24', '-0.18', '-0.21', '-0.50', '+0.00', '+0.00']\n",
      "step 20 total_reward +0.60\n",
      "['-0.11', '+0.84', '-0.17', '-0.72', '-0.98', '-0.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -108.37\n",
      "['-0.14', '+0.55', '-0.11', '-1.21', '-1.65', '-0.39', '+0.00', '+0.00']\n",
      "step 60 total_reward -195.20\n",
      "['-0.12', '+0.11', '+0.48', '-1.76', '-1.64', '+0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -217.65\n",
      "['-0.10', '+0.00', '+0.38', '+0.08', '-2.00', '-1.01', '+1.00', '+0.00']\n",
      "step 85 total_reward -306.38\n",
      "['+0.00', '+0.92', '+0.02', '-0.54', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.77\n",
      "['-0.00', '+0.68', '-0.03', '-1.07', '+0.23', '+0.24', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.09\n",
      "['-0.04', '+0.43', '-0.44', '-0.67', '+0.51', '+0.33', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.19\n",
      "['-0.21', '+0.27', '-1.27', '-0.44', '+0.87', '+0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -123.09\n",
      "['-0.58', '+0.13', '-2.16', '-0.62', '+1.26', '+0.37', '+0.00', '+0.00']\n",
      "step 80 total_reward -282.51\n",
      "['-1.01', '-0.14', '-2.16', '-1.16', '+1.63', '+0.37', '+0.00', '+0.00']\n",
      "step 100 total_reward -476.84\n",
      "['+0.01', '+0.94', '+0.73', '-0.14', '-0.02', '-0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.90\n",
      "['+0.17', '+0.81', '+0.87', '-0.69', '-0.57', '-0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.76\n",
      "['+0.35', '+0.51', '+0.95', '-1.34', '-1.83', '-1.73', '+0.00', '+0.00']\n",
      "step 40 total_reward -243.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.51', '+0.15', '-0.11', '-0.03', '-3.47', '+0.36', '+0.00', '+0.00']\n",
      "step 56 total_reward -521.91\n",
      "['+0.01', '+0.94', '+0.63', '-0.01', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.76\n",
      "['+0.13', '+0.85', '+0.59', '-0.54', '-0.15', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.32\n",
      "['+0.26', '+0.60', '+0.73', '-1.10', '-0.39', '-0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -81.57\n",
      "['+0.41', '+0.19', '+0.85', '-1.73', '-1.44', '-1.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -227.54\n",
      "['+0.51', '-0.12', '+1.51', '-0.65', '-2.41', '+2.55', '+1.00', '+0.00']\n",
      "step 71 total_reward -450.31\n",
      "['+0.01', '+0.93', '+0.29', '-0.49', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.62\n",
      "['+0.06', '+0.69', '+0.24', '-1.03', '+0.06', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.91\n",
      "['+0.11', '+0.35', '+0.39', '-1.37', '-0.10', '-0.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.12\n",
      "['+0.21', '-0.01', '+0.59', '-0.44', '-0.58', '+3.65', '+1.00', '+0.00']\n",
      "step 60 total_reward -166.23\n",
      "['-0.01', '+0.93', '-0.32', '-0.42', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.90\n",
      "['-0.08', '+0.72', '-0.39', '-0.96', '+0.28', '+0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.27\n",
      "['-0.20', '+0.46', '-0.78', '-1.00', '+0.68', '+0.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -102.46\n",
      "['-0.44', '+0.14', '-1.76', '-1.20', '+1.44', '+0.76', '+0.00', '+0.00']\n",
      "step 60 total_reward -266.66\n",
      "['-0.68', '-0.01', '-2.24', '-0.53', '+2.93', '+4.51', '+0.00', '+0.00']\n",
      "step 71 total_reward -538.96\n",
      "['+0.01', '+0.93', '+0.55', '-0.44', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.52\n",
      "['+0.11', '+0.71', '+0.46', '-0.97', '+0.18', '+0.24', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.18\n",
      "['+0.19', '+0.43', '+0.49', '-1.12', '+0.19', '-0.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -27.62\n",
      "['+0.29', '+0.09', '+0.54', '-1.19', '-0.28', '-0.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -31.90\n",
      "['+0.33', '-0.01', '+0.31', '-0.47', '-0.21', '+4.11', '+0.00', '+0.00']\n",
      "step 66 total_reward -135.61\n",
      "['-0.01', '+0.94', '-0.34', '+0.02', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.11\n",
      "['-0.06', '+0.86', '-0.19', '-0.52', '-0.35', '-0.58', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.21\n",
      "['-0.09', '+0.63', '+0.09', '-0.88', '-0.93', '-0.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.27\n",
      "['+0.07', '+0.36', '+1.44', '-1.11', '-1.49', '-0.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -245.04\n",
      "['+0.44', '+0.00', '+0.94', '+0.28', '-2.03', '-3.09', '+0.00', '+0.00']\n",
      "step 78 total_reward -526.11\n",
      "['-0.01', '+0.95', '-0.59', '+0.45', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.47\n",
      "['-0.14', '+1.00', '-0.73', '-0.10', '+0.50', '+0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.74\n",
      "['-0.29', '+0.88', '-0.82', '-0.75', '+1.70', '+1.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -204.43\n",
      "['-0.46', '+0.57', '-0.68', '-1.32', '+3.79', '+2.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -432.10\n",
      "['-0.58', '+0.10', '-0.58', '-1.67', '+6.62', '+3.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -731.24\n",
      "['-0.62', '-0.09', '-0.93', '-0.69', '+8.15', '+9.15', '+0.00', '+1.00']\n",
      "step 88 total_reward -953.68\n",
      "['+0.01', '+0.94', '+0.32', '+0.02', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.78\n",
      "['+0.05', '+0.86', '+0.14', '-0.53', '+0.36', '+0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.32\n",
      "['+0.08', '+0.63', '-0.16', '-0.88', '+1.07', '+0.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -135.46\n",
      "['+0.02', '+0.32', '-0.62', '-1.28', '+1.46', '+0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -197.12\n",
      "['-0.17', '+0.01', '-1.41', '+0.12', '+2.11', '+1.69', '+0.00', '+1.00']\n",
      "step 76 total_reward -382.23\n",
      "['-0.01', '+0.95', '-0.45', '+0.28', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.54\n",
      "['-0.08', '+0.95', '-0.25', '-0.27', '-0.36', '-0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.24\n",
      "['-0.13', '+0.79', '-0.21', '-0.81', '-1.30', '-0.92', '+0.00', '+0.00']\n",
      "step 40 total_reward -144.32\n",
      "['-0.17', '+0.48', '-0.18', '-1.27', '-2.01', '-0.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -231.15\n",
      "['-0.12', '+0.01', '+0.24', '-0.03', '-2.19', '+0.99', '+0.00', '+0.00']\n",
      "step 79 total_reward -396.18\n",
      "['+0.01', '+0.95', '+0.56', '+0.38', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.12\n",
      "['+0.12', '+0.98', '+0.53', '-0.15', '-0.01', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward +10.00\n",
      "['+0.22', '+0.85', '+0.45', '-0.69', '+0.18', '+0.33', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.86\n",
      "['+0.29', '+0.63', '+0.11', '-0.63', '+0.54', '+0.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -26.51\n",
      "['+0.23', '+0.47', '-0.76', '-0.49', '+0.94', '+0.39', '+0.00', '+0.00']\n",
      "step 80 total_reward -82.42\n",
      "['-0.01', '+0.30', '-1.73', '-0.64', '+1.23', '+0.31', '+0.00', '+0.00']\n",
      "step 100 total_reward -188.11\n",
      "['-0.49', '+0.08', '-2.91', '-0.97', '+1.60', '+0.38', '+0.00', '+0.00']\n",
      "step 120 total_reward -372.59\n",
      "['-0.96', '-0.19', '-2.67', '+0.07', '+2.03', '+0.96', '+0.00', '+1.00']\n",
      "step 136 total_reward -551.00\n",
      "['+0.01', '+0.95', '+0.57', '+0.29', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.34\n",
      "['+0.11', '+0.95', '+0.37', '-0.25', '+0.33', '+0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.73\n",
      "['+0.18', '+0.79', '+0.29', '-0.82', '+1.36', '+1.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -143.80\n",
      "['+0.23', '+0.48', '+0.24', '-1.25', '+2.12', '+0.31', '+0.00', '+0.00']\n",
      "step 60 total_reward -232.62\n",
      "['+0.21', '+0.01', '-0.10', '-0.05', '+2.17', '-1.45', '+0.00', '+1.00']\n",
      "step 80 total_reward -383.97\n",
      "['-0.01', '+0.94', '-0.27', '+0.05', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.18\n",
      "['-0.08', '+0.87', '-0.46', '-0.51', '+0.54', '+0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.03\n",
      "['-0.16', '+0.64', '-0.46', '-1.06', '+1.43', '+1.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -204.09\n",
      "['-0.27', '+0.22', '-0.73', '-1.98', '+2.93', '+1.71', '+0.00', '+0.00']\n",
      "step 60 total_reward -420.10\n",
      "['-0.30', '+0.08', '+0.73', '-0.37', '+3.18', '-2.74', '+0.00', '+0.00']\n",
      "step 65 total_reward -577.40\n",
      "['+0.01', '+0.93', '+0.32', '-0.39', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.79\n",
      "['+0.06', '+0.73', '+0.25', '-0.92', '+0.19', '+0.24', '+0.00', '+0.00']\n",
      "step 20 total_reward -44.98\n",
      "['+0.10', '+0.44', '+0.24', '-1.11', '+0.29', '-0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -47.99\n",
      "['+0.13', '+0.13', '+0.14', '-0.90', '-0.00', '-0.28', '+0.00', '+0.00']\n",
      "step 60 total_reward +25.07\n",
      "['+0.15', '-0.03', '-0.00', '-0.00', '+0.00', '-0.00', '+0.00', '+1.00']\n",
      "step 71 total_reward -85.68\n",
      "['-0.01', '+0.93', '-0.26', '-0.25', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.45\n",
      "['-0.07', '+0.77', '-0.38', '-0.79', '+0.44', '+0.54', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.01\n",
      "['-0.16', '+0.47', '-0.56', '-1.30', '+1.07', '+0.94', '+0.00', '+0.00']\n",
      "step 40 total_reward -171.40\n",
      "['-0.34', '-0.01', '-1.19', '-1.88', '+2.21', '+1.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -353.32\n",
      "['-0.36', '-0.06', '-1.66', '-0.79', '+2.25', '-3.68', '+0.00', '+1.00']\n",
      "step 62 total_reward -462.03\n",
      "['+0.00', '+0.95', '+0.20', '+0.36', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.78\n",
      "['+0.06', '+0.98', '+0.39', '-0.19', '-0.50', '-0.85', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.28\n",
      "['+0.14', '+0.83', '+0.47', '-0.80', '-1.78', '-1.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -218.04\n",
      "['+0.24', '+0.51', '+0.33', '-1.36', '-3.61', '-2.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -420.66\n",
      "['+0.27', '+0.01', '-0.02', '-1.85', '-6.07', '-2.49', '+1.00', '+1.00']\n",
      "step 80 total_reward -664.38\n",
      "['+0.26', '-0.01', '-0.81', '-0.18', '-5.96', '+0.00', '+1.00', '+0.00']\n",
      "step 81 total_reward -764.38\n",
      "['+0.01', '+0.94', '+0.58', '-0.04', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.26\n",
      "['+0.11', '+0.84', '+0.42', '-0.58', '+0.30', '+0.55', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.50\n",
      "['+0.19', '+0.60', '+0.30', '-0.93', '+0.75', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -84.06\n",
      "['+0.25', '+0.27', '+0.15', '-1.15', '+0.53', '-0.41', '+0.00', '+0.00']\n",
      "step 60 total_reward -55.50\n",
      "['+0.24', '+0.01', '-0.52', '-0.85', '+0.38', '+3.84', '+1.00', '+1.00']\n",
      "step 75 total_reward -117.48\n",
      "['-0.00', '+0.93', '-0.20', '-0.40', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.64\n",
      "['-0.05', '+0.72', '-0.27', '-0.94', '+0.33', '+0.36', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.15', '+0.44', '-0.66', '-1.06', '+0.75', '+0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -114.09\n",
      "['-0.39', '+0.11', '-1.88', '-1.20', '+1.44', '+0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -281.01\n",
      "['-0.60', '-0.04', '-2.21', '-0.21', '+2.53', '+2.87', '+0.00', '+0.00']\n",
      "step 70 total_reward -503.31\n",
      "['-0.01', '+0.95', '-0.50', '+0.25', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.99\n",
      "['-0.09', '+0.94', '-0.30', '-0.29', '-0.26', '-0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.46\n",
      "['-0.16', '+0.77', '-0.34', '-0.81', '-0.83', '-0.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -98.97\n",
      "['-0.23', '+0.46', '-0.40', '-1.26', '-1.07', '+0.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -140.60\n",
      "['-0.24', '+0.05', '+0.25', '-1.46', '-0.76', '+0.22', '+0.00', '+0.00']\n",
      "step 80 total_reward -103.26\n",
      "['-0.23', '-0.01', '-0.12', '+0.12', '-1.57', '-4.71', '+0.00', '+0.00']\n",
      "step 85 total_reward -182.92\n",
      "['-0.00', '+0.93', '-0.17', '-0.40', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.63\n",
      "['-0.05', '+0.72', '-0.24', '-0.94', '+0.33', '+0.36', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.72\n",
      "['-0.12', '+0.42', '-0.53', '-1.19', '+0.80', '+0.79', '+0.00', '+0.00']\n",
      "step 40 total_reward -121.76\n",
      "['-0.33', '+0.04', '-1.69', '-1.47', '+1.70', '+0.91', '+0.00', '+0.00']\n",
      "step 60 total_reward -301.23\n",
      "['-0.36', '+0.01', '-1.40', '+0.05', '+1.75', '-0.41', '+0.00', '+1.00']\n",
      "step 62 total_reward -399.05\n",
      "['+0.01', '+0.94', '+0.60', '-0.06', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.54\n",
      "['+0.12', '+0.84', '+0.45', '-0.60', '+0.29', '+0.52', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.05\n",
      "['+0.18', '+0.62', '-0.03', '-0.70', '+0.82', '+0.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -64.21\n",
      "['+0.18', '+0.35', '-0.15', '-1.02', '+0.91', '-0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -82.59\n",
      "['+0.06', '+0.03', '-0.87', '-1.15', '+0.68', '-0.27', '+0.00', '+1.00']\n",
      "step 80 total_reward -62.93\n",
      "['+0.01', '-0.02', '-0.68', '-0.37', '+0.25', '-3.12', '+0.00', '+0.00']\n",
      "step 86 total_reward -73.47\n",
      "['-0.01', '+0.93', '-0.59', '-0.40', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.40\n",
      "['-0.14', '+0.73', '-0.68', '-0.93', '+0.37', '+0.42', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.47\n",
      "['-0.28', '+0.36', '-0.92', '-1.49', '+1.12', '+1.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -169.01\n",
      "['-0.47', '+0.08', '-1.90', '+0.15', '+1.93', '+2.43', '+0.00', '+1.00']\n",
      "step 53 total_reward -398.06\n",
      "['-0.01', '+0.93', '-0.63', '-0.41', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.20\n",
      "['-0.15', '+0.72', '-0.71', '-0.95', '+0.38', '+0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.36\n",
      "['-0.30', '+0.35', '-0.97', '-1.51', '+1.22', '+1.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -179.84\n",
      "['-0.47', '+0.08', '-1.92', '-0.06', '+2.07', '+2.28', '+0.00', '+0.00']\n",
      "step 52 total_reward -384.81\n",
      "['-0.01', '+0.94', '-0.45', '+0.12', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.27\n",
      "['-0.10', '+0.90', '-0.48', '-0.42', '+0.14', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.52\n",
      "['-0.20', '+0.69', '-0.58', '-0.98', '+0.65', '+0.73', '+0.00', '+0.00']\n",
      "step 40 total_reward -110.64\n",
      "['-0.33', '+0.30', '-0.95', '-1.65', '+1.85', '+1.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -281.59\n",
      "['-0.51', '-0.07', '-0.70', '+0.11', '+2.91', '-0.00', '+0.00', '+0.00']\n",
      "step 73 total_reward -562.41\n",
      "['-0.01', '+0.96', '-0.73', '+0.47', '+0.02', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.77\n",
      "['-0.15', '+1.01', '-0.70', '-0.06', '+0.03', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward +9.11\n",
      "['-0.29', '+0.91', '-0.73', '-0.59', '+0.04', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -9.70\n",
      "['-0.45', '+0.65', '-0.92', '-1.16', '+0.62', '+0.98', '+0.00', '+0.00']\n",
      "step 60 total_reward -106.43\n",
      "['-0.65', '+0.21', '-0.97', '-1.81', '+2.06', '+1.88', '+0.00', '+0.00']\n",
      "step 80 total_reward -296.29\n",
      "['-0.71', '+0.05', '-1.39', '-0.27', '+2.65', '+4.34', '+0.00', '+0.00']\n",
      "step 86 total_reward -458.33\n",
      "['+0.00', '+0.94', '+0.18', '-0.13', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.50\n",
      "['+0.02', '+0.81', '+0.04', '-0.67', '+0.36', '+0.54', '+0.00', '+0.00']\n",
      "step 20 total_reward -69.20\n",
      "['+0.01', '+0.56', '-0.38', '-0.89', '+0.90', '+0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -129.83\n",
      "['-0.19', '+0.29', '-1.68', '-1.04', '+1.46', '+0.55', '+0.00', '+0.00']\n",
      "step 60 total_reward -273.07\n",
      "['-0.67', '-0.11', '-2.57', '-1.63', '+2.05', '+0.69', '+0.00', '+0.00']\n",
      "step 80 total_reward -473.14\n",
      "['-0.81', '-0.25', '-0.00', '+0.00', '+2.36', '+0.00', '+0.00', '+0.00']\n",
      "step 86 total_reward -613.51\n",
      "['+0.01', '+0.95', '+0.45', '+0.32', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.35\n",
      "['+0.08', '+0.96', '+0.28', '-0.22', '+0.29', '+0.63', '+0.00', '+0.00']\n",
      "step 20 total_reward -8.91\n",
      "['+0.13', '+0.81', '+0.18', '-0.80', '+1.29', '+1.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -140.66\n",
      "['+0.16', '+0.50', '+0.14', '-1.25', '+2.18', '+0.49', '+0.00', '+0.00']\n",
      "step 60 total_reward -245.67\n",
      "['+0.14', '+0.02', '-0.34', '-2.02', '+2.41', '+0.19', '+0.00', '+0.00']\n",
      "step 80 total_reward -311.94\n",
      "['+0.14', '+0.01', '-0.40', '-0.02', '+2.46', '+1.20', '+0.00', '+0.00']\n",
      "step 81 total_reward -411.94\n",
      "['-0.00', '+0.93', '-0.22', '-0.48', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.59\n",
      "['-0.06', '+0.70', '-0.29', '-1.01', '+0.32', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.02\n",
      "['-0.16', '+0.42', '-0.81', '-0.96', '+0.71', '+0.52', '+0.00', '+0.00']\n",
      "step 40 total_reward -102.69\n",
      "['-0.42', '+0.14', '-1.86', '-0.97', '+1.28', '+0.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -248.38\n",
      "['-0.74', '-0.05', '-2.28', '-0.45', '+2.75', '+2.97', '+0.00', '+0.00']\n",
      "step 75 total_reward -531.58\n",
      "['-0.01', '+0.94', '-0.45', '-0.10', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.93\n",
      "['-0.11', '+0.82', '-0.58', '-0.65', '+0.50', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.08\n",
      "['-0.23', '+0.55', '-0.63', '-1.24', '+1.32', '+1.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -192.30\n",
      "['-0.41', '+0.06', '-1.30', '-2.15', '+2.79', '+1.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -437.92\n",
      "['-0.46', '-0.02', '-1.91', '-0.82', '+3.26', '+6.22', '+0.00', '+0.00']\n",
      "step 63 total_reward -559.99\n",
      "['+0.00', '+0.95', '+0.11', '+0.28', '-0.00', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.23\n",
      "['+0.04', '+0.95', '+0.31', '-0.27', '-0.49', '-0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.64\n",
      "['+0.11', '+0.78', '+0.37', '-0.87', '-1.70', '-1.38', '+0.00', '+0.00']\n",
      "step 40 total_reward -217.52\n",
      "['+0.21', '+0.42', '+0.44', '-1.54', '-3.39', '-2.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -421.64\n",
      "['+0.22', '+0.01', '-1.18', '-0.10', '-4.07', '+1.56', '+0.00', '+1.00']\n",
      "step 76 total_reward -544.48\n",
      "['+0.01', '+0.94', '+0.61', '-0.18', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.42\n",
      "['+0.14', '+0.80', '+0.67', '-0.72', '-0.36', '-0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.52\n",
      "['+0.28', '+0.49', '+0.79', '-1.33', '-1.09', '-1.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -165.00\n",
      "['+0.43', '+0.12', '+0.83', '+0.02', '-2.43', '-2.50', '+0.00', '+0.00']\n",
      "step 57 total_reward -429.36\n",
      "['-0.01', '+0.94', '-0.32', '-0.05', '+0.01', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.72\n",
      "['-0.06', '+0.84', '-0.29', '-0.58', '-0.16', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.55\n",
      "['-0.12', '+0.59', '-0.25', '-0.86', '-0.26', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -51.46\n",
      "['-0.18', '+0.27', '-0.30', '-1.03', '-0.06', '+0.36', '+0.00', '+0.00']\n",
      "step 60 total_reward -22.59\n",
      "['-0.22', '+0.01', '+0.49', '-0.45', '+0.02', '-3.07', '+0.00', '+1.00']\n",
      "step 78 total_reward -106.90\n",
      "['+0.01', '+0.95', '+0.28', '+0.38', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.10\n",
      "['+0.05', '+0.98', '+0.12', '-0.16', '+0.30', '+0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.12\n",
      "['+0.07', '+0.85', '+0.00', '-0.76', '+1.29', '+1.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -145.45\n",
      "['+0.04', '+0.53', '-0.36', '-1.42', '+2.57', '+1.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -315.34\n",
      "['-0.04', '+0.00', '-0.04', '-0.51', '+3.32', '-6.03', '+0.00', '+0.00']\n",
      "step 79 total_reward -570.88\n",
      "['+0.01', '+0.94', '+0.58', '+0.01', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.81\n",
      "['+0.11', '+0.86', '+0.43', '-0.53', '+0.28', '+0.52', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.20', '+0.63', '+0.24', '-0.88', '+0.76', '+0.39', '+0.00', '+0.00']\n",
      "step 40 total_reward -80.85\n",
      "['+0.25', '+0.30', '+0.13', '-1.19', '+0.68', '-0.38', '+0.00', '+0.00']\n",
      "step 60 total_reward -76.24\n",
      "['+0.24', '-0.01', '+0.05', '-0.61', '+0.27', '-5.02', '+0.00', '+1.00']\n",
      "step 77 total_reward -136.07\n",
      "['+0.00', '+0.94', '+0.09', '+0.10', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.45\n",
      "['+0.00', '+0.89', '-0.10', '-0.45', '+0.40', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.71\n",
      "['-0.01', '+0.67', '-0.18', '-0.96', '+1.21', '+0.82', '+0.00', '+0.00']\n",
      "step 40 total_reward -176.35\n",
      "['-0.16', '+0.33', '-1.07', '-1.46', '+2.14', '+1.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -327.67\n",
      "['-0.37', '-0.05', '-2.04', '-0.85', '+3.08', '+6.69', '+0.00', '+0.00']\n",
      "step 74 total_reward -588.24\n",
      "['+0.01', '+0.95', '+0.70', '+0.19', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.68\n",
      "['+0.14', '+0.92', '+0.51', '-0.35', '+0.29', '+0.67', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.97\n",
      "['+0.24', '+0.74', '+0.34', '-0.75', '+0.84', '+0.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.28\n",
      "['+0.23', '+0.50', '-0.28', '-0.93', '+1.25', '+0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -114.19\n",
      "['+0.11', '+0.18', '-1.15', '-1.16', '+1.19', '-0.06', '+0.00', '+0.00']\n",
      "step 80 total_reward -145.29\n",
      "['-0.08', '-0.00', '-1.58', '+0.11', '+1.84', '+1.03', '+0.00', '+0.00']\n",
      "step 92 total_reward -290.67\n",
      "['+0.01', '+0.95', '+0.42', '+0.25', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.77\n",
      "['+0.08', '+0.94', '+0.23', '-0.29', '+0.36', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.63\n",
      "['+0.12', '+0.77', '+0.18', '-0.85', '+1.33', '+1.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -151.96\n",
      "['+0.08', '+0.43', '-0.47', '-1.46', '+2.27', '+0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -281.26\n",
      "['-0.05', '+0.00', '-1.25', '-0.61', '+2.94', '+7.00', '+0.00', '+0.00']\n",
      "step 75 total_reward -489.80\n",
      "['+0.01', '+0.95', '+0.66', '+0.31', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.79\n",
      "['+0.14', '+0.96', '+0.61', '-0.22', '+0.06', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward +2.67\n",
      "['+0.25', '+0.81', '+0.56', '-0.76', '+0.33', '+0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.96\n",
      "['+0.31', '+0.60', '-0.09', '-0.58', '+0.58', '+0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -19.03\n",
      "['+0.29', '+0.37', '-0.15', '-0.83', '+0.44', '-0.41', '+0.00', '+0.00']\n",
      "step 80 total_reward -13.14\n",
      "['+0.21', '+0.19', '-0.49', '-0.44', '+0.03', '-0.41', '+0.00', '+0.00']\n",
      "step 100 total_reward +59.25\n",
      "['+0.11', '+0.01', '-0.53', '-0.76', '-0.36', '-0.09', '+1.00', '+0.00']\n",
      "step 120 total_reward +26.00\n",
      "['+0.09', '-0.02', '-0.58', '-0.43', '-0.20', '+3.10', '+1.00', '+0.00']\n",
      "step 123 total_reward -69.68\n",
      "['-0.01', '+0.95', '-0.38', '+0.24', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.57\n",
      "['-0.07', '+0.94', '-0.23', '-0.29', '-0.11', '-0.48', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.95\n",
      "['-0.13', '+0.77', '-0.27', '-0.81', '-0.49', '-0.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -73.11\n",
      "['-0.19', '+0.45', '-0.38', '-1.30', '-0.57', '+0.27', '+0.00', '+0.00']\n",
      "step 60 total_reward -101.91\n",
      "['-0.24', '+0.05', '-0.12', '-1.39', '-0.23', '+0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -50.30\n",
      "['-0.24', '-0.03', '-0.21', '-0.86', '+0.01', '+5.30', '+1.00', '+0.00']\n",
      "step 84 total_reward -139.37\n",
      "['-0.00', '+0.94', '-0.23', '-0.14', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.42\n",
      "['-0.06', '+0.81', '-0.35', '-0.68', '+0.44', '+0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.35\n",
      "['-0.15', '+0.55', '-0.66', '-1.02', '+1.10', '+0.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -171.99\n",
      "['-0.32', '+0.15', '-1.22', '-1.77', '+2.36', '+1.37', '+0.00', '+0.00']\n",
      "step 60 total_reward -373.68\n",
      "['-0.44', '-0.08', '-1.37', '+0.20', '+2.92', '+0.00', '+0.00', '+0.00']\n",
      "step 68 total_reward -563.11\n",
      "['-0.00', '+0.92', '-0.09', '-0.59', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.50\n",
      "['-0.03', '+0.66', '-0.14', '-1.12', '+0.17', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.12\n",
      "['-0.09', '+0.38', '-0.48', '-0.82', '+0.34', '+0.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -23.30\n",
      "['-0.24', '+0.17', '-0.98', '-0.63', '+0.48', '+0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -55.39\n",
      "['-0.29', '+0.11', '+0.04', '-0.12', '+0.94', '+0.34', '+0.00', '+0.00']\n",
      "step 80 total_reward +0.04\n",
      "['-0.26', '+0.04', '+0.36', '-0.09', '+1.61', '+0.36', '+0.00', '+1.00']\n",
      "step 100 total_reward -174.39\n",
      "['+0.01', '+0.93', '+0.53', '-0.51', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.54\n",
      "['+0.11', '+0.69', '+0.48', '-1.04', '+0.11', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.61\n",
      "['+0.22', '+0.35', '+0.63', '-1.37', '-0.02', '-0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -29.08\n",
      "['+0.33', '-0.02', '+0.57', '-0.64', '-0.36', '+4.84', '+1.00', '+0.00']\n",
      "step 58 total_reward -162.93\n",
      "['+0.00', '+0.95', '+0.17', '+0.23', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.20\n",
      "['+0.02', '+0.93', '-0.04', '-0.32', '+0.45', '+0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.23\n",
      "['+0.01', '+0.75', '-0.09', '-0.88', '+1.53', '+1.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -192.62\n",
      "['-0.08', '+0.37', '-0.84', '-1.82', '+2.64', '+1.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -381.99\n",
      "['-0.19', '-0.00', '-0.35', '-0.03', '+3.08', '+0.18', '+0.00', '+0.00']\n",
      "step 72 total_reward -581.37\n",
      "['+0.01', '+0.94', '+0.55', '+0.11', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.68\n",
      "['+0.10', '+0.89', '+0.34', '-0.44', '+0.32', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.29\n",
      "['+0.17', '+0.68', '+0.20', '-0.89', '+1.06', '+0.76', '+0.00', '+0.00']\n",
      "step 40 total_reward -117.40\n",
      "['+0.15', '+0.37', '-0.22', '-1.25', '+1.55', '+0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -173.71\n",
      "['-0.01', '+0.01', '-1.18', '+0.21', '+2.04', '+2.54', '+0.00', '+1.00']\n",
      "step 78 total_reward -299.66\n",
      "['-0.01', '+0.92', '-0.29', '-0.53', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.46\n",
      "['-0.07', '+0.68', '-0.34', '-1.07', '+0.29', '+0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.57\n",
      "['-0.16', '+0.38', '-0.68', '-1.08', '+0.75', '+0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -96.38\n",
      "['-0.40', '+0.07', '-1.57', '-0.71', '+1.62', '+4.05', '+0.00', '+1.00']\n",
      "step 60 total_reward -223.11\n",
      "['-0.45', '+0.05', '-1.66', '-0.06', '+2.27', '+2.51', '+0.00', '+0.00']\n",
      "step 63 total_reward -365.85\n",
      "['+0.01', '+0.93', '+0.70', '-0.37', '-0.02', '-0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.37\n",
      "['+0.15', '+0.74', '+0.69', '-0.90', '-0.19', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.94\n",
      "['+0.30', '+0.39', '+0.86', '-1.44', '-0.75', '-0.93', '+0.00', '+0.00']\n",
      "step 40 total_reward -121.21\n",
      "['+0.50', '-0.10', '+1.93', '-0.31', '-2.51', '-3.67', '+1.00', '+0.00']\n",
      "step 60 total_reward -390.63\n",
      "['+0.00', '+0.93', '+0.08', '-0.51', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.50\n",
      "['+0.01', '+0.69', '+0.02', '-1.05', '+0.22', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.87\n",
      "['-0.03', '+0.43', '-0.43', '-0.69', '+0.44', '+0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -31.53\n",
      "['-0.19', '+0.27', '-1.15', '-0.47', '+0.67', '+0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -93.30\n",
      "['-0.51', '+0.15', '-1.89', '-0.44', '+0.92', '+0.26', '+0.00', '+0.00']\n",
      "step 80 total_reward -213.38\n",
      "['-0.84', '+0.06', '-1.85', '-0.23', '+2.69', '+2.75', '+0.00', '+0.00']\n",
      "step 98 total_reward -499.89\n",
      "['+0.01', '+0.95', '+0.40', '+0.46', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.66\n",
      "['+0.08', '+1.01', '+0.37', '-0.07', '+0.07', '+0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward +13.26\n",
      "['+0.15', '+0.90', '+0.26', '-0.62', '+0.29', '+0.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -28.65\n",
      "['+0.20', '+0.64', '+0.06', '-0.98', '+0.78', '+0.44', '+0.00', '+0.00']\n",
      "step 60 total_reward -86.99\n",
      "['+0.22', '+0.29', '-0.09', '-1.31', '+0.80', '-0.23', '+0.00', '+0.00']\n",
      "step 80 total_reward -92.13\n",
      "['+0.16', '-0.00', '-0.21', '-0.22', '+0.50', '-3.19', '+0.00', '+1.00']\n",
      "step 96 total_reward -145.52\n",
      "['-0.00', '+0.95', '-0.07', '+0.45', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.44\n",
      "['-0.03', '+1.00', '-0.27', '-0.11', '+0.48', '+0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.37\n",
      "['-0.09', '+0.88', '-0.35', '-0.75', '+1.82', '+1.79', '+0.00', '+0.00']\n",
      "step 40 total_reward -212.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.17', '+0.58', '-0.27', '-1.28', '+3.72', '+2.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -421.87\n",
      "['-0.19', '+0.13', '+0.14', '-1.44', '+6.20', '+2.58', '+0.00', '+0.00']\n",
      "step 80 total_reward -650.02\n",
      "['-0.18', '-0.01', '-0.30', '-0.76', '+6.86', '-3.98', '+0.00', '+1.00']\n",
      "step 86 total_reward -812.25\n",
      "['+0.00', '+0.95', '+0.13', '+0.42', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.07\n",
      "['+0.02', '+1.00', '+0.01', '-0.12', '+0.31', '+0.51', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.79\n",
      "['+0.01', '+0.87', '-0.11', '-0.72', '+1.17', '+1.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -136.58\n",
      "['-0.03', '+0.58', '-0.42', '-1.38', '+2.35', '+1.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -297.36\n",
      "['-0.17', '+0.00', '-0.49', '-0.73', '+3.38', '-7.30', '+0.00', '+0.00']\n",
      "step 79 total_reward -597.66\n",
      "['-0.00', '+0.94', '-0.15', '+0.00', '+0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.09\n",
      "['-0.03', '+0.86', '-0.17', '-0.52', '-0.07', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.35\n",
      "['-0.06', '+0.62', '-0.17', '-0.96', '+0.04', '+0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.94\n",
      "['-0.11', '+0.37', '-0.34', '-0.75', '+0.20', '+0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -36.08\n",
      "['-0.21', '+0.21', '-0.67', '-0.40', '+0.43', '+0.26', '+0.00', '+0.00']\n",
      "step 80 total_reward -51.91\n",
      "['-0.34', '+0.08', '-0.15', '-0.05', '+0.44', '-0.34', '+0.00', '+0.00']\n",
      "step 100 total_reward +1.78\n",
      "['-0.36', '+0.03', '+0.05', '-0.14', '-0.41', '+0.72', '+1.00', '+1.00']\n",
      "step 118 total_reward -90.85\n",
      "['+0.00', '+0.95', '+0.15', '+0.43', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.09\n",
      "['+0.05', '+1.00', '+0.35', '-0.12', '-0.49', '-0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.38\n",
      "['+0.13', '+0.88', '+0.43', '-0.75', '-1.82', '-1.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -214.86\n",
      "['+0.22', '+0.57', '+0.33', '-1.28', '-3.61', '-2.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -412.40\n",
      "['+0.24', '+0.11', '-0.19', '-1.59', '-6.06', '-2.59', '+0.00', '+0.00']\n",
      "step 80 total_reward -653.68\n",
      "['+0.23', '-0.03', '+0.11', '-0.88', '-6.70', '+4.81', '+1.00', '+0.00']\n",
      "step 86 total_reward -818.05\n",
      "['+0.00', '+0.93', '+0.07', '-0.51', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.57\n",
      "['+0.01', '+0.69', '+0.01', '-1.04', '+0.24', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.99\n",
      "['-0.04', '+0.44', '-0.51', '-0.65', '+0.52', '+0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.89\n",
      "['-0.21', '+0.27', '-1.23', '-0.51', '+0.82', '+0.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -119.23\n",
      "['-0.56', '+0.12', '-2.12', '-0.64', '+1.14', '+0.31', '+0.00', '+0.00']\n",
      "step 80 total_reward -266.98\n",
      "['-0.72', '+0.04', '-1.21', '+0.40', '+1.62', '+1.06', '+0.00', '+1.00']\n",
      "step 88 total_reward -385.55\n",
      "['-0.01', '+0.93', '-0.57', '-0.48', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.70\n",
      "['-0.13', '+0.70', '-0.63', '-1.02', '+0.39', '+0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.19\n",
      "['-0.27', '+0.31', '-0.98', '-1.51', '+1.19', '+1.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -175.66\n",
      "['-0.50', '-0.06', '-0.59', '+0.14', '+2.09', '+1.87', '+0.00', '+1.00']\n",
      "step 56 total_reward -441.21\n",
      "['-0.01', '+0.94', '-0.60', '-0.06', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.43\n",
      "['-0.13', '+0.84', '-0.60', '-0.59', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.58\n",
      "['-0.25', '+0.58', '-0.68', '-1.13', '+0.19', '+0.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -58.92\n",
      "['-0.42', '+0.20', '-1.29', '-1.23', '+0.91', '+0.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -164.35\n",
      "['-0.59', '-0.00', '-0.93', '+0.12', '+1.91', '+0.82', '+0.00', '+0.00']\n",
      "step 73 total_reward -339.49\n",
      "['-0.01', '+0.93', '-0.27', '-0.23', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.61\n",
      "['-0.07', '+0.78', '-0.38', '-0.77', '+0.45', '+0.56', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.08\n",
      "['-0.17', '+0.50', '-0.68', '-1.15', '+1.06', '+0.83', '+0.00', '+0.00']\n",
      "step 40 total_reward -168.18\n",
      "['-0.40', '+0.07', '-1.68', '-1.77', '+2.13', '+1.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -376.50\n",
      "['-0.48', '-0.04', '-1.21', '-0.00', '+2.50', '+4.13', '+0.00', '+0.00']\n",
      "step 65 total_reward -509.42\n",
      "['+0.01', '+0.94', '+0.52', '-0.03', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.87\n",
      "['+0.10', '+0.85', '+0.35', '-0.58', '+0.33', '+0.61', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.10\n",
      "['+0.16', '+0.60', '+0.21', '-0.96', '+0.88', '+0.38', '+0.00', '+0.00']\n",
      "step 40 total_reward -103.56\n",
      "['+0.20', '+0.26', '-0.03', '-1.23', '+0.82', '-0.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -94.40\n",
      "['+0.16', '-0.01', '-0.12', '-0.48', '+0.35', '-5.29', '+0.00', '+1.00']\n",
      "step 75 total_reward -143.17\n",
      "['+0.01', '+0.94', '+0.48', '-0.18', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.01\n",
      "['+0.09', '+0.80', '+0.37', '-0.72', '+0.26', '+0.37', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.64\n",
      "['+0.14', '+0.56', '-0.05', '-0.74', '+0.63', '+0.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -53.33\n",
      "['+0.11', '+0.29', '-0.41', '-0.88', '+0.59', '-0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -48.43\n",
      "['-0.03', '+0.07', '-1.01', '-0.62', '+0.46', '-0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward -39.16\n",
      "['-0.14', '-0.03', '-0.96', '-0.44', '+0.04', '-2.79', '+1.00', '+0.00']\n",
      "step 90 total_reward -107.53\n",
      "['+0.01', '+0.93', '+0.53', '-0.45', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.78\n",
      "['+0.11', '+0.71', '+0.47', '-0.98', '+0.11', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.72\n",
      "['+0.20', '+0.42', '+0.52', '-1.13', '+0.14', '-0.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -26.15\n",
      "['+0.29', '+0.11', '+0.49', '-1.02', '-0.17', '-0.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -7.06\n",
      "['+0.33', '-0.00', '+0.29', '-0.52', '-0.16', '+4.05', '+1.00', '+0.00']\n",
      "step 67 total_reward -112.36\n",
      "['+0.00', '+0.94', '+0.21', '+0.11', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.70\n",
      "['+0.03', '+0.89', '+0.02', '-0.44', '+0.40', '+0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.73\n",
      "['+0.04', '+0.68', '-0.04', '-0.95', '+1.15', '+0.76', '+0.00', '+0.00']\n",
      "step 40 total_reward -159.18\n",
      "['-0.12', '+0.34', '-1.51', '-1.44', '+1.90', '+0.75', '+0.00', '+0.00']\n",
      "step 60 total_reward -321.16\n",
      "['-0.37', '+0.02', '-1.80', '-0.02', '+2.57', '+5.41', '+0.00', '+0.00']\n",
      "step 73 total_reward -571.39\n",
      "['+0.01', '+0.95', '+0.52', '+0.32', '-0.01', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.52\n",
      "['+0.13', '+0.96', '+0.70', '-0.24', '-0.58', '-0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -73.62\n",
      "['+0.28', '+0.80', '+0.76', '-0.87', '-1.94', '-1.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -238.88\n",
      "['+0.43', '+0.45', '+0.60', '-1.41', '-4.07', '-2.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -467.67\n",
      "['+0.51', '+0.11', '+0.01', '-0.84', '-5.99', '+4.59', '+1.00', '+1.00']\n",
      "step 74 total_reward -758.76\n",
      "['+0.00', '+0.92', '+0.04', '-0.55', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.05\n",
      "['+0.00', '+0.67', '-0.02', '-1.09', '+0.15', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.13\n",
      "['-0.03', '+0.40', '-0.36', '-0.74', '+0.29', '+0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -11.21\n",
      "['-0.15', '+0.25', '-0.86', '-0.30', '+0.34', '+0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -18.90\n",
      "['-0.37', '+0.18', '-1.20', '-0.24', '+0.28', '-0.25', '+0.00', '+0.00']\n",
      "step 80 total_reward -62.14\n",
      "['-0.59', '+0.05', '-0.64', '+0.11', '-0.25', '-0.20', '+1.00', '+0.00']\n",
      "step 99 total_reward -182.50\n",
      "['+0.01', '+0.93', '+0.44', '-0.48', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.54\n",
      "['+0.09', '+0.70', '+0.38', '-1.01', '+0.15', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.86\n",
      "['+0.16', '+0.40', '+0.39', '-1.18', '+0.16', '-0.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -28.00\n",
      "['+0.23', '+0.07', '+0.40', '-1.16', '-0.21', '-0.36', '+1.00', '+0.00']\n",
      "step 60 total_reward -6.36\n",
      "['+0.23', '+0.03', '-0.55', '-0.33', '-0.16', '+2.04', '+1.00', '+0.00']\n",
      "step 63 total_reward -90.43\n",
      "['+0.00', '+0.94', '+0.12', '-0.05', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.46\n",
      "['+0.01', '+0.84', '-0.04', '-0.60', '+0.42', '+0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward -78.57\n",
      "['-0.00', '+0.59', '-0.34', '-0.97', '+1.09', '+0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -165.20\n",
      "['-0.19', '+0.26', '-1.42', '-1.42', '+1.90', '+0.99', '+0.00', '+0.00']\n",
      "step 60 total_reward -322.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.32', '+0.08', '-0.88', '-0.27', '+2.37', '+6.12', '+0.00', '+0.00']\n",
      "step 68 total_reward -495.07\n",
      "['-0.01', '+0.93', '-0.48', '-0.21', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.26\n",
      "['-0.09', '+0.79', '-0.41', '-0.74', '-0.18', '-0.23', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.46\n",
      "['-0.18', '+0.50', '-0.45', '-1.16', '-0.34', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -65.84\n",
      "['-0.25', '+0.15', '-0.27', '-1.05', '-0.09', '+0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -5.90\n",
      "['-0.28', '-0.04', '-0.75', '-0.66', '+0.06', '+3.85', '+1.00', '+0.00']\n",
      "step 71 total_reward -113.67\n",
      "['-0.01', '+0.93', '-0.30', '-0.22', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.29\n",
      "['-0.08', '+0.78', '-0.40', '-0.76', '+0.43', '+0.52', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.47\n",
      "['-0.17', '+0.50', '-0.64', '-1.14', '+1.02', '+0.85', '+0.00', '+0.00']\n",
      "step 40 total_reward -158.99\n",
      "['-0.40', '+0.09', '-1.78', '-1.72', '+2.04', '+1.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -369.85\n",
      "['-0.47', '-0.01', '-0.99', '-0.10', '+2.29', '+5.49', '+0.00', '+0.00']\n",
      "step 64 total_reward -500.44\n",
      "['-0.01', '+0.94', '-0.62', '+0.01', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.89\n",
      "['-0.14', '+0.86', '-0.72', '-0.53', '+0.35', '+0.57', '+0.00', '+0.00']\n",
      "step 20 total_reward -55.59\n",
      "['-0.28', '+0.62', '-0.77', '-1.10', '+1.00', '+0.94', '+0.00', '+0.00']\n",
      "step 40 total_reward -147.44\n",
      "['-0.46', '+0.19', '-1.19', '-1.91', '+2.38', '+1.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -359.68\n",
      "['-0.51', '+0.08', '+0.00', '-0.00', '+2.71', '+0.00', '+0.00', '+0.00']\n",
      "step 64 total_reward -493.96\n",
      "['-0.01', '+0.92', '-0.60', '-0.54', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.48\n",
      "['-0.14', '+0.68', '-0.67', '-1.07', '+0.32', '+0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.51\n",
      "['-0.29', '+0.28', '-1.10', '-1.48', '+1.01', '+0.91', '+0.00', '+0.00']\n",
      "step 40 total_reward -154.96\n",
      "['-0.43', '+0.06', '-0.65', '+0.22', '+1.77', '+1.37', '+0.00', '+1.00']\n",
      "step 51 total_reward -318.79\n",
      "['-0.01', '+0.94', '-0.56', '-0.09', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.58\n",
      "['-0.12', '+0.83', '-0.58', '-0.62', '-0.01', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.66\n",
      "['-0.24', '+0.56', '-0.68', '-1.17', '+0.27', '+0.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -71.00\n",
      "['-0.41', '+0.16', '-1.29', '-1.41', '+1.13', '+0.93', '+0.00', '+0.00']\n",
      "step 60 total_reward -201.01\n",
      "['-0.44', '+0.12', '-0.15', '+0.12', '+1.51', '+0.70', '+0.00', '+0.00']\n",
      "step 63 total_reward -293.18\n",
      "['+0.00', '+0.93', '+0.17', '-0.38', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.65\n",
      "['+0.03', '+0.73', '+0.11', '-0.91', '+0.10', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.53\n",
      "['+0.04', '+0.43', '+0.11', '-1.15', '+0.13', '-0.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.08\n",
      "['+0.07', '+0.08', '+0.26', '-1.23', '-0.35', '-0.67', '+0.00', '+0.00']\n",
      "step 60 total_reward -44.48\n",
      "['+0.09', '-0.01', '+0.10', '-0.40', '-0.36', '+3.87', '+1.00', '+0.00']\n",
      "step 65 total_reward -148.01\n",
      "['-0.01', '+0.95', '-0.47', '+0.19', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.67\n",
      "['-0.12', '+0.92', '-0.66', '-0.37', '+0.56', '+0.95', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.74\n",
      "['-0.25', '+0.72', '-0.71', '-0.96', '+1.80', '+1.43', '+0.00', '+0.00']\n",
      "step 40 total_reward -231.69\n",
      "['-0.40', '+0.34', '-0.54', '-1.60', '+3.69', '+2.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -446.98\n",
      "['-0.41', '+0.09', '+0.86', '+0.05', '+3.96', '-2.58', '+1.00', '+0.00']\n",
      "step 71 total_reward -503.78\n",
      "['+0.02', '+0.93', '+0.79', '-0.37', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.03\n",
      "['+0.16', '+0.74', '+0.71', '-0.90', '+0.13', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.58\n",
      "['+0.32', '+0.41', '+0.86', '-1.33', '+0.00', '-0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -29.41\n",
      "['+0.45', '+0.08', '+0.69', '-0.41', '-0.45', '+4.23', '+1.00', '+0.00']\n",
      "step 55 total_reward -184.51\n",
      "['-0.00', '+0.93', '-0.23', '-0.42', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.62\n",
      "['-0.06', '+0.72', '-0.32', '-0.96', '+0.36', '+0.41', '+0.00', '+0.00']\n",
      "step 20 total_reward -70.28\n",
      "['-0.16', '+0.41', '-0.60', '-1.28', '+0.95', '+0.93', '+0.00', '+0.00']\n",
      "step 40 total_reward -143.39\n",
      "['-0.35', '-0.04', '-1.23', '-1.54', '+2.15', '+4.01', '+0.00', '+1.00']\n",
      "step 60 total_reward -302.36\n",
      "['-0.36', '-0.05', '-1.75', '-0.30', '+2.40', '+2.15', '+0.00', '+1.00']\n",
      "step 61 total_reward -402.36\n",
      "['-0.01', '+0.94', '-0.62', '-0.04', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.09\n",
      "['-0.15', '+0.84', '-0.77', '-0.60', '+0.51', '+0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.50\n",
      "['-0.30', '+0.58', '-0.82', '-1.20', '+1.50', '+1.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -206.00\n",
      "['-0.47', '+0.12', '+0.15', '-0.45', '+3.12', '-4.56', '+0.00', '+0.00']\n",
      "step 60 total_reward -534.27\n",
      "['+0.01', '+0.95', '+0.64', '+0.36', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.08\n",
      "['+0.15', '+0.97', '+0.76', '-0.19', '-0.48', '-0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.57\n",
      "['+0.31', '+0.83', '+0.86', '-0.83', '-1.62', '-1.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -202.24\n",
      "['+0.50', '+0.47', '+0.88', '-1.53', '-3.62', '-2.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -441.83\n",
      "['+0.67', '-0.07', '-0.63', '-0.53', '-6.32', '+3.15', '+1.00', '+0.00']\n",
      "step 80 total_reward -830.72\n",
      "['-0.00', '+0.95', '-0.18', '+0.15', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.49\n",
      "['-0.06', '+0.91', '-0.37', '-0.40', '+0.50', '+0.85', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.76\n",
      "['-0.13', '+0.71', '-0.40', '-0.95', '+1.51', '+1.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -206.98\n",
      "['-0.21', '+0.33', '-0.37', '-1.64', '+2.92', '+1.74', '+0.00', '+0.00']\n",
      "step 60 total_reward -381.64\n",
      "['-0.25', '+0.04', '+0.68', '-0.76', '+3.89', '+6.27', '+1.00', '+0.00']\n",
      "step 70 total_reward -606.01\n",
      "total rewards [-419.50770427407065, -468.74116108606296, -78.81131875081178, -482.1718258272197, -127.0508284064715, -573.933416767014, -131.21606354507074, -444.478561907007, -611.7069553877491, -11.165530058571747, -621.0562441227926, -376.58278444243524, -613.9230506352454, -260.1718035331409, -666.5612101359501, -642.8003822596331, -422.8067924495066, -1091.543071351622, -356.8571553682406, -463.32303570510174, -174.63611264081774, -494.30789079906845, -570.2548999991462, -499.101211093707, -87.03330140903923, -31.937237000425824, -516.708910095506, -353.5851498684124, -211.43303453161403, -544.7749212384916, -519.5047233363875, -445.6224812495314, -397.6814062570313, -648.1261309993732, -1151.4295121148834, -392.11483879665144, -317.6421678015366, -643.8132126338498, -557.3678798495616, -589.856160182906, -101.31168398651873, -154.20688286076398, -77.19809217388263, -78.6374530236557, -187.1981372302123, -695.1492313418144, -396.71303042003836, -245.79383849867494, -582.5052003305354, -829.1697007182007, -805.2123521981428, -314.4582813503299, -822.7433199700322, -82.30682505108717, -128.15452452294267, -801.3874375500176, -509.38352254723907, -586.2324986423054, -390.122596407812, -437.7757315025242, -448.24609557024377, -113.68221834426919, -131.34253364454437, -610.2957865935796, -131.91810144744272, 3.209768251757936, -504.61204067161225, -464.2890634928483, -408.71129208140763, -236.53727758709624, -522.1255266931873, -522.1606686384234, -999.9166466125617, -357.84557452937827, -535.083589498531, -514.140992819129, -710.5323553907443, -519.324618396807, -473.2535268251448, -476.60948199666245, -725.3222201739926, -214.6880191487046, -152.27687366032973, -456.99067122281144, -203.09119558147358, -131.67109289394895, -436.506342693195, -391.1139820289532, -538.1947550876055, -405.4893452298514, -265.53392245363574, -88.51493690396548, -234.26453326243094, -209.07252280482646, -853.9567853395521, -367.840479020112, -543.4643942963007, -367.7809680970001, -371.93655659673294, -836.1971555724302, -602.9845277028809, -137.91749197101487, -153.55247023099108, -383.4110734526179, -638.0479398866112, -717.5010677825504, -400.67106436497824, -452.0702067851024, -265.062866427587, -105.1393389246789, -149.28382040751154, -896.6716280583796, -477.2320598668642, -547.2270626850703, -328.0779647586046, -444.0507238911383, -645.7746914192425, -581.8367779150683, -425.31913563005725, -592.886134155146, -490.3065504249292, -590.9341542520809, -572.298264898689, -306.38485560996, -476.8377873954321, -521.9108871117804, -450.3058207174389, -166.22546822874648, -538.9643752147792, -135.60946151118435, -526.1147210946083, -953.6776311238103, -382.23153485053024, -396.1780145337586, -551.0047798317871, -383.9701093375827, -577.4032826887768, -85.68418711177706, -462.02927689059584, -764.382399774698, -117.4750803379654, -503.3142105300198, -182.9160660606634, -399.0533955887022, -73.46658057689102, -398.06263565273736, -384.8073300911448, -562.412236068162, -458.32945906771454, -613.5093711768382, -411.9413314420989, -531.5802100047888, -559.990618906194, -544.478844703439, -429.3556375806917, -106.89803114440073, -570.8796654473163, -136.07017008057375, -588.2403226765628, -290.6698739963897, -489.79700007262045, -69.67545377445836, -139.36880560204312, -563.1106552992069, -174.38702824772662, -162.9312783521599, -581.370213095015, -299.664184393166, -365.8538446672465, -390.6274977135987, -499.8856777985559, -145.52474116934798, -812.2545036351097, -597.6634932862939, -90.85391477950154, -818.0500643508599, -385.5536501379318, -441.2139923432294, -339.49197218014456, -509.41762351037914, -143.16557869600015, -107.5321249629759, -112.35807326385616, -571.3884334419485, -758.761477081653, -182.50329651651873, -90.4312077950381, -495.0673817555339, -113.67449303281084, -500.4377771883915, -493.9585028031775, -318.79203425725314, -293.17561621360414, -148.0092493686239, -503.7774372708649, -184.51008771790828, -402.35898780653457, -534.2698956477675, -830.7152349078407, -606.0129023185154]\n",
      "average total reward -420.58271429697925\n"
     ]
    }
   ],
   "source": [
    "!python lunar_lander_ml_images_player_model4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_model_reward_comparisons[\"Model 4\"] = '-420.58271429697925'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-04-23 00:42:15.143883: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "['-0.00', '+0.93', '-0.17', '-0.25', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.61\n",
      "['-0.05', '+0.77', '-0.23', '-0.79', '+0.29', '+0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -66.72\n",
      "['-0.13', '+0.55', '-0.68', '-0.77', '+0.62', '+0.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -103.96\n",
      "['-0.30', '+0.24', '-1.25', '-1.31', '+1.55', '+1.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -258.85\n",
      "['-0.41', '+0.06', '-1.24', '+0.02', '+1.80', '-2.21', '+0.00', '+1.00']\n",
      "step 69 total_reward -408.69\n",
      "['+0.01', '+0.95', '+0.68', '+0.17', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.70\n",
      "['+0.13', '+0.92', '+0.48', '-0.36', '+0.30', '+0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.57\n",
      "['+0.23', '+0.72', '+0.46', '-0.90', '+1.11', '+0.73', '+0.00', '+0.00']\n",
      "step 40 total_reward -121.51\n",
      "['+0.33', '+0.39', '+0.51', '-1.30', '+1.33', '-0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -158.57\n",
      "['+0.44', '+0.00', '+0.92', '+0.04', '+1.17', '+1.98', '+0.00', '+1.00']\n",
      "step 80 total_reward -78.47\n",
      "['+0.55', '+0.00', '+0.57', '-0.09', '+0.60', '-1.64', '+0.00', '+1.00']\n",
      "step 96 total_reward -111.63\n",
      "['+0.01', '+0.94', '+0.69', '-0.09', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.25\n",
      "['+0.14', '+0.83', '+0.59', '-0.63', '+0.18', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.59\n",
      "['+0.26', '+0.56', '+0.69', '-1.14', '+0.34', '-0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -63.70\n",
      "['+0.41', '+0.14', '+0.82', '-1.65', '-0.21', '-0.71', '+0.00', '+0.00']\n",
      "step 60 total_reward -85.07\n",
      "['+0.44', '+0.06', '+1.20', '-0.24', '-0.29', '+0.00', '+1.00', '+0.00']\n",
      "step 63 total_reward -185.25\n",
      "['+0.00', '+0.93', '+0.11', '-0.34', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.41\n",
      "['+0.02', '+0.74', '+0.06', '-0.88', '+0.20', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.18\n",
      "['-0.01', '+0.53', '-0.38', '-0.51', '+0.40', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -34.73\n",
      "['-0.13', '+0.39', '-0.73', '-0.68', '+0.75', '+0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -97.42\n",
      "['-0.34', '+0.15', '-1.21', '-0.97', '+1.52', '+0.51', '+0.00', '+0.00']\n",
      "step 80 total_reward -227.90\n",
      "['-0.51', '-0.07', '-1.56', '-0.30', '+2.38', '+1.10', '+0.00', '+1.00']\n",
      "step 94 total_reward -414.47\n",
      "['+0.01', '+0.94', '+0.40', '-0.15', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.15\n",
      "['+0.07', '+0.81', '+0.28', '-0.69', '+0.30', '+0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -48.60\n",
      "['+0.14', '+0.53', '+0.37', '-1.14', '+0.58', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -95.67\n",
      "['+0.22', '+0.13', '+0.36', '-1.38', '+0.15', '-0.55', '+0.00', '+0.00']\n",
      "step 60 total_reward -48.39\n",
      "['+0.24', '-0.03', '+0.04', '+0.00', '+0.01', '-0.00', '+0.00', '+0.00']\n",
      "step 68 total_reward -124.39\n",
      "['+0.01', '+0.94', '+0.28', '-0.00', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.18\n",
      "['+0.05', '+0.86', '+0.16', '-0.54', '+0.33', '+0.49', '+0.00', '+0.00']\n",
      "step 20 total_reward -51.76\n",
      "['+0.08', '+0.61', '+0.19', '-1.05', '+0.79', '+0.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.59\n",
      "['+0.12', '+0.24', '-0.03', '-1.30', '+0.63', '-0.39', '+0.00', '+0.00']\n",
      "step 60 total_reward -98.07\n",
      "['+0.09', '-0.02', '-0.15', '-0.58', '+0.13', '-4.65', '+1.00', '+0.00']\n",
      "step 74 total_reward -145.10\n",
      "['+0.01', '+0.93', '+0.74', '-0.34', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.33\n",
      "['+0.15', '+0.74', '+0.69', '-0.88', '+0.03', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.99\n",
      "['+0.31', '+0.40', '+0.86', '-1.41', '-0.23', '-0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -62.82\n",
      "['+0.43', '+0.07', '+0.53', '-0.02', '-0.71', '+0.94', '+1.00', '+0.00']\n",
      "step 55 total_reward -187.16\n",
      "['+0.01', '+0.94', '+0.75', '-0.19', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.10\n",
      "['+0.16', '+0.80', '+0.69', '-0.72', '+0.04', '+0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.49\n",
      "['+0.30', '+0.50', '+0.83', '-1.25', '-0.09', '-0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.05\n",
      "['+0.47', '+0.08', '+0.78', '-0.53', '-1.12', '-7.00', '+1.00', '+0.00']\n",
      "step 59 total_reward -249.88\n",
      "['+0.00', '+0.94', '+0.04', '+0.01', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.26\n",
      "['-0.01', '+0.86', '-0.10', '-0.53', '+0.40', '+0.57', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.45\n",
      "['-0.05', '+0.65', '-0.65', '-0.71', '+0.97', '+0.58', '+0.00', '+0.00']\n",
      "step 40 total_reward -161.55\n",
      "['-0.23', '+0.39', '-0.95', '-1.20', '+1.92', '+1.41', '+0.00', '+0.00']\n",
      "step 60 total_reward -294.45\n",
      "['-0.48', '-0.07', '-1.56', '-0.20', '+3.33', '-0.00', '+0.00', '+0.00']\n",
      "step 80 total_reward -591.49\n",
      "['+0.00', '+0.93', '+0.08', '-0.42', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.59\n",
      "['+0.01', '+0.72', '+0.01', '-0.92', '+0.27', '+0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.84\n",
      "['-0.05', '+0.51', '-0.64', '-0.54', '+0.58', '+0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -65.80\n",
      "['-0.21', '+0.29', '-1.17', '-0.96', '+1.21', '+0.82', '+0.00', '+0.00']\n",
      "step 60 total_reward -182.22\n",
      "['-0.39', '+0.10', '-1.19', '-0.22', '+1.64', '-3.75', '+0.00', '+1.00']\n",
      "step 72 total_reward -382.29\n",
      "['+0.01', '+0.95', '+0.72', '+0.34', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.81\n",
      "['+0.14', '+0.97', '+0.52', '-0.20', '+0.28', '+0.67', '+0.00', '+0.00']\n",
      "step 20 total_reward -5.26\n",
      "['+0.24', '+0.82', '+0.43', '-0.77', '+1.31', '+1.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -129.05\n",
      "['+0.32', '+0.53', '+0.38', '-1.21', '+2.17', '+0.39', '+0.00', '+0.00']\n",
      "step 60 total_reward -229.26\n",
      "['+0.38', '+0.10', '+0.31', '-1.68', '+2.14', '-0.15', '+0.00', '+0.00']\n",
      "step 80 total_reward -248.66\n",
      "['+0.41', '-0.09', '+1.12', '-0.62', '+2.21', '+3.35', '+0.00', '+1.00']\n",
      "step 87 total_reward -371.35\n",
      "['-0.01', '+0.94', '-0.49', '+0.12', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.20\n",
      "['-0.12', '+0.90', '-0.69', '-0.44', '+0.59', '+0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -87.29\n",
      "['-0.30', '+0.70', '-1.38', '-0.85', '+1.59', '+1.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -255.89\n",
      "['-0.58', '+0.36', '-1.29', '-1.49', '+3.08', '+1.83', '+0.00', '+0.00']\n",
      "step 60 total_reward -432.02\n",
      "['-0.72', '+0.09', '+0.01', '-0.00', '+3.89', '-0.03', '+1.00', '+0.00']\n",
      "step 71 total_reward -653.65\n",
      "['+0.01', '+0.92', '+0.44', '-0.58', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.69\n",
      "['+0.09', '+0.67', '+0.42', '-1.11', '+0.06', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.19\n",
      "['+0.19', '+0.26', '+0.64', '-1.47', '-0.30', '-0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -58.67\n",
      "['+0.26', '+0.04', '+0.06', '-0.02', '-0.66', '-1.04', '+1.00', '+0.00']\n",
      "step 51 total_reward -176.74\n",
      "['-0.01', '+0.94', '-0.57', '-0.17', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.89\n",
      "['-0.13', '+0.80', '-0.66', '-0.71', '+0.45', '+0.51', '+0.00', '+0.00']\n",
      "step 20 total_reward -70.97\n",
      "['-0.31', '+0.58', '-1.05', '-0.96', '+1.07', '+0.93', '+0.00', '+0.00']\n",
      "step 40 total_reward -165.03\n",
      "['-0.54', '+0.20', '-1.31', '-1.61', '+2.40', '+1.30', '+0.00', '+0.00']\n",
      "step 60 total_reward -357.03\n",
      "['-0.65', '-0.01', '-1.68', '-0.76', '+3.02', '+6.09', '+0.00', '+0.00']\n",
      "step 68 total_reward -525.07\n",
      "['+0.01', '+0.92', '+0.38', '-0.58', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.09\n",
      "['+0.08', '+0.67', '+0.36', '-1.11', '+0.10', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.44\n",
      "['+0.16', '+0.25', '+0.54', '-1.56', '-0.27', '-0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -62.09\n",
      "['+0.20', '+0.02', '-0.25', '-0.22', '-0.39', '+1.01', '+1.00', '+0.00']\n",
      "step 60 total_reward +75.04\n",
      "['+0.18', '-0.02', '-0.29', '-0.25', '+0.02', '+1.32', '+1.00', '+1.00']\n",
      "step 68 total_reward +1.91\n",
      "['-0.00', '+0.94', '-0.16', '-0.20', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.61\n",
      "['-0.04', '+0.79', '-0.23', '-0.73', '+0.30', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -69.43\n",
      "['-0.13', '+0.58', '-0.73', '-0.66', '+0.64', '+0.40', '+0.00', '+0.00']\n",
      "step 40 total_reward -109.39\n",
      "['-0.30', '+0.29', '-1.16', '-1.22', '+1.48', '+1.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -247.33\n",
      "['-0.51', '-0.05', '-0.95', '+0.01', '+2.19', '+0.00', '+0.00', '+1.00']\n",
      "step 77 total_reward -461.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.01', '+0.94', '+0.28', '+0.07', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.61\n",
      "['+0.05', '+0.88', '+0.13', '-0.48', '+0.36', '+0.57', '+0.00', '+0.00']\n",
      "step 20 total_reward -48.09\n",
      "['+0.08', '+0.66', '+0.06', '-0.93', '+0.92', '+0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -127.74\n",
      "['+0.10', '+0.31', '+0.06', '-1.34', '+0.93', '-0.39', '+0.00', '+0.00']\n",
      "step 60 total_reward -136.80\n",
      "['+0.04', '-0.00', '-0.30', '-0.21', '+0.58', '-2.75', '+0.00', '+1.00']\n",
      "step 76 total_reward -137.85\n",
      "['-0.01', '+0.95', '-0.60', '+0.31', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.79\n",
      "['-0.15', '+0.96', '-0.79', '-0.26', '+0.61', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.21\n",
      "['-0.31', '+0.79', '-0.84', '-0.90', '+2.09', '+1.92', '+0.00', '+0.00']\n",
      "step 40 total_reward -255.67\n",
      "['-0.53', '+0.40', '-0.98', '-1.54', '+4.24', '+2.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -513.03\n",
      "['-0.63', '+0.09', '-0.00', '-0.00', '+5.84', '+0.00', '+0.00', '+1.00']\n",
      "step 73 total_reward -720.07\n",
      "['-0.01', '+0.95', '-0.58', '+0.35', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.82\n",
      "['-0.14', '+0.97', '-0.78', '-0.21', '+0.61', '+1.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.74\n",
      "['-0.30', '+0.82', '-0.83', '-0.83', '+2.03', '+1.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -246.07\n",
      "['-0.51', '+0.45', '-0.93', '-1.51', '+3.96', '+2.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -482.08\n",
      "['-0.65', '-0.01', '-0.87', '-1.06', '+5.94', '+8.10', '+0.00', '+0.00']\n",
      "step 77 total_reward -785.38\n",
      "['-0.01', '+0.95', '-0.67', '+0.41', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.04\n",
      "['-0.16', '+0.99', '-0.85', '-0.15', '+0.61', '+0.96', '+0.00', '+0.00']\n",
      "step 20 total_reward -74.19\n",
      "['-0.34', '+0.86', '-0.90', '-0.80', '+2.05', '+1.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -244.62\n",
      "['-0.54', '+0.51', '-0.86', '-1.43', '+4.31', '+2.61', '+0.00', '+0.00']\n",
      "step 60 total_reward -499.44\n",
      "['-0.65', '+0.00', '-0.60', '-1.90', '+7.05', '+2.82', '+0.00', '+0.00']\n",
      "step 80 total_reward -799.27\n",
      "['-0.68', '-0.07', '-1.37', '-0.54', '+7.31', '-1.28', '+0.00', '+1.00']\n",
      "step 83 total_reward -915.15\n",
      "['+0.01', '+0.93', '+0.74', '-0.32', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.07\n",
      "['+0.15', '+0.75', '+0.67', '-0.86', '+0.11', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.07\n",
      "['+0.30', '+0.41', '+0.84', '-1.38', '-0.05', '-0.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.47\n",
      "['+0.45', '+0.00', '+1.01', '-0.58', '-0.66', '+4.49', '+1.00', '+0.00']\n",
      "step 57 total_reward -228.23\n",
      "['-0.01', '+0.95', '-0.50', '+0.43', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.19\n",
      "['-0.13', '+1.00', '-0.69', '-0.13', '+0.58', '+0.96', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.31\n",
      "['-0.27', '+0.87', '-0.76', '-0.77', '+2.01', '+1.79', '+0.00', '+0.00']\n",
      "step 40 total_reward -239.23\n",
      "['-0.47', '+0.50', '-0.89', '-1.56', '+3.92', '+2.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -482.14\n",
      "['-0.60', '-0.07', '-0.67', '-2.10', '+6.15', '+2.22', '+0.00', '+0.00']\n",
      "step 80 total_reward -739.26\n",
      "['-0.62', '-0.13', '-0.30', '-0.94', '+6.28', '-6.22', '+1.00', '+1.00']\n",
      "step 82 total_reward -834.52\n",
      "['+0.00', '+0.94', '+0.07', '-0.10', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.12\n",
      "['+0.00', '+0.82', '-0.04', '-0.64', '+0.36', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.95\n",
      "['-0.04', '+0.61', '-0.64', '-0.67', '+0.82', '+0.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.40\n",
      "['-0.22', '+0.35', '-1.04', '-1.15', '+1.58', '+1.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -255.48\n",
      "['-0.45', '-0.01', '-0.76', '-0.58', '+2.65', '+6.64', '+0.00', '+0.00']\n",
      "step 78 total_reward -497.20\n",
      "['-0.00', '+0.95', '-0.09', '+0.36', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.49\n",
      "['-0.04', '+0.97', '-0.29', '-0.20', '+0.47', '+0.85', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.08\n",
      "['-0.10', '+0.83', '-0.36', '-0.78', '+1.69', '+1.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -206.30\n",
      "['-0.22', '+0.48', '-0.67', '-1.57', '+3.08', '+1.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -401.31\n",
      "['-0.32', '+0.01', '+0.05', '+0.01', '+4.27', '-0.17', '+0.00', '+0.00']\n",
      "step 77 total_reward -662.00\n",
      "['-0.00', '+0.95', '-0.04', '+0.31', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.76\n",
      "['-0.02', '+0.96', '-0.23', '-0.24', '+0.45', '+0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.62\n",
      "['-0.07', '+0.80', '-0.28', '-0.81', '+1.54', '+1.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -192.81\n",
      "['-0.21', '+0.47', '-0.78', '-1.49', '+2.78', '+1.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -373.04\n",
      "['-0.34', '+0.03', '-0.59', '-0.29', '+4.12', '+5.62', '+1.00', '+0.00']\n",
      "step 76 total_reward -631.17\n",
      "['-0.01', '+0.94', '-0.53', '-0.11', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.28\n",
      "['-0.13', '+0.82', '-0.66', '-0.66', '+0.54', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.34\n",
      "['-0.29', '+0.58', '-0.99', '-1.03', '+1.32', '+1.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -195.38\n",
      "['-0.51', '+0.16', '-1.26', '-1.84', '+2.81', '+1.58', '+0.00', '+0.00']\n",
      "step 60 total_reward -415.33\n",
      "['-0.52', '+0.14', '-0.17', '-0.77', '+2.77', '-5.97', '+0.00', '+0.00']\n",
      "step 61 total_reward -515.33\n",
      "['+0.02', '+0.94', '+0.79', '-0.03', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.50\n",
      "['+0.16', '+0.85', '+0.67', '-0.57', '+0.18', '+0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.42\n",
      "['+0.30', '+0.59', '+0.75', '-1.08', '+0.42', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -65.74\n",
      "['+0.46', '+0.19', '+0.93', '-1.58', '-0.01', '-0.80', '+0.00', '+0.00']\n",
      "step 60 total_reward -61.19\n",
      "['+0.53', '-0.00', '+1.26', '-0.24', '-0.28', '-0.00', '+0.00', '+1.00']\n",
      "step 68 total_reward -180.55\n",
      "['-0.00', '+0.92', '-0.19', '-0.53', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.44\n",
      "['-0.05', '+0.68', '-0.29', '-0.95', '+0.20', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.66\n",
      "['-0.13', '+0.40', '-0.53', '-1.17', '+0.50', '+0.63', '+0.00', '+0.00']\n",
      "step 40 total_reward -76.73\n",
      "['-0.28', '+0.02', '-0.86', '-1.46', '+1.22', '+0.76', '+0.00', '+1.00']\n",
      "step 60 total_reward -168.10\n",
      "['-0.30', '+0.01', '-0.69', '+0.37', '+1.70', '+2.59', '+0.00', '+1.00']\n",
      "step 62 total_reward -257.27\n",
      "['-0.02', '+0.94', '-0.80', '+0.12', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.63\n",
      "['-0.19', '+0.89', '-0.99', '-0.45', '+0.65', '+1.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -90.53\n",
      "['-0.40', '+0.67', '-1.18', '-1.09', '+2.13', '+1.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -278.41\n",
      "['-0.64', '+0.26', '-1.02', '-1.60', '+4.47', '+2.74', '+0.00', '+0.00']\n",
      "step 60 total_reward -532.91\n",
      "['-0.70', '+0.16', '-0.96', '+0.02', '+4.40', '-0.90', '+1.00', '+0.00']\n",
      "step 66 total_reward -551.18\n",
      "['-0.01', '+0.95', '-0.59', '+0.32', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.90\n",
      "['-0.15', '+0.96', '-0.79', '-0.25', '+0.61', '+0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -78.24\n",
      "['-0.31', '+0.80', '-0.99', '-0.89', '+2.02', '+1.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -259.84\n",
      "['-0.56', '+0.41', '-1.11', '-1.55', '+3.93', '+2.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -494.48\n",
      "['-0.72', '+0.03', '-0.93', '-0.08', '+4.49', '-0.16', '+1.00', '+0.00']\n",
      "step 77 total_reward -579.32\n",
      "['-0.02', '+0.93', '-0.81', '-0.43', '+0.02', '+0.23', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.66\n",
      "['-0.18', '+0.72', '-0.90', '-0.84', '+0.32', '+0.29', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.93\n",
      "['-0.38', '+0.43', '-1.11', '-1.27', '+0.95', '+1.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -139.70\n",
      "['-0.65', '-0.01', '-1.42', '-1.75', '+1.95', '+1.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -305.87\n",
      "['-0.70', '-0.09', '-1.83', '-0.34', '+2.28', '+0.52', '+0.00', '+1.00']\n",
      "step 63 total_reward -415.11\n",
      "['+0.01', '+0.93', '+0.58', '-0.39', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.56\n",
      "['+0.12', '+0.73', '+0.54', '-0.93', '+0.03', '+0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.52\n",
      "['+0.24', '+0.37', '+0.72', '-1.47', '-0.30', '-0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -74.68\n",
      "['+0.40', '+0.05', '+0.73', '+0.14', '-1.44', '-1.33', '+1.00', '+0.00']\n",
      "step 60 total_reward -86.38\n",
      "['+0.55', '+0.03', '+0.78', '-0.27', '-2.24', '-0.37', '+0.00', '+0.00']\n",
      "step 80 total_reward -201.44\n",
      "['+0.64', '-0.02', '+0.58', '+0.02', '-2.37', '-1.75', '+1.00', '+0.00']\n",
      "step 91 total_reward -330.19\n",
      "['+0.00', '+0.94', '+0.08', '-0.12', '-0.00', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.03', '+0.82', '+0.22', '-0.66', '-0.45', '-0.62', '+0.00', '+0.00']\n",
      "step 20 total_reward -90.84\n",
      "['+0.08', '+0.54', '+0.29', '-1.25', '-1.22', '-1.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -199.73\n",
      "['+0.16', '+0.06', '+0.66', '-2.14', '-2.74', '-1.62', '+0.00', '+0.00']\n",
      "step 60 total_reward -412.23\n",
      "['+0.17', '+0.00', '+0.20', '+0.17', '-3.01', '-0.96', '+0.00', '+0.00']\n",
      "step 62 total_reward -522.95\n",
      "['+0.01', '+0.95', '+0.51', '+0.42', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.47\n",
      "['+0.10', '+0.99', '+0.32', '-0.13', '+0.31', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.51\n",
      "['+0.15', '+0.87', '+0.20', '-0.73', '+1.43', '+1.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -144.92\n",
      "['+0.18', '+0.58', '+0.15', '-1.23', '+2.74', '+1.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -296.02\n",
      "['+0.19', '+0.11', '+0.10', '-2.06', '+3.38', '+0.50', '+0.00', '+0.00']\n",
      "step 80 total_reward -405.85\n",
      "['+0.20', '+0.00', '+0.10', '+0.10', '+3.31', '-0.53', '+0.00', '+0.00']\n",
      "step 84 total_reward -519.53\n",
      "['+0.01', '+0.94', '+0.32', '+0.13', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.81\n",
      "['+0.05', '+0.90', '+0.14', '-0.42', '+0.37', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.50\n",
      "['+0.09', '+0.69', '+0.15', '-0.95', '+1.06', '+0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -141.90\n",
      "['+0.13', '+0.34', '+0.14', '-1.35', '+1.25', '-0.24', '+0.00', '+0.00']\n",
      "step 60 total_reward -167.34\n",
      "['+0.04', '+0.00', '-0.69', '+0.08', '+1.94', '+0.90', '+0.00', '+1.00']\n",
      "step 79 total_reward -259.72\n",
      "['+0.00', '+0.94', '+0.03', '-0.14', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.12\n",
      "['-0.01', '+0.81', '-0.09', '-0.68', '+0.37', '+0.49', '+0.00', '+0.00']\n",
      "step 20 total_reward -80.76\n",
      "['-0.05', '+0.57', '-0.60', '-0.78', '+0.86', '+0.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -140.25\n",
      "['-0.21', '+0.26', '-1.13', '-1.39', '+1.76', '+1.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -289.04\n",
      "['-0.32', '+0.06', '-0.61', '-0.05', '+2.25', '+3.71', '+0.00', '+0.00']\n",
      "step 69 total_reward -454.43\n",
      "['-0.01', '+0.92', '-0.44', '-0.59', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.34\n",
      "['-0.10', '+0.67', '-0.51', '-0.93', '+0.21', '+0.20', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.08\n",
      "['-0.22', '+0.36', '-0.72', '-1.32', '+0.68', '+0.84', '+0.00', '+0.00']\n",
      "step 40 total_reward -98.25\n",
      "['-0.41', '-0.05', '-0.91', '-1.26', '+1.68', '+5.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -205.36\n",
      "['-0.41', '-0.05', '-0.27', '+0.10', '+1.75', '+0.66', '+0.00', '+0.00']\n",
      "step 61 total_reward -305.36\n",
      "['-0.00', '+0.96', '-0.10', '+0.48', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.25\n",
      "['-0.04', '+1.01', '-0.29', '-0.08', '+0.49', '+0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.77\n",
      "['-0.10', '+0.90', '-0.38', '-0.71', '+1.83', '+1.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -209.16\n",
      "['-0.21', '+0.58', '-0.49', '-1.58', '+3.54', '+1.83', '+0.00', '+0.00']\n",
      "step 60 total_reward -438.14\n",
      "['-0.28', '+0.00', '-0.23', '-2.14', '+5.57', '+1.84', '+1.00', '+0.00']\n",
      "step 80 total_reward -648.67\n",
      "['-0.28', '-0.02', '-0.89', '-0.15', '+5.66', '+2.53', '+1.00', '+0.00']\n",
      "step 81 total_reward -748.67\n",
      "['-0.01', '+0.94', '-0.47', '+0.13', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.87\n",
      "['-0.12', '+0.90', '-0.66', '-0.44', '+0.58', '+0.94', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.49\n",
      "['-0.28', '+0.70', '-1.25', '-0.84', '+1.51', '+0.97', '+0.00', '+0.00']\n",
      "step 40 total_reward -237.25\n",
      "['-0.53', '+0.36', '-1.16', '-1.53', '+2.98', '+1.86', '+0.00', '+0.00']\n",
      "step 60 total_reward -414.98\n",
      "['-0.72', '-0.05', '-1.64', '-0.42', '+4.37', '+5.26', '+1.00', '+0.00']\n",
      "step 75 total_reward -683.40\n",
      "['-0.01', '+0.93', '-0.69', '-0.23', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.42\n",
      "['-0.16', '+0.78', '-0.78', '-0.77', '+0.49', '+0.55', '+0.00', '+0.00']\n",
      "step 20 total_reward -73.21\n",
      "['-0.36', '+0.53', '-1.09', '-1.12', '+1.23', '+1.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -179.99\n",
      "['-0.61', '+0.11', '-1.07', '-0.52', '+2.83', '+6.88', '+0.00', '+0.00']\n",
      "step 60 total_reward -473.39\n",
      "['+0.00', '+0.93', '+0.21', '-0.26', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.00\n",
      "['+0.04', '+0.77', '+0.15', '-0.79', '+0.21', '+0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.26\n",
      "['+0.07', '+0.49', '+0.21', '-1.10', '+0.35', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -71.89\n",
      "['+0.11', '+0.14', '+0.13', '-1.01', '-0.05', '-0.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -4.23\n",
      "['+0.12', '-0.03', '-0.02', '-0.65', '-0.04', '+4.11', '+1.00', '+1.00']\n",
      "step 71 total_reward -115.04\n",
      "['-0.00', '+0.94', '-0.14', '-0.12', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.49\n",
      "['-0.02', '+0.82', '-0.05', '-0.66', '-0.29', '-0.36', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.53\n",
      "['-0.00', '+0.59', '+0.39', '-0.70', '-0.65', '-0.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -97.21\n",
      "['+0.14', '+0.36', '+0.81', '-1.04', '-1.15', '-0.83', '+0.00', '+0.00']\n",
      "step 60 total_reward -180.56\n",
      "['+0.31', '+0.08', '+0.31', '+0.11', '-1.91', '-1.06', '+0.00', '+0.00']\n",
      "step 76 total_reward -425.24\n",
      "['+0.01', '+0.93', '+0.36', '-0.29', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.57\n",
      "['+0.07', '+0.76', '+0.30', '-0.82', '+0.18', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.03\n",
      "['+0.14', '+0.48', '+0.42', '-1.16', '+0.20', '-0.33', '+0.00', '+0.00']\n",
      "step 40 total_reward -55.01\n",
      "['+0.24', '+0.10', '+0.62', '-1.23', '-0.40', '-0.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -68.58\n",
      "['+0.28', '-0.03', '+0.60', '-0.57', '-0.32', '+4.07', '+1.00', '+0.00']\n",
      "step 67 total_reward -174.15\n",
      "['-0.00', '+0.94', '-0.05', '+0.03', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.77\n",
      "['-0.03', '+0.87', '-0.20', '-0.51', '+0.43', '+0.63', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.00\n",
      "['-0.09', '+0.65', '-0.70', '-0.78', '+1.06', '+0.63', '+0.00', '+0.00']\n",
      "step 40 total_reward -178.67\n",
      "['-0.27', '+0.35', '-1.01', '-1.35', '+2.08', '+1.44', '+0.00', '+0.00']\n",
      "step 60 total_reward -324.78\n",
      "['-0.49', '-0.05', '-1.72', '-1.03', '+3.36', '+5.80', '+0.00', '+0.00']\n",
      "step 77 total_reward -587.37\n",
      "['+0.01', '+0.94', '+0.66', '-0.05', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.17\n",
      "['+0.13', '+0.84', '+0.55', '-0.58', '+0.19', '+0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.39\n",
      "['+0.25', '+0.59', '+0.63', '-1.09', '+0.42', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -70.66\n",
      "['+0.39', '+0.19', '+0.77', '-1.51', '-0.04', '-0.71', '+0.00', '+0.00']\n",
      "step 60 total_reward -56.04\n",
      "['+0.45', '-0.05', '-0.21', '-0.45', '-0.14', '+4.63', '+1.00', '+0.00']\n",
      "step 70 total_reward -174.86\n",
      "['-0.01', '+0.93', '-0.51', '-0.51', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.34\n",
      "['-0.12', '+0.69', '-0.63', '-0.88', '+0.30', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.21\n",
      "['-0.27', '+0.41', '-0.88', '-1.20', '+0.79', '+0.87', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.18\n",
      "['-0.51', '+0.01', '-1.26', '-1.55', '+1.58', '+0.64', '+0.00', '+0.00']\n",
      "step 60 total_reward -250.97\n",
      "['-0.56', '-0.04', '-1.35', '-0.05', '+2.07', '+0.41', '+0.00', '+1.00']\n",
      "step 64 total_reward -383.50\n",
      "['-0.01', '+0.94', '-0.60', '+0.06', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.49\n",
      "['-0.15', '+0.88', '-0.78', '-0.49', '+0.61', '+0.90', '+0.00', '+0.00']\n",
      "step 20 total_reward -87.46\n",
      "['-0.34', '+0.67', '-1.29', '-0.91', '+1.51', '+1.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -232.24\n",
      "['-0.60', '+0.30', '-1.27', '-1.70', '+3.04', '+1.85', '+0.00', '+0.00']\n",
      "step 60 total_reward -432.63\n",
      "['-0.81', '-0.12', '-1.76', '-0.51', '+4.52', '+5.77', '+1.00', '+0.00']\n",
      "step 75 total_reward -698.45\n",
      "['-0.01', '+0.95', '-0.53', '+0.29', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.80\n",
      "['-0.13', '+0.95', '-0.72', '-0.27', '+0.58', '+0.96', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.71\n",
      "['-0.28', '+0.79', '-0.94', '-0.87', '+1.89', '+1.40', '+0.00', '+0.00']\n",
      "step 40 total_reward -247.16\n",
      "['-0.53', '+0.40', '-1.13', '-1.58', '+3.50', '+1.97', '+0.00', '+0.00']\n",
      "step 60 total_reward -459.45\n",
      "['-0.67', '+0.04', '-0.34', '+0.02', '+4.43', '+0.00', '+1.00', '+0.00']\n",
      "step 73 total_reward -703.06\n",
      "['-0.00', '+0.93', '-0.07', '-0.21', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.84\n",
      "['-0.03', '+0.79', '-0.17', '-0.75', '+0.37', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.11', '+0.56', '-0.67', '-0.78', '+0.83', '+0.58', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.47\n",
      "['-0.27', '+0.24', '-1.16', '-1.38', '+1.82', '+1.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -291.33\n",
      "['-0.36', '+0.09', '-0.41', '-0.09', '+2.31', '+3.66', '+0.00', '+0.00']\n",
      "step 68 total_reward -437.70\n",
      "['-0.00', '+0.94', '-0.17', '-0.17', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.47\n",
      "['-0.05', '+0.80', '-0.25', '-0.70', '+0.34', '+0.39', '+0.00', '+0.00']\n",
      "step 20 total_reward -73.25\n",
      "['-0.14', '+0.58', '-0.71', '-0.76', '+0.73', '+0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.63\n",
      "['-0.30', '+0.26', '-1.07', '-1.34', '+1.63', '+1.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -264.94\n",
      "['-0.46', '-0.07', '-0.75', '+0.03', '+2.30', '+2.24', '+0.00', '+0.00']\n",
      "step 75 total_reward -460.47\n",
      "['-0.00', '+0.94', '-0.12', '-0.13', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.59\n",
      "['-0.04', '+0.82', '-0.21', '-0.66', '+0.36', '+0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.07\n",
      "['-0.12', '+0.61', '-0.84', '-0.59', '+0.79', '+0.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -139.90\n",
      "['-0.30', '+0.34', '-1.04', '-1.23', '+1.72', '+1.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -276.34\n",
      "['-0.55', '-0.06', '-1.53', '-0.41', '+2.94', '+4.93', '+0.00', '+0.00']\n",
      "step 79 total_reward -533.91\n",
      "['-0.01', '+0.95', '-0.74', '+0.29', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.11\n",
      "['-0.18', '+0.95', '-0.93', '-0.27', '+0.63', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.58\n",
      "['-0.37', '+0.78', '-1.09', '-0.94', '+2.15', '+1.91', '+0.00', '+0.00']\n",
      "step 40 total_reward -272.40\n",
      "['-0.63', '+0.39', '-1.12', '-1.54', '+4.37', '+2.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -529.11\n",
      "['-0.83', '-0.18', '-1.15', '-2.10', '+7.03', '+2.95', '+0.00', '+1.00']\n",
      "step 80 total_reward -845.63\n",
      "['-0.83', '-0.21', '-0.43', '-1.01', '+7.23', '+9.16', '+0.00', '+1.00']\n",
      "step 81 total_reward -945.63\n",
      "['+0.02', '+0.95', '+0.79', '+0.42', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.85\n",
      "['+0.16', '+1.00', '+0.59', '-0.12', '+0.25', '+0.63', '+0.00', '+0.00']\n",
      "step 20 total_reward -0.12\n",
      "['+0.26', '+0.87', '+0.47', '-0.72', '+1.31', '+1.33', '+0.00', '+0.00']\n",
      "step 40 total_reward -121.84\n",
      "['+0.35', '+0.59', '+0.41', '-1.19', '+2.48', '+0.80', '+0.00', '+0.00']\n",
      "step 60 total_reward -256.65\n",
      "['+0.41', '+0.16', '+0.25', '-1.67', '+2.86', '+0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward -314.10\n",
      "['+0.44', '-0.10', '+0.51', '+0.02', '+3.21', '+2.87', '+0.00', '+0.00']\n",
      "step 90 total_reward -452.55\n",
      "['-0.01', '+0.94', '-0.40', '+0.02', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.19\n",
      "['-0.10', '+0.86', '-0.58', '-0.53', '+0.55', '+0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -87.24\n",
      "['-0.25', '+0.64', '-1.04', '-0.93', '+1.40', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -217.53\n",
      "['-0.46', '+0.27', '-1.13', '-1.72', '+2.83', '+1.73', '+0.00', '+0.00']\n",
      "step 60 total_reward -413.74\n",
      "['-0.55', '+0.07', '+0.11', '-0.04', '+3.42', '-0.38', '+0.00', '+0.00']\n",
      "step 68 total_reward -588.83\n",
      "['+0.01', '+0.95', '+0.38', '+0.18', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.65\n",
      "['+0.07', '+0.91', '+0.18', '-0.37', '+0.36', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.91\n",
      "['+0.10', '+0.72', '+0.15', '-0.92', '+1.26', '+0.92', '+0.00', '+0.00']\n",
      "step 40 total_reward -154.43\n",
      "['+0.13', '+0.39', '+0.15', '-1.33', '+1.70', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -207.20\n",
      "['+0.10', '-0.00', '-0.37', '-0.09', '+1.82', '-0.77', '+0.00', '+1.00']\n",
      "step 77 total_reward -310.20\n",
      "['-0.01', '+0.95', '-0.42', '+0.43', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.11\n",
      "['-0.11', '+1.00', '-0.61', '-0.13', '+0.56', '+0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.78\n",
      "['-0.24', '+0.87', '-0.68', '-0.76', '+1.96', '+1.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -232.78\n",
      "['-0.41', '+0.51', '-0.81', '-1.57', '+3.73', '+2.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -462.39\n",
      "['-0.53', '-0.07', '-0.54', '-2.16', '+5.81', '+2.05', '+1.00', '+0.00']\n",
      "step 80 total_reward -696.06\n",
      "['-0.53', '-0.09', '-0.42', '-1.39', '+6.04', '+8.34', '+1.00', '+0.00']\n",
      "step 81 total_reward -796.06\n",
      "['-0.01', '+0.93', '-0.58', '-0.40', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.46\n",
      "['-0.14', '+0.73', '-0.70', '-0.82', '+0.36', '+0.36', '+0.00', '+0.00']\n",
      "step 20 total_reward -55.70\n",
      "['-0.32', '+0.48', '-1.02', '-1.11', '+0.90', '+0.96', '+0.00', '+0.00']\n",
      "step 40 total_reward -139.40\n",
      "['-0.55', '+0.07', '-1.29', '-1.58', '+1.97', '+0.85', '+0.00', '+0.00']\n",
      "step 60 total_reward -299.81\n",
      "['-0.66', '-0.12', '-1.55', '-0.30', '+2.34', '-1.07', '+0.00', '+1.00']\n",
      "step 68 total_reward -461.34\n",
      "['-0.02', '+0.93', '-0.76', '-0.40', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.33\n",
      "['-0.17', '+0.73', '-0.80', '-0.85', '+0.19', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.26\n",
      "['-0.35', '+0.44', '-1.00', '-1.20', '+0.54', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -89.77\n",
      "['-0.58', '+0.04', '-1.24', '-1.56', '+1.19', '+0.38', '+0.00', '+0.00']\n",
      "step 60 total_reward -202.18\n",
      "['-0.65', '-0.09', '+0.10', '-0.10', '+1.36', '-0.52', '+0.00', '+1.00']\n",
      "step 66 total_reward -318.89\n",
      "['-0.01', '+0.94', '-0.30', '-0.06', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.58\n",
      "['-0.08', '+0.84', '-0.40', '-0.60', '+0.42', '+0.52', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.40\n",
      "['-0.21', '+0.64', '-1.04', '-0.68', '+0.95', '+0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -166.92\n",
      "['-0.42', '+0.35', '-1.20', '-1.36', '+2.04', '+1.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -321.04\n",
      "['-0.63', '-0.00', '-1.66', '-0.87', '+3.16', '+6.14', '+0.00', '+0.00']\n",
      "step 75 total_reward -561.05\n",
      "['+0.01', '+0.93', '+0.58', '-0.49', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.53\n",
      "['+0.12', '+0.70', '+0.56', '-1.02', '-0.03', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.07\n",
      "['+0.25', '+0.30', '+0.75', '-1.58', '-0.45', '-0.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -90.99\n",
      "['+0.33', '+0.08', '-0.02', '+0.01', '-1.52', '-0.16', '+1.00', '+0.00']\n",
      "step 51 total_reward -206.65\n",
      "['+0.01', '+0.93', '+0.40', '-0.46', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.92\n",
      "['+0.08', '+0.71', '+0.36', '-0.99', '+0.07', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.70\n",
      "['+0.17', '+0.33', '+0.55', '-1.53', '-0.23', '-0.68', '+0.00', '+0.00']\n",
      "step 40 total_reward -69.42\n",
      "['+0.26', '+0.01', '-0.01', '-0.26', '-0.39', '+1.64', '+1.00', '+0.00']\n",
      "step 60 total_reward +69.44\n",
      "['+0.26', '-0.01', '-0.06', '-0.31', '+0.05', '+1.86', '+0.00', '+1.00']\n",
      "step 65 total_reward -6.50\n",
      "['-0.00', '+0.94', '-0.20', '-0.16', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.77\n",
      "['-0.05', '+0.80', '-0.22', '-0.70', '+0.17', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.71\n",
      "['-0.11', '+0.57', '-0.49', '-0.69', '+0.33', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -63.53\n",
      "['-0.22', '+0.29', '-0.83', '-1.14', '+0.90', '+0.74', '+0.00', '+0.00']\n",
      "step 60 total_reward -156.57\n",
      "['-0.36', '+0.05', '-0.69', '+0.12', '+1.69', '+1.05', '+0.00', '+1.00']\n",
      "step 74 total_reward -309.60\n",
      "['-0.01', '+0.95', '-0.66', '+0.47', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.12\n",
      "['-0.16', '+1.01', '-0.85', '-0.10', '+0.62', '+0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -73.07\n",
      "['-0.34', '+0.89', '-0.89', '-0.75', '+2.08', '+1.90', '+0.00', '+0.00']\n",
      "step 40 total_reward -243.43\n",
      "['-0.51', '+0.58', '-0.73', '-1.27', '+4.47', '+2.80', '+0.00', '+0.00']\n",
      "step 60 total_reward -495.07\n",
      "['-0.64', '+0.14', '-0.66', '-1.56', '+7.49', '+2.90', '+0.00', '+0.00']\n",
      "step 80 total_reward -810.22\n",
      "['-0.72', '-0.14', '-0.41', '+0.01', '+9.39', '-0.00', '+0.00', '+0.00']\n",
      "step 92 total_reward -1120.94\n",
      "['-0.01', '+0.94', '-0.43', '+0.06', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.99\n",
      "['-0.11', '+0.88', '-0.58', '-0.49', '+0.53', '+0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -80.48\n",
      "['-0.26', '+0.68', '-1.12', '-0.83', '+1.29', '+0.83', '+0.00', '+0.00']\n",
      "step 40 total_reward -206.63\n",
      "['-0.49', '+0.33', '-1.20', '-1.57', '+2.62', '+1.69', '+0.00', '+0.00']\n",
      "step 60 total_reward -384.67\n",
      "['-0.68', '-0.06', '-0.28', '-0.22', '+3.95', '+5.17', '+1.00', '+0.00']\n",
      "step 75 total_reward -643.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.01', '+0.95', '-0.51', '+0.28', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.86\n",
      "['-0.13', '+0.95', '-0.70', '-0.28', '+0.60', '+0.96', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.96\n",
      "['-0.28', '+0.78', '-0.93', '-0.90', '+1.91', '+1.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -251.70\n",
      "['-0.52', '+0.40', '-1.11', '-1.55', '+3.59', '+2.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -466.22\n",
      "['-0.64', '+0.05', '+0.32', '+0.01', '+3.99', '-0.96', '+1.00', '+0.00']\n",
      "step 74 total_reward -553.86\n",
      "['-0.02', '+0.95', '-0.78', '+0.36', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.13\n",
      "['-0.19', '+0.97', '-0.97', '-0.21', '+0.66', '+1.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.63\n",
      "['-0.39', '+0.82', '-1.00', '-0.86', '+2.16', '+1.96', '+0.00', '+0.00']\n",
      "step 40 total_reward -258.54\n",
      "['-0.60', '+0.47', '-0.94', '-1.41', '+4.53', '+2.75', '+0.00', '+0.00']\n",
      "step 60 total_reward -518.39\n",
      "['-0.73', '+0.05', '-0.17', '+0.04', '+6.90', '+0.48', '+0.00', '+1.00']\n",
      "step 77 total_reward -864.51\n",
      "['-0.00', '+0.94', '-0.01', '+0.07', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.21\n",
      "['-0.02', '+0.88', '-0.19', '-0.48', '+0.45', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -80.89\n",
      "['-0.06', '+0.66', '-0.53', '-0.90', '+1.24', '+0.80', '+0.00', '+0.00']\n",
      "step 40 total_reward -193.67\n",
      "['-0.22', '+0.33', '-0.89', '-1.50', '+2.39', '+1.51', '+0.00', '+0.00']\n",
      "step 60 total_reward -354.13\n",
      "['-0.34', '+0.04', '-0.30', '-0.56', '+3.03', '-5.58', '+0.00', '+0.00']\n",
      "step 71 total_reward -569.45\n",
      "['-0.01', '+0.95', '-0.26', '+0.22', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.30\n",
      "['-0.07', '+0.93', '-0.45', '-0.33', '+0.53', '+0.88', '+0.00', '+0.00']\n",
      "step 20 total_reward -72.98\n",
      "['-0.17', '+0.75', '-0.66', '-0.89', '+1.59', '+1.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -218.40\n",
      "['-0.36', '+0.40', '-0.87', '-1.49', '+2.97', '+1.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -396.34\n",
      "['-0.47', '+0.08', '-0.44', '-0.25', '+4.11', '+5.29', '+1.00', '+0.00']\n",
      "step 72 total_reward -630.41\n",
      "['+0.01', '+0.93', '+0.50', '-0.39', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.47\n",
      "['+0.11', '+0.73', '+0.50', '-0.92', '-0.12', '-0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.99\n",
      "['+0.22', '+0.37', '+0.67', '-1.49', '-0.58', '-0.85', '+0.00', '+0.00']\n",
      "step 40 total_reward -108.38\n",
      "['+0.32', '+0.07', '-0.16', '-0.15', '-1.39', '+0.82', '+1.00', '+0.00']\n",
      "step 53 total_reward -274.74\n",
      "['-0.02', '+0.95', '-0.80', '+0.24', '+0.02', '+0.23', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.81\n",
      "['-0.19', '+0.93', '-0.99', '-0.33', '+0.67', '+1.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -89.00\n",
      "['-0.41', '+0.75', '-1.45', '-1.00', '+2.15', '+1.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -300.74\n",
      "['-0.70', '+0.35', '-1.29', '-1.52', '+4.32', '+2.55', '+0.00', '+0.00']\n",
      "step 60 total_reward -534.60\n",
      "['-0.87', '-0.02', '+0.24', '-0.75', '+5.99', '-7.00', '+0.00', '+1.00']\n",
      "step 75 total_reward -853.24\n",
      "['+0.00', '+0.93', '+0.17', '-0.36', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.20\n",
      "['+0.03', '+0.74', '+0.12', '-0.89', '+0.18', '+0.20', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.22\n",
      "['+0.03', '+0.50', '-0.07', '-0.80', '+0.33', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -36.11\n",
      "['+0.02', '+0.22', '-0.07', '-0.85', '-0.02', '-0.46', '+0.00', '+0.00']\n",
      "step 60 total_reward +14.82\n",
      "['+0.02', '+0.02', '+0.04', '-0.61', '-0.47', '+0.00', '+1.00', '+0.00']\n",
      "step 80 total_reward +18.75\n",
      "['+0.02', '-0.01', '-0.01', '-0.33', '-0.35', '+2.59', '+1.00', '+0.00']\n",
      "step 83 total_reward -73.49\n",
      "['-0.01', '+0.94', '-0.34', '-0.11', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.08\n",
      "['-0.09', '+0.82', '-0.44', '-0.65', '+0.43', '+0.50', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.40\n",
      "['-0.22', '+0.61', '-0.95', '-0.81', '+0.95', '+0.73', '+0.00', '+0.00']\n",
      "step 40 total_reward -159.22\n",
      "['-0.43', '+0.27', '-1.37', '-1.55', '+2.15', '+1.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -348.60\n",
      "['-0.61', '-0.06', '-1.72', '-0.85', '+3.08', '+6.48', '+0.00', '+0.00']\n",
      "step 73 total_reward -559.49\n",
      "['+0.01', '+0.95', '+0.31', '+0.25', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.85\n",
      "['+0.05', '+0.94', '+0.11', '-0.30', '+0.36', '+0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.02\n",
      "['+0.07', '+0.77', '+0.06', '-0.85', '+1.34', '+1.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -160.95\n",
      "['+0.06', '+0.44', '-0.08', '-1.30', '+2.06', '+0.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -246.16\n",
      "['-0.02', '+0.01', '-0.55', '-0.07', '+2.11', '-1.88', '+0.00', '+1.00']\n",
      "step 78 total_reward -412.92\n",
      "['-0.01', '+0.93', '-0.35', '-0.45', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.41\n",
      "['-0.09', '+0.71', '-0.45', '-0.90', '+0.34', '+0.38', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.57\n",
      "['-0.21', '+0.42', '-0.76', '-1.21', '+0.91', '+0.92', '+0.00', '+0.00']\n",
      "step 40 total_reward -135.53\n",
      "['-0.40', '+0.06', '+0.06', '-0.06', '+1.69', '-1.04', '+0.00', '+0.00']\n",
      "step 59 total_reward -373.62\n",
      "['+0.01', '+0.94', '+0.42', '-0.18', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.09\n",
      "['+0.08', '+0.80', '+0.34', '-0.71', '+0.20', '+0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.38\n",
      "['+0.16', '+0.50', '+0.46', '-1.22', '+0.26', '-0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -70.14\n",
      "['+0.26', '+0.09', '+0.59', '-1.44', '-0.34', '-0.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -80.82\n",
      "['+0.29', '-0.01', '+0.26', '-0.48', '-0.23', '+4.69', '+1.00', '+0.00']\n",
      "step 65 total_reward -178.43\n",
      "['-0.01', '+0.94', '-0.58', '-0.17', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.02\n",
      "['-0.14', '+0.80', '-0.66', '-0.71', '+0.44', '+0.50', '+0.00', '+0.00']\n",
      "step 20 total_reward -69.60\n",
      "['-0.33', '+0.60', '-1.18', '-0.85', '+1.02', '+0.85', '+0.00', '+0.00']\n",
      "step 40 total_reward -166.31\n",
      "['-0.59', '+0.25', '-1.45', '-1.55', '+2.35', '+1.47', '+0.00', '+0.00']\n",
      "step 60 total_reward -362.46\n",
      "['-0.78', '-0.07', '-0.57', '-0.36', '+3.11', '-4.81', '+0.00', '+0.00']\n",
      "step 73 total_reward -584.28\n",
      "['-0.00', '+0.95', '-0.13', '+0.32', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.29\n",
      "['-0.04', '+0.96', '-0.32', '-0.24', '+0.49', '+0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.28\n",
      "['-0.11', '+0.81', '-0.38', '-0.81', '+1.66', '+1.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -207.21\n",
      "['-0.27', '+0.45', '-0.87', '-1.57', '+3.00', '+1.62', '+0.00', '+0.00']\n",
      "step 60 total_reward -404.57\n",
      "['-0.39', '+0.08', '+0.07', '-0.04', '+4.17', '+1.09', '+1.00', '+0.00']\n",
      "step 74 total_reward -645.99\n",
      "['-0.01', '+0.95', '-0.70', '+0.22', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.20\n",
      "['-0.17', '+0.93', '-0.88', '-0.34', '+0.60', '+0.95', '+0.00', '+0.00']\n",
      "step 20 total_reward -80.32\n",
      "['-0.37', '+0.74', '-1.46', '-0.94', '+1.83', '+1.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -273.64\n",
      "['-0.67', '+0.37', '-1.38', '-1.54', '+3.55', '+2.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -473.37\n",
      "['-0.87', '-0.04', '-0.99', '-0.53', '+4.73', '-6.66', '+1.00', '+0.00']\n",
      "step 75 total_reward -754.87\n",
      "['+0.00', '+0.94', '+0.07', '-0.18', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.65\n",
      "['+0.00', '+0.80', '-0.05', '-0.72', '+0.36', '+0.49', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.76\n",
      "['-0.05', '+0.57', '-0.63', '-0.73', '+0.83', '+0.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -130.13\n",
      "['-0.20', '+0.27', '-1.13', '-1.33', '+1.74', '+1.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -278.67\n",
      "['-0.33', '+0.07', '-0.50', '-0.09', '+2.36', '+3.90', '+0.00', '+0.00']\n",
      "step 70 total_reward -449.83\n",
      "['+0.01', '+0.94', '+0.46', '-0.15', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.38\n",
      "['+0.09', '+0.81', '+0.37', '-0.68', '+0.22', '+0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.78\n",
      "['+0.17', '+0.52', '+0.48', '-1.19', '+0.35', '-0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -75.86\n",
      "['+0.28', '+0.10', '+0.55', '-1.47', '-0.20', '-0.58', '+0.00', '+0.00']\n",
      "step 60 total_reward -65.69\n",
      "['+0.32', '-0.08', '+1.15', '-0.35', '-0.43', '+0.00', '+1.00', '+1.00']\n",
      "step 68 total_reward -177.66\n",
      "['+0.01', '+0.92', '+0.38', '-0.58', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.99\n",
      "['+0.08', '+0.67', '+0.36', '-1.11', '+0.05', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.34\n",
      "['+0.17', '+0.25', '+0.62', '-1.48', '-0.32', '-0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -62.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.24', '+0.03', '+0.09', '-0.41', '-0.88', '-5.83', '+1.00', '+0.00']\n",
      "step 51 total_reward -182.50\n",
      "['+0.00', '+0.94', '+0.14', '-0.13', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.75\n",
      "['+0.02', '+0.81', '+0.02', '-0.67', '+0.34', '+0.47', '+0.00', '+0.00']\n",
      "step 20 total_reward -70.11\n",
      "['-0.01', '+0.59', '-0.50', '-0.71', '+0.81', '+0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.63\n",
      "['-0.18', '+0.36', '-1.00', '-1.02', '+1.51', '+1.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -228.70\n",
      "['-0.39', '+0.09', '-1.07', '+0.06', '+2.28', '+3.22', '+0.00', '+0.00']\n",
      "step 75 total_reward -467.18\n",
      "['-0.01', '+0.95', '-0.33', '+0.15', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.54\n",
      "['-0.09', '+0.91', '-0.52', '-0.41', '+0.54', '+0.92', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.13\n",
      "['-0.21', '+0.71', '-0.92', '-0.87', '+1.49', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -221.70\n",
      "['-0.40', '+0.35', '-0.90', '-1.56', '+2.89', '+1.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -394.97\n",
      "['-0.54', '+0.00', '-1.58', '-0.82', '+3.70', '-3.46', '+0.00', '+0.00']\n",
      "step 73 total_reward -638.84\n",
      "['-0.01', '+0.95', '-0.43', '+0.33', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.26\n",
      "['-0.11', '+0.97', '-0.63', '-0.23', '+0.59', '+0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -73.44\n",
      "['-0.24', '+0.81', '-0.74', '-0.82', '+1.94', '+1.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -240.07\n",
      "['-0.47', '+0.43', '-1.03', '-1.57', '+3.55', '+1.94', '+0.00', '+0.00']\n",
      "step 60 total_reward -460.39\n",
      "['-0.65', '-0.16', '-0.90', '-1.74', '+5.52', '+2.00', '+0.00', '+0.00']\n",
      "step 80 total_reward -669.14\n",
      "['-0.65', '-0.18', '-0.66', '-0.70', '+5.77', '+7.29', '+1.00', '+0.00']\n",
      "step 81 total_reward -769.14\n",
      "['-0.00', '+0.94', '-0.18', '-0.04', '+0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.66\n",
      "['-0.03', '+0.84', '-0.06', '-0.58', '-0.35', '-0.49', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.97\n",
      "['-0.02', '+0.62', '+0.37', '-0.73', '-0.85', '-0.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -119.96\n",
      "['+0.14', '+0.38', '+0.86', '-1.12', '-1.62', '-1.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -235.81\n",
      "['+0.30', '-0.01', '+0.94', '-0.06', '-3.24', '-0.00', '+0.00', '+0.00']\n",
      "step 79 total_reward -512.47\n",
      "['+0.00', '+0.94', '+0.15', '+0.03', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.60\n",
      "['+0.02', '+0.87', '+0.00', '-0.51', '+0.41', '+0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.75\n",
      "['+0.01', '+0.64', '-0.42', '-0.82', '+1.01', '+0.60', '+0.00', '+0.00']\n",
      "step 40 total_reward -149.00\n",
      "['-0.16', '+0.36', '-0.98', '-1.24', '+1.79', '+1.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -271.47\n",
      "['-0.37', '+0.02', '-1.37', '-0.21', '+2.74', '+4.43', '+0.00', '+0.00']\n",
      "step 76 total_reward -502.18\n",
      "['-0.01', '+0.95', '-0.57', '+0.24', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.05\n",
      "['-0.14', '+0.93', '-0.76', '-0.32', '+0.59', '+0.96', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.02\n",
      "['-0.31', '+0.75', '-1.31', '-0.92', '+1.83', '+1.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -269.71\n",
      "['-0.63', '+0.38', '-1.46', '-1.55', '+3.44', '+1.98', '+0.00', '+0.00']\n",
      "step 60 total_reward -476.80\n",
      "['-0.89', '-0.07', '-1.06', '+0.12', '+4.32', '-1.49', '+1.00', '+0.00']\n",
      "step 78 total_reward -605.58\n",
      "['+0.00', '+0.94', '+0.05', '-0.10', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.56\n",
      "['-0.00', '+0.82', '-0.06', '-0.64', '+0.35', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.83\n",
      "['-0.06', '+0.61', '-0.74', '-0.61', '+0.76', '+0.39', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.91\n",
      "['-0.24', '+0.36', '-1.03', '-1.13', '+1.50', '+1.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -249.02\n",
      "['-0.48', '+0.02', '-0.92', '-0.18', '+2.54', '+4.97', '+0.00', '+0.00']\n",
      "step 78 total_reward -494.14\n",
      "['+0.01', '+0.94', '+0.57', '-0.18', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.04\n",
      "['+0.11', '+0.80', '+0.49', '-0.71', '+0.14', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.66\n",
      "['+0.22', '+0.50', '+0.61', '-1.23', '+0.16', '-0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.85\n",
      "['+0.36', '+0.07', '+0.75', '-1.55', '-0.44', '-0.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -101.02\n",
      "['+0.42', '-0.11', '+0.76', '-0.76', '-0.33', '+5.38', '+1.00', '+1.00']\n",
      "step 68 total_reward -209.43\n",
      "['+0.00', '+0.94', '+0.08', '-0.05', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.22\n",
      "['+0.00', '+0.84', '-0.03', '-0.59', '+0.34', '+0.47', '+0.00', '+0.00']\n",
      "step 20 total_reward -74.35\n",
      "['-0.02', '+0.61', '-0.43', '-0.79', '+0.82', '+0.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.34\n",
      "['-0.16', '+0.32', '-0.93', '-1.25', '+1.49', '+0.98', '+0.00', '+0.00']\n",
      "step 60 total_reward -243.85\n",
      "['-0.35', '-0.01', '-1.21', '-0.28', '+2.04', '-2.97', '+0.00', '+1.00']\n",
      "step 76 total_reward -453.13\n",
      "['+0.01', '+0.94', '+0.39', '+0.12', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.48\n",
      "['+0.07', '+0.90', '+0.20', '-0.42', '+0.36', '+0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.66\n",
      "['+0.11', '+0.69', '+0.21', '-0.95', '+1.11', '+0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -140.54\n",
      "['+0.16', '+0.34', '+0.27', '-1.35', '+1.23', '-0.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -162.38\n",
      "['+0.15', '+0.00', '-0.50', '-0.19', '+1.17', '+4.40', '+0.00', '+1.00']\n",
      "step 76 total_reward -230.08\n",
      "['+0.01', '+0.95', '+0.47', '+0.19', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.16\n",
      "['+0.08', '+0.92', '+0.26', '-0.36', '+0.35', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.75\n",
      "['+0.14', '+0.73', '+0.25', '-0.90', '+1.17', '+0.82', '+0.00', '+0.00']\n",
      "step 40 total_reward -138.25\n",
      "['+0.20', '+0.40', '+0.28', '-1.31', '+1.49', '-0.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -181.90\n",
      "['+0.19', '-0.00', '-0.31', '+0.12', '+1.77', '+0.96', '+0.00', '+0.00']\n",
      "step 79 total_reward -244.35\n",
      "['+0.01', '+0.94', '+0.68', '-0.03', '-0.02', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.70\n",
      "['+0.15', '+0.85', '+0.68', '-0.57', '-0.17', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.27\n",
      "['+0.28', '+0.59', '+0.77', '-1.12', '-0.42', '-0.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -81.51\n",
      "['+0.45', '+0.17', '+0.89', '-1.75', '-1.42', '-1.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -223.98\n",
      "['+0.46', '+0.14', '-0.10', '-0.07', '-1.52', '+0.44', '+1.00', '+0.00']\n",
      "step 62 total_reward -324.49\n",
      "['+0.01', '+0.94', '+0.27', '-0.13', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.08\n",
      "['+0.06', '+0.81', '+0.27', '-0.66', '-0.07', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.65\n",
      "['+0.12', '+0.53', '+0.36', '-1.20', '-0.23', '-0.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -80.84\n",
      "['+0.22', '+0.12', '+0.96', '-1.42', '-1.00', '-0.87', '+0.00', '+0.00']\n",
      "step 60 total_reward -176.81\n",
      "['+0.28', '+0.02', '+0.39', '+0.19', '-1.62', '-1.16', '+1.00', '+0.00']\n",
      "step 66 total_reward -269.87\n",
      "['-0.01', '+0.93', '-0.38', '-0.48', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.07\n",
      "['-0.09', '+0.70', '-0.49', '-0.87', '+0.20', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.89\n",
      "['-0.22', '+0.45', '-0.76', '-1.04', '+0.55', '+0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -84.78\n",
      "['-0.41', '+0.11', '-0.12', '+0.02', '+1.44', '-0.16', '+0.00', '+1.00']\n",
      "step 60 total_reward -299.31\n",
      "['+0.01', '+0.95', '+0.47', '+0.40', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.35\n",
      "['+0.09', '+0.99', '+0.28', '-0.15', '+0.32', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -3.70\n",
      "['+0.13', '+0.86', '+0.18', '-0.73', '+1.41', '+1.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -144.53\n",
      "['+0.16', '+0.57', '+0.12', '-1.21', '+2.52', '+0.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -274.52\n",
      "['+0.16', '+0.11', '-0.10', '-1.97', '+2.90', '+0.24', '+0.00', '+0.00']\n",
      "step 80 total_reward -351.09\n",
      "['+0.16', '-0.00', '-0.24', '-0.92', '+3.08', '+7.31', '+0.00', '+0.00']\n",
      "step 84 total_reward -460.56\n",
      "['+0.01', '+0.92', '+0.59', '-0.56', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.51\n",
      "['+0.13', '+0.67', '+0.61', '-1.09', '-0.15', '-0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.56\n",
      "['+0.27', '+0.26', '+0.83', '-1.63', '-0.77', '-0.96', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.43\n",
      "['+0.37', '+0.03', '-0.04', '-0.03', '-1.42', '+0.19', '+1.00', '+0.00']\n",
      "step 50 total_reward -277.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.01', '+0.95', '+0.28', '+0.45', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.92\n",
      "['+0.05', '+1.00', '+0.09', '-0.10', '+0.37', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.55\n",
      "['+0.05', '+0.89', '-0.01', '-0.70', '+1.56', '+1.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -166.30\n",
      "['+0.03', '+0.58', '-0.20', '-1.42', '+2.97', '+1.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -350.83\n",
      "['+0.01', '+0.00', '-0.10', '-0.25', '+4.28', '+4.99', '+1.00', '+0.00']\n",
      "step 79 total_reward -631.84\n",
      "['+0.00', '+0.94', '+0.05', '+0.04', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.50\n",
      "['-0.00', '+0.87', '-0.11', '-0.50', '+0.42', '+0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.63\n",
      "['-0.04', '+0.66', '-0.54', '-0.84', '+1.08', '+0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -173.16\n",
      "['-0.22', '+0.35', '-1.01', '-1.38', '+1.98', '+1.30', '+0.00', '+0.00']\n",
      "step 60 total_reward -311.88\n",
      "['-0.43', '-0.03', '-1.65', '-0.67', '+3.09', '+5.81', '+0.00', '+0.00']\n",
      "step 76 total_reward -552.01\n",
      "['-0.01', '+0.94', '-0.57', '+0.04', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.63\n",
      "['-0.14', '+0.87', '-0.72', '-0.52', '+0.58', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.71\n",
      "['-0.31', '+0.66', '-1.08', '-0.91', '+1.40', '+1.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -206.29\n",
      "['-0.53', '+0.29', '-1.12', '-1.74', '+2.95', '+1.81', '+0.00', '+0.00']\n",
      "step 60 total_reward -415.58\n",
      "['-0.63', '+0.07', '-0.68', '-0.09', '+3.57', '-2.08', '+0.00', '+0.00']\n",
      "step 68 total_reward -593.24\n",
      "['-0.01', '+0.94', '-0.54', '-0.05', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.73\n",
      "['-0.13', '+0.84', '-0.65', '-0.59', '+0.51', '+0.61', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.73\n",
      "['-0.30', '+0.61', '-1.06', '-0.96', '+1.15', '+0.88', '+0.00', '+0.00']\n",
      "step 40 total_reward -181.70\n",
      "['-0.52', '+0.23', '-1.29', '-1.64', '+2.53', '+1.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -375.01\n",
      "['-0.67', '-0.04', '-1.04', '-0.02', '+3.17', '-0.00', '+0.00', '+0.00']\n",
      "step 71 total_reward -577.29\n",
      "['+0.01', '+0.93', '+0.48', '-0.23', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.32\n",
      "['+0.10', '+0.78', '+0.41', '-0.76', '+0.14', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.42\n",
      "['+0.19', '+0.47', '+0.55', '-1.28', '+0.11', '-0.39', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.43\n",
      "['+0.29', '+0.06', '-0.06', '-0.76', '-0.55', '-0.81', '+0.00', '+0.00']\n",
      "step 60 total_reward -16.54\n",
      "['+0.28', '+0.03', '-0.53', '-0.28', '-0.20', '+2.61', '+1.00', '+0.00']\n",
      "step 64 total_reward -67.90\n",
      "['-0.01', '+0.93', '-0.57', '-0.45', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.49\n",
      "['-0.13', '+0.71', '-0.69', '-0.84', '+0.34', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -51.81\n",
      "['-0.30', '+0.42', '-0.93', '-1.25', '+0.93', '+0.99', '+0.00', '+0.00']\n",
      "step 40 total_reward -138.69\n",
      "['-0.55', '+0.01', '-1.97', '-0.49', '+2.46', '+1.92', '+0.00', '+1.00']\n",
      "step 60 total_reward -405.10\n",
      "['+0.00', '+0.92', '+0.05', '-0.53', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.65\n",
      "['+0.01', '+0.69', '-0.00', '-0.91', '+0.13', '+0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.70\n",
      "['-0.02', '+0.47', '-0.31', '-0.49', '+0.23', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward +8.55\n",
      "['-0.10', '+0.29', '-0.61', '-0.75', '+0.57', '+0.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -50.13\n",
      "['-0.29', '+0.06', '-0.99', '-0.90', '+0.93', '-0.03', '+0.00', '+0.00']\n",
      "step 80 total_reward -124.66\n",
      "['-0.43', '+0.02', '-0.92', '-0.14', '+2.69', '+3.33', '+0.00', '+0.00']\n",
      "step 95 total_reward -366.37\n",
      "['+0.01', '+0.95', '+0.72', '+0.40', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.82\n",
      "['+0.14', '+0.99', '+0.53', '-0.14', '+0.26', '+0.65', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.10\n",
      "['+0.24', '+0.86', '+0.42', '-0.72', '+1.29', '+1.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -122.69\n",
      "['+0.32', '+0.58', '+0.37', '-1.18', '+2.32', '+0.62', '+0.00', '+0.00']\n",
      "step 60 total_reward -242.99\n",
      "['+0.38', '+0.15', '+0.25', '-1.65', '+2.50', '-0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -279.04\n",
      "['+0.40', '-0.08', '+0.63', '-0.74', '+2.62', '+5.81', '+0.00', '+0.00']\n",
      "step 89 total_reward -408.65\n",
      "['-0.01', '+0.93', '-0.73', '-0.31', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.10\n",
      "['-0.17', '+0.76', '-0.84', '-0.80', '+0.51', '+0.58', '+0.00', '+0.00']\n",
      "step 20 total_reward -73.89\n",
      "['-0.38', '+0.51', '-1.17', '-1.12', '+1.33', '+1.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -189.56\n",
      "['-0.61', '+0.15', '-1.91', '-0.36', '+2.58', '+1.78', '+0.00', '+0.00']\n",
      "step 57 total_reward -471.55\n",
      "['+0.00', '+0.93', '+0.16', '-0.41', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.24\n",
      "['+0.03', '+0.72', '+0.10', '-0.90', '+0.20', '+0.23', '+0.00', '+0.00']\n",
      "step 20 total_reward -48.11\n",
      "['+0.02', '+0.50', '-0.26', '-0.64', '+0.48', '+0.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.88\n",
      "['-0.10', '+0.33', '-0.84', '-0.71', '+0.85', '+0.63', '+0.00', '+0.00']\n",
      "step 60 total_reward -103.49\n",
      "['-0.38', '+0.11', '-1.56', '-0.90', '+1.33', '+0.14', '+0.00', '+1.00']\n",
      "step 80 total_reward -220.68\n",
      "['-0.45', '+0.09', '-1.37', '-0.02', '+2.28', '+2.50', '+0.00', '+0.00']\n",
      "step 85 total_reward -359.21\n",
      "['-0.01', '+0.93', '-0.67', '-0.26', '+0.02', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.16\n",
      "['-0.15', '+0.77', '-0.67', '-0.79', '+0.17', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.27\n",
      "['-0.30', '+0.57', '-0.88', '-0.83', '+0.45', '+0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -68.28\n",
      "['-0.50', '+0.24', '-1.31', '-1.30', '+1.43', '+1.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -223.12\n",
      "['-0.66', '-0.01', '-1.20', '-0.45', '+1.88', '-4.16', '+0.00', '+1.00']\n",
      "step 72 total_reward -399.32\n",
      "['+0.02', '+0.95', '+0.76', '+0.30', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.91\n",
      "['+0.15', '+0.96', '+0.56', '-0.24', '+0.29', '+0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.50\n",
      "['+0.26', '+0.80', '+0.49', '-0.80', '+1.26', '+1.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.71\n",
      "['+0.35', '+0.50', '+0.47', '-1.22', '+1.92', '+0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -205.59\n",
      "['+0.44', '+0.07', '+0.43', '-1.69', '+1.65', '-0.34', '+0.00', '+0.00']\n",
      "step 80 total_reward -205.57\n",
      "['+0.46', '-0.03', '+0.03', '+0.06', '+1.96', '+0.66', '+0.00', '+1.00']\n",
      "step 85 total_reward -265.88\n",
      "['+0.01', '+0.93', '+0.68', '-0.27', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.01\n",
      "['+0.14', '+0.77', '+0.62', '-0.80', '+0.09', '+0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.73\n",
      "['+0.28', '+0.45', '+0.79', '-1.33', '-0.10', '-0.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.88\n",
      "['+0.45', '-0.03', '+0.89', '-1.79', '-0.92', '-0.63', '+1.00', '+0.00']\n",
      "step 60 total_reward -161.03\n",
      "['+0.47', '-0.07', '+1.36', '-0.95', '-0.84', '+4.05', '+1.00', '+0.00']\n",
      "step 62 total_reward -258.43\n",
      "['+0.00', '+0.95', '+0.11', '+0.18', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.55\n",
      "['+0.01', '+0.91', '-0.10', '-0.38', '+0.45', '+0.85', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.53\n",
      "['-0.02', '+0.72', '-0.31', '-0.88', '+1.43', '+0.99', '+0.00', '+0.00']\n",
      "step 40 total_reward -192.27\n",
      "['-0.17', '+0.38', '-0.81', '-1.47', '+2.59', '+1.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -355.97\n",
      "['-0.31', '-0.01', '-1.01', '-0.58', '+3.59', '-4.99', '+0.00', '+0.00']\n",
      "step 74 total_reward -598.63\n",
      "['-0.01', '+0.94', '-0.32', '+0.03', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.45\n",
      "['-0.08', '+0.87', '-0.46', '-0.51', '+0.51', '+0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.44\n",
      "['-0.20', '+0.65', '-0.94', '-0.86', '+1.19', '+0.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -192.46\n",
      "['-0.39', '+0.30', '-1.04', '-1.56', '+2.42', '+1.56', '+0.00', '+0.00']\n",
      "step 60 total_reward -357.93\n",
      "['-0.58', '-0.11', '-1.36', '-0.19', '+3.34', '-0.01', '+0.00', '+0.00']\n",
      "step 76 total_reward -607.51\n",
      "['-0.01', '+0.95', '-0.42', '+0.22', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.57\n",
      "['-0.11', '+0.93', '-0.62', '-0.34', '+0.57', '+0.94', '+0.00', '+0.00']\n",
      "step 20 total_reward -78.29\n",
      "['-0.24', '+0.75', '-1.01', '-0.88', '+1.66', '+1.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -238.19\n",
      "['-0.48', '+0.39', '-1.08', '-1.50', '+3.16', '+1.92', '+0.00', '+0.00']\n",
      "step 60 total_reward -423.51\n",
      "['-0.65', '-0.03', '-1.34', '-0.97', '+4.60', '+7.75', '+1.00', '+0.00']\n",
      "step 75 total_reward -700.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.01', '+0.93', '+0.38', '-0.42', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.61\n",
      "['+0.08', '+0.72', '+0.33', '-0.95', '+0.14', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.65\n",
      "['+0.16', '+0.35', '+0.50', '-1.47', '-0.00', '-0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -45.37\n",
      "['+0.24', '+0.01', '-0.29', '-0.27', '-0.29', '+3.14', '+1.00', '+0.00']\n",
      "step 56 total_reward -148.55\n",
      "['+0.00', '+0.93', '+0.16', '-0.50', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.39\n",
      "['+0.03', '+0.70', '+0.08', '-0.90', '+0.14', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.49\n",
      "['+0.04', '+0.45', '+0.05', '-0.89', '+0.23', '-0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -18.72\n",
      "['+0.04', '+0.19', '-0.04', '-0.73', '+0.00', '-0.25', '+0.00', '+0.00']\n",
      "step 60 total_reward +42.22\n",
      "['+0.04', '+0.01', '-0.02', '-0.64', '-0.19', '+0.03', '+1.00', '+0.00']\n",
      "step 80 total_reward +52.95\n",
      "['+0.04', '-0.03', '-0.00', '+0.00', '-0.00', '+0.00', '+1.00', '+1.00']\n",
      "step 85 total_reward -20.73\n",
      "['-0.00', '+0.95', '-0.13', '+0.35', '+0.00', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.25\n",
      "['-0.04', '+0.97', '-0.32', '-0.21', '+0.47', '+0.88', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.55\n",
      "['-0.11', '+0.82', '-0.39', '-0.79', '+1.70', '+1.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -208.63\n",
      "['-0.26', '+0.47', '-0.77', '-1.55', '+3.11', '+1.67', '+0.00', '+0.00']\n",
      "step 60 total_reward -408.00\n",
      "['-0.40', '-0.09', '-1.57', '-1.11', '+4.80', '+6.86', '+1.00', '+0.00']\n",
      "step 79 total_reward -709.54\n",
      "['+0.01', '+0.93', '+0.53', '-0.46', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.62\n",
      "['+0.11', '+0.71', '+0.49', '-0.99', '+0.06', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.10\n",
      "['+0.22', '+0.33', '+0.68', '-1.53', '-0.26', '-0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -69.96\n",
      "['+0.34', '-0.02', '+0.69', '-0.31', '-0.66', '+3.40', '+0.00', '+0.00']\n",
      "step 55 total_reward -201.67\n",
      "['-0.01', '+0.94', '-0.46', '+0.12', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.00\n",
      "['-0.12', '+0.90', '-0.66', '-0.44', '+0.59', '+0.96', '+0.00', '+0.00']\n",
      "step 20 total_reward -86.70\n",
      "['-0.28', '+0.70', '-1.27', '-0.89', '+1.58', '+1.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -249.44\n",
      "['-0.54', '+0.34', '-1.22', '-1.59', '+3.09', '+1.82', '+0.00', '+0.00']\n",
      "step 60 total_reward -434.63\n",
      "['-0.70', '+0.02', '-1.91', '-0.79', '+3.94', '-2.05', '+0.00', '+0.00']\n",
      "step 72 total_reward -668.29\n",
      "['-0.01', '+0.94', '-0.60', '+0.06', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.79\n",
      "['-0.15', '+0.88', '-0.79', '-0.50', '+0.62', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -90.67\n",
      "['-0.33', '+0.66', '-1.18', '-0.99', '+1.67', '+1.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -242.36\n",
      "['-0.57', '+0.26', '-1.15', '-1.83', '+3.34', '+1.84', '+0.00', '+0.00']\n",
      "step 60 total_reward -462.01\n",
      "['-0.67', '+0.04', '-1.63', '-0.34', '+4.02', '-0.16', '+0.00', '+0.00']\n",
      "step 68 total_reward -643.44\n",
      "['+0.00', '+0.93', '+0.24', '-0.33', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.15\n",
      "['+0.04', '+0.75', '+0.19', '-0.86', '+0.15', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.85\n",
      "['+0.09', '+0.45', '+0.28', '-1.22', '+0.14', '-0.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.12\n",
      "['+0.16', '+0.08', '+0.46', '-1.11', '-0.41', '-0.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -51.43\n",
      "['+0.18', '-0.02', '+0.68', '-0.70', '-0.40', '+3.55', '+1.00', '+0.00']\n",
      "step 66 total_reward -155.19\n",
      "['+0.01', '+0.95', '+0.35', '+0.15', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.37\n",
      "['+0.06', '+0.91', '+0.16', '-0.40', '+0.34', '+0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.82\n",
      "['+0.10', '+0.71', '+0.15', '-0.94', '+1.09', '+0.75', '+0.00', '+0.00']\n",
      "step 40 total_reward -141.14\n",
      "['+0.14', '+0.36', '+0.21', '-1.34', '+1.32', '-0.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -173.22\n",
      "['+0.07', '-0.00', '-0.88', '-0.49', '+1.59', '+5.87', '+0.00', '+1.00']\n",
      "step 78 total_reward -222.24\n",
      "['+0.01', '+0.95', '+0.62', '+0.38', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.20\n",
      "['+0.12', '+0.98', '+0.44', '-0.16', '+0.27', '+0.64', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.62\n",
      "['+0.20', '+0.85', '+0.33', '-0.74', '+1.30', '+1.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -128.39\n",
      "['+0.26', '+0.56', '+0.28', '-1.20', '+2.27', '+0.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -241.98\n",
      "['+0.30', '+0.12', '+0.04', '-1.77', '+2.40', '-0.00', '+0.00', '+0.00']\n",
      "step 80 total_reward -281.47\n",
      "['+0.30', '-0.01', '+0.23', '-0.19', '+2.50', '+3.19', '+0.00', '+0.00']\n",
      "step 85 total_reward -394.48\n",
      "['+0.01', '+0.93', '+0.59', '-0.34', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.33\n",
      "['+0.13', '+0.74', '+0.59', '-0.87', '-0.15', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.37\n",
      "['+0.26', '+0.40', '+0.75', '-1.44', '-0.59', '-0.82', '+0.00', '+0.00']\n",
      "step 40 total_reward -107.61\n",
      "['+0.37', '+0.07', '+0.29', '+0.16', '-1.68', '-1.13', '+1.00', '+0.00']\n",
      "step 55 total_reward -252.62\n",
      "['-0.01', '+0.95', '-0.42', '+0.23', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.66\n",
      "['-0.11', '+0.93', '-0.60', '-0.33', '+0.52', '+0.88', '+0.00', '+0.00']\n",
      "step 20 total_reward -72.23\n",
      "['-0.24', '+0.75', '-0.95', '-0.88', '+1.62', '+1.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -228.81\n",
      "['-0.46', '+0.40', '-1.01', '-1.49', '+3.08', '+1.87', '+0.00', '+0.00']\n",
      "step 60 total_reward -408.97\n",
      "['-0.60', '+0.04', '-0.84', '-0.72', '+4.32', '+7.82', '+0.00', '+0.00']\n",
      "step 73 total_reward -661.10\n",
      "['-0.01', '+0.93', '-0.58', '-0.50', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.32\n",
      "['-0.13', '+0.70', '-0.70', '-0.79', '+0.32', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.34\n",
      "['-0.30', '+0.47', '-0.96', '-1.04', '+0.89', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -122.17\n",
      "['-0.54', '+0.09', '-1.36', '-1.52', '+1.96', '+0.95', '+0.00', '+0.00']\n",
      "step 60 total_reward -292.15\n",
      "['-0.57', '+0.06', '-0.51', '+0.10', '+2.05', '+0.85', '+0.00', '+1.00']\n",
      "step 62 total_reward -400.32\n",
      "['-0.01', '+0.93', '-0.52', '-0.21', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.25\n",
      "['-0.12', '+0.79', '-0.59', '-0.75', '+0.39', '+0.42', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.81\n",
      "['-0.28', '+0.55', '-0.94', '-0.96', '+0.92', '+0.83', '+0.00', '+0.00']\n",
      "step 40 total_reward -142.60\n",
      "['-0.49', '+0.18', '-1.26', '-1.53', '+2.10', '+1.12', '+0.00', '+0.00']\n",
      "step 60 total_reward -316.43\n",
      "['-0.65', '-0.11', '-1.34', '-0.51', '+2.87', '+6.25', '+0.00', '+0.00']\n",
      "step 72 total_reward -508.78\n",
      "['-0.00', '+0.95', '-0.04', '+0.41', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.58\n",
      "['-0.03', '+0.99', '-0.24', '-0.14', '+0.48', '+0.85', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.84\n",
      "['-0.08', '+0.86', '-0.32', '-0.77', '+1.77', '+1.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -209.22\n",
      "['-0.19', '+0.51', '-0.60', '-1.63', '+3.42', '+1.85', '+0.00', '+0.00']\n",
      "step 60 total_reward -435.29\n",
      "['-0.26', '+0.01', '+0.21', '-0.19', '+5.04', '-2.86', '+1.00', '+0.00']\n",
      "step 77 total_reward -700.47\n",
      "['-0.00', '+0.93', '-0.20', '-0.31', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.58\n",
      "['-0.05', '+0.75', '-0.25', '-0.85', '+0.26', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.28\n",
      "['-0.14', '+0.54', '-0.64', '-0.77', '+0.57', '+0.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -88.98\n",
      "['-0.30', '+0.22', '-1.03', '-1.30', '+1.53', '+1.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -234.06\n",
      "['-0.44', '-0.06', '-0.96', '+0.03', '+2.19', '+1.18', '+0.00', '+1.00']\n",
      "step 74 total_reward -410.37\n",
      "['-0.00', '+0.93', '-0.22', '-0.34', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.76\n",
      "['-0.05', '+0.75', '-0.27', '-0.87', '+0.29', '+0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.19\n",
      "['-0.16', '+0.52', '-0.71', '-0.85', '+0.63', '+0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -101.49\n",
      "['-0.33', '+0.19', '-1.10', '-1.29', '+1.54', '+0.88', '+0.00', '+0.00']\n",
      "step 60 total_reward -237.01\n",
      "['-0.42', '+0.04', '-0.90', '-0.29', '+1.71', '-3.05', '+0.00', '+1.00']\n",
      "step 68 total_reward -368.35\n",
      "['-0.00', '+0.94', '-0.15', '-0.06', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.09\n",
      "['-0.05', '+0.84', '-0.26', '-0.60', '+0.41', '+0.51', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.68\n",
      "['-0.13', '+0.62', '-0.76', '-0.74', '+0.92', '+0.53', '+0.00', '+0.00']\n",
      "step 40 total_reward -157.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.29', '+0.31', '-0.98', '-1.40', '+1.95', '+1.38', '+0.00', '+0.00']\n",
      "step 60 total_reward -304.62\n",
      "['-0.46', '-0.04', '-1.48', '-0.48', '+2.93', '+4.93', '+0.00', '+0.00']\n",
      "step 75 total_reward -523.51\n",
      "['-0.01', '+0.93', '-0.34', '-0.46', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.70\n",
      "['-0.08', '+0.71', '-0.34', '-0.89', '+0.09', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.55\n",
      "['-0.16', '+0.45', '-0.52', '-1.02', '+0.30', '+0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.95\n",
      "['-0.30', '+0.12', '-0.85', '-1.23', '+0.92', '+0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -131.11\n",
      "['-0.37', '+0.05', '-0.56', '+0.11', '+2.17', '+2.32', '+0.00', '+0.00']\n",
      "step 70 total_reward -279.25\n",
      "['+0.02', '+0.95', '+0.78', '+0.41', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.68\n",
      "['+0.15', '+0.99', '+0.59', '-0.13', '+0.26', '+0.64', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.46\n",
      "['+0.26', '+0.87', '+0.47', '-0.72', '+1.29', '+1.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -120.98\n",
      "['+0.35', '+0.58', '+0.42', '-1.19', '+2.37', '+0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -246.74\n",
      "['+0.42', '+0.16', '+0.29', '-1.66', '+2.63', '+0.01', '+0.00', '+0.00']\n",
      "step 80 total_reward -292.66\n",
      "['+0.43', '+0.04', '-0.38', '-0.08', '+2.70', '+1.24', '+0.00', '+0.00']\n",
      "step 85 total_reward -406.37\n",
      "['+0.00', '+0.93', '+0.15', '-0.27', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.22\n",
      "['+0.02', '+0.77', '+0.07', '-0.80', '+0.26', '+0.31', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.64\n",
      "['-0.00', '+0.55', '-0.45', '-0.61', '+0.56', '+0.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -68.22\n",
      "['-0.15', '+0.36', '-0.92', '-0.81', '+0.97', '+0.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -145.21\n",
      "['-0.42', '+0.09', '-1.49', '-1.13', '+1.66', '+0.43', '+0.00', '+0.00']\n",
      "step 80 total_reward -284.27\n",
      "['-0.46', '+0.04', '-0.34', '+0.05', '+1.69', '-1.20', '+0.00', '+1.00']\n",
      "step 83 total_reward -395.65\n",
      "['+0.00', '+0.93', '+0.11', '-0.40', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.50\n",
      "['+0.02', '+0.72', '+0.07', '-0.94', '+0.17', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.77\n",
      "['-0.00', '+0.52', '-0.33', '-0.50', '+0.34', '+0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -18.71\n",
      "['-0.11', '+0.40', '-0.67', '-0.57', '+0.62', '+0.58', '+0.00', '+0.00']\n",
      "step 60 total_reward -67.35\n",
      "['-0.30', '+0.19', '-1.19', '-0.86', '+1.40', '+0.64', '+0.00', '+0.00']\n",
      "step 80 total_reward -201.39\n",
      "['-0.45', '+0.00', '-0.72', '+0.07', '+1.79', '-0.38', '+0.00', '+1.00']\n",
      "step 93 total_reward -344.70\n",
      "['+0.00', '+0.95', '+0.07', '+0.19', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.44\n",
      "['-0.00', '+0.92', '-0.13', '-0.37', '+0.42', '+0.81', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.54\n",
      "['-0.02', '+0.73', '-0.23', '-0.87', '+1.23', '+0.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -170.79\n",
      "['-0.18', '+0.41', '-0.92', '-1.40', '+2.18', '+1.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -317.48\n",
      "['-0.37', '-0.02', '-1.15', '-0.08', '+3.24', '-0.00', '+0.00', '+0.00']\n",
      "step 77 total_reward -571.17\n",
      "['+0.00', '+0.92', '+0.09', '-0.54', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.54\n",
      "['+0.01', '+0.68', '-0.01', '-0.95', '+0.18', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.36\n",
      "['-0.02', '+0.45', '-0.36', '-0.60', '+0.32', '+0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -8.45\n",
      "['-0.14', '+0.24', '-0.90', '-0.70', '+0.73', '+0.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -80.03\n",
      "['-0.36', '+0.06', '-0.74', '+0.18', '+1.78', '+0.83', '+0.00', '+0.00']\n",
      "step 80 total_reward -291.93\n",
      "['-0.01', '+0.95', '-0.75', '+0.44', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.24\n",
      "['-0.18', '+1.00', '-0.94', '-0.12', '+0.65', '+1.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.33\n",
      "['-0.37', '+0.88', '-0.98', '-0.78', '+2.19', '+2.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -257.81\n",
      "['-0.57', '+0.55', '-0.83', '-1.28', '+4.62', '+2.81', '+0.00', '+0.00']\n",
      "step 60 total_reward -513.34\n",
      "['-0.74', '+0.14', '-1.53', '-0.34', '+8.03', '+5.36', '+0.00', '+1.00']\n",
      "step 80 total_reward -844.60\n",
      "['-1.01', '-0.01', '-1.84', '-0.34', '+12.28', '+2.35', '+1.00', '+0.00']\n",
      "step 97 total_reward -1407.98\n",
      "['-0.01', '+0.95', '-0.26', '+0.23', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.40\n",
      "['-0.07', '+0.93', '-0.46', '-0.32', '+0.50', '+0.90', '+0.00', '+0.00']\n",
      "step 20 total_reward -69.76\n",
      "['-0.17', '+0.76', '-0.69', '-0.85', '+1.61', '+1.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -219.56\n",
      "['-0.36', '+0.41', '-0.87', '-1.45', '+3.06', '+1.81', '+0.00', '+0.00']\n",
      "step 60 total_reward -403.50\n",
      "['-0.53', '-0.17', '-0.80', '-2.17', '+4.85', '+1.94', '+1.00', '+0.00']\n",
      "step 80 total_reward -637.55\n",
      "['-0.55', '-0.19', '-1.85', '-0.37', '+5.11', '+4.26', '+1.00', '+0.00']\n",
      "step 81 total_reward -737.55\n",
      "['-0.01', '+0.93', '-0.55', '-0.28', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.18\n",
      "['-0.13', '+0.76', '-0.61', '-0.82', '+0.39', '+0.41', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.74\n",
      "['-0.29', '+0.51', '-0.96', '-1.09', '+0.92', '+0.87', '+0.00', '+0.00']\n",
      "step 40 total_reward -145.07\n",
      "['-0.52', '+0.10', '-1.30', '-1.62', '+2.01', '+0.83', '+0.00', '+0.00']\n",
      "step 60 total_reward -312.53\n",
      "['-0.65', '-0.15', '-1.07', '-0.26', '+2.60', '+5.53', '+0.00', '+0.00']\n",
      "step 70 total_reward -489.01\n",
      "['-0.01', '+0.95', '-0.46', '+0.39', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.21\n",
      "['-0.12', '+0.99', '-0.65', '-0.17', '+0.56', '+0.94', '+0.00', '+0.00']\n",
      "step 20 total_reward -66.04\n",
      "['-0.25', '+0.85', '-0.72', '-0.80', '+1.96', '+1.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -236.36\n",
      "['-0.43', '+0.50', '-0.77', '-1.47', '+3.86', '+2.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -463.90\n",
      "['-0.52', '+0.07', '+0.09', '-0.19', '+6.05', '-0.89', '+0.00', '+1.00']\n",
      "step 77 total_reward -746.05\n",
      "['+0.01', '+0.95', '+0.54', '+0.45', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.24\n",
      "['+0.10', '+1.01', '+0.35', '-0.09', '+0.29', '+0.68', '+0.00', '+0.00']\n",
      "step 20 total_reward +0.89\n",
      "['+0.16', '+0.89', '+0.24', '-0.69', '+1.38', '+1.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -134.23\n",
      "['+0.21', '+0.62', '+0.19', '-1.18', '+2.65', '+0.98', '+0.00', '+0.00']\n",
      "step 60 total_reward -282.41\n",
      "['+0.22', '+0.17', '+0.05', '-1.90', '+3.27', '+0.45', '+0.00', '+0.00']\n",
      "step 80 total_reward -379.92\n",
      "['+0.23', '+0.00', '+0.32', '-0.69', '+3.29', '-6.31', '+0.00', '+0.00']\n",
      "step 86 total_reward -504.40\n",
      "['-0.01', '+0.95', '-0.48', '+0.37', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.53\n",
      "['-0.12', '+0.98', '-0.68', '-0.20', '+0.59', '+0.99', '+0.00', '+0.00']\n",
      "step 20 total_reward -72.92\n",
      "['-0.26', '+0.83', '-0.74', '-0.81', '+2.02', '+1.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -243.53\n",
      "['-0.50', '+0.44', '-1.09', '-1.62', '+3.83', '+2.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -491.14\n",
      "['-0.64', '+0.04', '-1.24', '-0.24', '+5.32', '-1.13', '+1.00', '+0.00']\n",
      "step 74 total_reward -748.21\n",
      "['+0.01', '+0.95', '+0.26', '+0.27', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.01\n",
      "['+0.04', '+0.95', '+0.06', '-0.28', '+0.40', '+0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.41\n",
      "['+0.05', '+0.78', '+0.01', '-0.83', '+1.40', '+1.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -167.23\n",
      "['-0.02', '+0.44', '-0.72', '-1.53', '+2.36', '+0.89', '+0.00', '+0.00']\n",
      "step 60 total_reward -319.35\n",
      "['-0.17', '-0.00', '-0.99', '+0.09', '+3.16', '+0.69', '+0.00', '+0.00']\n",
      "step 76 total_reward -553.75\n",
      "['+0.00', '+0.94', '+0.19', '+0.11', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.06\n",
      "['+0.02', '+0.89', '+0.04', '-0.43', '+0.40', '+0.58', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.40\n",
      "['+0.04', '+0.69', '-0.07', '-0.92', '+0.98', '+0.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -141.01\n",
      "['-0.10', '+0.39', '-1.08', '-1.22', '+1.58', '+0.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -248.25\n",
      "['-0.35', '+0.01', '-1.27', '+0.02', '+2.32', '+1.10', '+0.00', '+1.00']\n",
      "step 78 total_reward -483.04\n",
      "['+0.01', '+0.93', '+0.28', '-0.45', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.22\n",
      "['+0.06', '+0.71', '+0.22', '-0.96', '+0.12', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.61\n",
      "['+0.10', '+0.38', '+0.35', '-1.34', '-0.09', '-0.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.20', '+0.01', '+0.61', '-1.07', '-0.71', '-0.27', '+1.00', '+0.00']\n",
      "step 60 total_reward -65.95\n",
      "['+0.21', '-0.00', '+0.67', '-0.28', '-0.69', '+2.69', '+1.00', '+0.00']\n",
      "step 61 total_reward -165.95\n",
      "['+0.01', '+0.94', '+0.33', '-0.01', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.54\n",
      "['+0.09', '+0.85', '+0.45', '-0.55', '-0.48', '-0.62', '+0.00', '+0.00']\n",
      "step 20 total_reward -79.53\n",
      "['+0.17', '+0.61', '+0.50', '-1.13', '-1.19', '-0.99', '+0.00', '+0.00']\n",
      "step 40 total_reward -179.50\n",
      "['+0.28', '+0.18', '+0.43', '-1.77', '-2.68', '-1.95', '+0.00', '+0.00']\n",
      "step 60 total_reward -358.85\n",
      "['+0.30', '+0.05', '-0.56', '-0.53', '-3.11', '+4.07', '+0.00', '+0.00']\n",
      "step 65 total_reward -506.45\n",
      "['-0.00', '+0.93', '-0.17', '-0.46', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.44\n",
      "['-0.04', '+0.71', '-0.25', '-0.87', '+0.22', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -44.66\n",
      "['-0.12', '+0.47', '-0.52', '-0.94', '+0.51', '+0.56', '+0.00', '+0.00']\n",
      "step 40 total_reward -73.20\n",
      "['-0.26', '+0.13', '-0.88', '-1.29', '+1.30', '+0.61', '+0.00', '+0.00']\n",
      "step 60 total_reward -183.12\n",
      "['-0.46', '-0.07', '-0.97', '-0.04', '+3.01', '-1.53', '+0.00', '+0.00']\n",
      "step 76 total_reward -462.38\n",
      "['+0.02', '+0.95', '+0.79', '+0.38', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.02\n",
      "['+0.16', '+0.98', '+0.60', '-0.16', '+0.28', '+0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward -3.54\n",
      "['+0.27', '+0.85', '+0.49', '-0.74', '+1.34', '+1.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -126.25\n",
      "['+0.36', '+0.56', '+0.43', '-1.21', '+2.38', '+0.67', '+0.00', '+0.00']\n",
      "step 60 total_reward -248.53\n",
      "['+0.43', '+0.13', '+0.32', '-1.69', '+2.64', '+0.07', '+0.00', '+0.00']\n",
      "step 80 total_reward -295.90\n",
      "['+0.45', '-0.09', '-0.23', '-0.20', '+2.92', '+3.26', '+0.00', '+0.00']\n",
      "step 89 total_reward -433.63\n",
      "['-0.01', '+0.95', '-0.55', '+0.44', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.45\n",
      "['-0.14', '+1.00', '-0.74', '-0.13', '+0.59', '+0.95', '+0.00', '+0.00']\n",
      "step 20 total_reward -69.07\n",
      "['-0.29', '+0.87', '-0.80', '-0.76', '+2.01', '+1.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -237.61\n",
      "['-0.47', '+0.53', '-0.77', '-1.44', '+3.92', '+2.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -463.87\n",
      "['-0.58', '+0.02', '-0.50', '-1.88', '+6.36', '+2.45', '+0.00', '+0.00']\n",
      "step 80 total_reward -727.66\n",
      "['-0.60', '-0.09', '-0.69', '-0.72', '+6.82', '-4.81', '+1.00', '+1.00']\n",
      "step 84 total_reward -866.55\n",
      "['-0.01', '+0.95', '-0.65', '+0.30', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.21\n",
      "['-0.16', '+0.95', '-0.85', '-0.26', '+0.63', '+1.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.33\n",
      "['-0.34', '+0.79', '-1.12', '-0.92', '+2.05', '+1.71', '+0.00', '+0.00']\n",
      "step 40 total_reward -270.11\n",
      "['-0.61', '+0.39', '-1.20', '-1.57', '+3.97', '+2.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -504.06\n",
      "['-0.81', '-0.20', '-1.51', '-1.52', '+6.37', '+7.58', '+1.00', '+1.00']\n",
      "step 80 total_reward -865.62\n",
      "['+0.02', '+0.94', '+0.80', '-0.14', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.29\n",
      "['+0.16', '+0.81', '+0.69', '-0.68', '+0.16', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.07\n",
      "['+0.31', '+0.53', '+0.83', '-1.19', '+0.22', '-0.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -52.14\n",
      "['+0.49', '+0.09', '+0.99', '-1.72', '-0.49', '-0.96', '+0.00', '+0.00']\n",
      "step 60 total_reward -122.52\n",
      "['+0.54', '-0.04', '+1.19', '-0.84', '-0.59', '+4.70', '+1.00', '+0.00']\n",
      "step 65 total_reward -239.00\n",
      "['+0.01', '+0.93', '+0.35', '-0.45', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.05\n",
      "['+0.07', '+0.71', '+0.31', '-0.95', '+0.07', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.46\n",
      "['+0.15', '+0.37', '+0.48', '-1.37', '-0.17', '-0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.86\n",
      "['+0.26', '-0.03', '+0.92', '-0.75', '-0.58', '+4.33', '+1.00', '+0.00']\n",
      "step 59 total_reward -203.38\n",
      "['-0.00', '+0.93', '-0.18', '-0.42', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.38\n",
      "['-0.04', '+0.72', '-0.22', '-0.88', '+0.23', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.09\n",
      "['-0.13', '+0.53', '-0.59', '-0.71', '+0.55', '+0.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -69.66\n",
      "['-0.28', '+0.23', '-1.01', '-1.21', '+1.49', '+1.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -213.47\n",
      "['-0.40', '-0.00', '-0.13', '+0.04', '+1.97', '+0.43', '+0.00', '+0.00']\n",
      "step 72 total_reward -380.95\n",
      "['-0.01', '+0.93', '-0.48', '-0.51', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.57\n",
      "['-0.11', '+0.70', '-0.61', '-0.84', '+0.30', '+0.29', '+0.00', '+0.00']\n",
      "step 20 total_reward -44.36\n",
      "['-0.27', '+0.43', '-0.89', '-1.16', '+0.79', '+0.86', '+0.00', '+0.00']\n",
      "step 40 total_reward -117.66\n",
      "['-0.49', '+0.06', '-1.38', '+0.02', '+2.27', '+2.73', '+0.00', '+1.00']\n",
      "step 60 total_reward -357.18\n",
      "['-0.00', '+0.95', '-0.17', '+0.34', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.02\n",
      "['-0.05', '+0.97', '-0.36', '-0.21', '+0.49', '+0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.06\n",
      "['-0.13', '+0.82', '-0.42', '-0.79', '+1.65', '+1.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -203.95\n",
      "['-0.27', '+0.47', '-0.74', '-1.53', '+2.98', '+1.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -389.94\n",
      "['-0.40', '-0.03', '-0.57', '-0.04', '+4.17', '-1.81', '+1.00', '+0.00']\n",
      "step 78 total_reward -661.97\n",
      "['-0.00', '+0.94', '-0.13', '+0.02', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.40\n",
      "['-0.04', '+0.86', '-0.28', '-0.52', '+0.45', '+0.64', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.59\n",
      "['-0.12', '+0.65', '-0.85', '-0.77', '+1.10', '+0.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -185.35\n",
      "['-0.30', '+0.33', '-0.98', '-1.43', '+2.21', '+1.47', '+0.00', '+0.00']\n",
      "step 60 total_reward -335.93\n",
      "['-0.45', '+0.02', '-1.04', '-1.02', '+3.34', '+7.22', '+0.00', '+0.00']\n",
      "step 73 total_reward -549.88\n",
      "['-0.00', '+0.95', '-0.04', '+0.19', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.88\n",
      "['-0.02', '+0.92', '-0.23', '-0.37', '+0.44', '+0.81', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.22\n",
      "['-0.07', '+0.73', '-0.54', '-0.83', '+1.25', '+0.79', '+0.00', '+0.00']\n",
      "step 40 total_reward -183.77\n",
      "['-0.27', '+0.42', '-1.01', '-1.38', '+2.27', '+1.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -336.27\n",
      "['-0.48', '-0.04', '-1.04', '-0.49', '+3.44', '-4.74', '+0.00', '+0.00']\n",
      "step 78 total_reward -613.02\n",
      "['+0.00', '+0.93', '+0.10', '-0.42', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.62\n",
      "['+0.01', '+0.72', '+0.04', '-0.91', '+0.21', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.37\n",
      "['-0.02', '+0.51', '-0.47', '-0.51', '+0.45', '+0.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.19\n",
      "['-0.16', '+0.34', '-0.83', '-0.80', '+0.86', '+0.74', '+0.00', '+0.00']\n",
      "step 60 total_reward -114.11\n",
      "['-0.41', '+0.10', '-1.09', '-0.11', '+1.70', '+3.49', '+0.00', '+1.00']\n",
      "step 80 total_reward -314.96\n",
      "['-0.01', '+0.92', '-0.60', '-0.55', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.49\n",
      "['-0.14', '+0.69', '-0.72', '-0.83', '+0.29', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.49\n",
      "['-0.30', '+0.41', '-0.92', '-1.22', '+0.87', '+1.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -121.47\n",
      "['-0.52', '-0.01', '-1.17', '-1.66', '+1.78', '+0.85', '+0.00', '+1.00']\n",
      "step 60 total_reward -256.23\n",
      "['-0.53', '-0.02', '-0.65', '-0.15', '+1.69', '-2.73', '+0.00', '+1.00']\n",
      "step 61 total_reward -356.23\n",
      "['+0.01', '+0.94', '+0.49', '-0.06', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.21\n",
      "['+0.09', '+0.84', '+0.38', '-0.59', '+0.22', '+0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.59\n",
      "['+0.18', '+0.58', '+0.47', '-1.10', '+0.46', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -82.93\n",
      "['+0.28', '+0.18', '+0.55', '-1.40', '-0.01', '-0.73', '+0.00', '+0.00']\n",
      "step 60 total_reward -43.01\n",
      "['+0.32', '-0.01', '+0.26', '-0.73', '-0.16', '+5.16', '+0.00', '+0.00']\n",
      "step 69 total_reward -166.60\n",
      "['+0.00', '+0.92', '+0.01', '-0.56', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.82\n",
      "['-0.00', '+0.68', '-0.05', '-0.90', '+0.18', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.81\n",
      "['-0.04', '+0.46', '-0.39', '-0.60', '+0.37', '+0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -15.82\n",
      "['-0.16', '+0.25', '-0.91', '-0.75', '+0.88', '+0.65', '+0.00', '+0.00']\n",
      "step 60 total_reward -100.04\n",
      "['-0.33', '+0.04', '-0.49', '+0.21', '+1.57', '+1.03', '+0.00', '+1.00']\n",
      "step 77 total_reward -243.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.01', '+0.94', '+0.41', '+0.02', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.72\n",
      "['+0.07', '+0.86', '+0.27', '-0.52', '+0.32', '+0.51', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.12\n",
      "['+0.13', '+0.63', '+0.32', '-1.03', '+0.78', '+0.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -113.12\n",
      "['+0.21', '+0.25', '+0.34', '-1.37', '+0.53', '-0.61', '+0.00', '+0.00']\n",
      "step 60 total_reward -91.33\n",
      "['+0.22', '-0.03', '+0.13', '-0.90', '+0.03', '-5.28', '+0.00', '+1.00']\n",
      "step 74 total_reward -135.03\n",
      "['+0.01', '+0.93', '+0.53', '-0.33', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.27\n",
      "['+0.11', '+0.75', '+0.45', '-0.86', '+0.19', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.18\n",
      "['+0.21', '+0.41', '+0.63', '-1.37', '+0.08', '-0.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.69\n",
      "['+0.32', '+0.01', '+0.14', '-0.20', '-0.44', '+3.33', '+1.00', '+0.00']\n",
      "step 57 total_reward -193.44\n",
      "['+0.01', '+0.95', '+0.27', '+0.35', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.88\n",
      "['+0.04', '+0.97', '+0.07', '-0.20', '+0.38', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -16.01\n",
      "['+0.05', '+0.82', '-0.02', '-0.79', '+1.55', '+1.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -176.50\n",
      "['-0.01', '+0.48', '-0.54', '-1.66', '+2.83', '+1.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -367.99\n",
      "['-0.10', '+0.01', '-0.17', '-0.03', '+3.71', '-0.79', '+0.00', '+0.00']\n",
      "step 76 total_reward -598.07\n",
      "['+0.00', '+0.95', '+0.07', '+0.28', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.99\n",
      "['+0.02', '+0.95', '+0.07', '-0.26', '-0.02', '-0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward +2.55\n",
      "['+0.03', '+0.79', '+0.07', '-0.79', '-0.03', '-0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.78\n",
      "['+0.05', '+0.53', '+0.19', '-0.75', '-0.06', '-0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -14.09\n",
      "['+0.11', '+0.25', '+0.48', '-0.86', '-0.44', '-0.51', '+0.00', '+0.00']\n",
      "step 80 total_reward -50.54\n",
      "['+0.26', '+0.05', '+0.85', '-0.25', '-1.25', '-2.53', '+1.00', '+0.00']\n",
      "step 100 total_reward -115.59\n",
      "['+0.31', '+0.03', '+0.57', '+0.15', '-1.92', '-1.18', '+0.00', '+0.00']\n",
      "step 106 total_reward -292.35\n",
      "['-0.00', '+0.95', '-0.23', '+0.45', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.81\n",
      "['-0.07', '+1.00', '-0.43', '-0.11', '+0.52', '+0.91', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.70\n",
      "['-0.16', '+0.88', '-0.50', '-0.75', '+1.89', '+1.78', '+0.00', '+0.00']\n",
      "step 40 total_reward -222.51\n",
      "['-0.29', '+0.53', '-0.64', '-1.55', '+3.72', '+2.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -456.48\n",
      "['-0.37', '+0.07', '+0.02', '-1.35', '+5.70', '+7.62', '+1.00', '+0.00']\n",
      "step 77 total_reward -732.38\n",
      "['-0.00', '+0.94', '-0.11', '+0.06', '+0.00', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.14\n",
      "['-0.04', '+0.88', '-0.24', '-0.49', '+0.42', '+0.59', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.47\n",
      "['-0.10', '+0.67', '-0.67', '-0.75', '+1.03', '+0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -166.66\n",
      "['-0.28', '+0.38', '-0.93', '-1.32', '+2.00', '+1.44', '+0.00', '+0.00']\n",
      "step 60 total_reward -305.43\n",
      "['-0.53', '-0.12', '-1.12', '-0.06', '+3.22', '-0.00', '+0.00', '+0.00']\n",
      "step 80 total_reward -602.21\n",
      "['-0.01', '+0.93', '-0.33', '-0.24', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.43\n",
      "['-0.07', '+0.78', '-0.33', '-0.77', '+0.08', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.90\n",
      "['-0.15', '+0.57', '-0.47', '-0.70', '+0.19', '+0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -32.88\n",
      "['-0.26', '+0.28', '-0.79', '-1.07', '+0.83', '+0.85', '+0.00', '+0.00']\n",
      "step 60 total_reward -127.65\n",
      "['-0.43', '-0.02', '-1.27', '-0.40', '+2.02', '+3.02', '+0.00', '+0.00']\n",
      "step 80 total_reward -251.34\n",
      "['-0.47', '-0.03', '-1.16', '-0.13', '+2.50', '+4.22', '+0.00', '+0.00']\n",
      "step 83 total_reward -385.57\n",
      "['+0.01', '+0.93', '+0.40', '-0.51', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.01\n",
      "['+0.09', '+0.69', '+0.39', '-1.04', '-0.01', '+0.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.09\n",
      "['+0.18', '+0.30', '+0.59', '-1.51', '-0.36', '-0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.66\n",
      "['+0.26', '+0.04', '-0.09', '-0.02', '-1.53', '-0.15', '+1.00', '+0.00']\n",
      "step 55 total_reward -185.56\n",
      "['+0.01', '+0.95', '+0.41', '+0.39', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.58\n",
      "['+0.07', '+0.99', '+0.22', '-0.15', '+0.34', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -5.86\n",
      "['+0.11', '+0.85', '+0.13', '-0.73', '+1.42', '+1.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -149.05\n",
      "['+0.13', '+0.56', '+0.08', '-1.22', '+2.55', '+0.83', '+0.00', '+0.00']\n",
      "step 60 total_reward -282.00\n",
      "['+0.12', '+0.09', '-0.13', '-2.03', '+3.02', '+0.42', '+0.00', '+0.00']\n",
      "step 80 total_reward -370.47\n",
      "['+0.12', '-0.01', '-0.08', '-0.00', '+3.14', '+0.00', '+0.00', '+0.00']\n",
      "step 84 total_reward -482.32\n",
      "['+0.01', '+0.92', '+0.64', '-0.56', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.80\n",
      "['+0.14', '+0.67', '+0.64', '-1.09', '-0.02', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.51\n",
      "['+0.28', '+0.26', '+0.83', '-1.65', '-0.55', '-0.92', '+0.00', '+0.00']\n",
      "step 40 total_reward -100.82\n",
      "['+0.41', '-0.08', '+0.30', '+0.26', '-1.46', '-1.48', '+1.00', '+0.00']\n",
      "step 55 total_reward -216.85\n",
      "['+0.01', '+0.93', '+0.69', '-0.23', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.09\n",
      "['+0.14', '+0.78', '+0.62', '-0.76', '-0.03', '+0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.23\n",
      "['+0.28', '+0.47', '+0.78', '-1.30', '-0.22', '-0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -63.23\n",
      "['+0.44', '-0.00', '+0.93', '-1.91', '-1.20', '-1.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -213.12\n",
      "['+0.48', '-0.09', '+0.84', '+0.21', '-2.01', '-2.69', '+1.00', '+0.00']\n",
      "step 64 total_reward -318.73\n",
      "['-0.01', '+0.93', '-0.64', '-0.29', '+0.02', '+0.19', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.60\n",
      "['-0.15', '+0.76', '-0.73', '-0.78', '+0.38', '+0.38', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.46\n",
      "['-0.33', '+0.54', '-1.05', '-0.96', '+0.93', '+0.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -139.03\n",
      "['-0.58', '+0.16', '-1.45', '-1.52', '+2.20', '+1.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -332.29\n",
      "['-0.64', '+0.06', '-0.14', '+0.12', '+2.71', '+0.41', '+0.00', '+0.00']\n",
      "step 65 total_reward -467.52\n",
      "['+0.00', '+0.92', '-0.00', '-0.57', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.89\n",
      "['-0.01', '+0.68', '-0.05', '-0.91', '+0.18', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.16\n",
      "['-0.05', '+0.46', '-0.50', '-0.49', '+0.38', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -14.38\n",
      "['-0.18', '+0.26', '-1.01', '-0.73', '+0.87', '+0.64', '+0.00', '+0.00']\n",
      "step 60 total_reward -106.63\n",
      "['-0.38', '+0.08', '-0.94', '+0.16', '+1.76', '+3.47', '+0.00', '+1.00']\n",
      "step 77 total_reward -271.19\n",
      "['+0.00', '+0.94', '+0.11', '-0.02', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.80\n",
      "['+0.01', '+0.85', '-0.03', '-0.56', '+0.38', '+0.57', '+0.00', '+0.00']\n",
      "step 20 total_reward -73.85\n",
      "['-0.02', '+0.63', '-0.48', '-0.77', '+0.95', '+0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -145.70\n",
      "['-0.18', '+0.35', '-0.97', '-1.23', '+1.81', '+1.24', '+0.00', '+0.00']\n",
      "step 60 total_reward -277.38\n",
      "['-0.36', '+0.07', '-1.00', '-0.74', '+2.61', '+7.13', '+0.00', '+0.00']\n",
      "step 73 total_reward -507.97\n",
      "['-0.01', '+0.95', '-0.56', '+0.19', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.16\n",
      "['-0.14', '+0.92', '-0.76', '-0.37', '+0.59', '+0.96', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.38\n",
      "['-0.31', '+0.73', '-1.19', '-0.89', '+1.78', '+1.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -255.24\n",
      "['-0.56', '+0.37', '-1.09', '-1.48', '+3.52', '+2.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -452.42\n",
      "['-0.76', '-0.10', '-0.92', '+0.03', '+4.58', '-1.71', '+0.00', '+0.00']\n",
      "step 79 total_reward -641.68\n",
      "['+0.00', '+0.93', '+0.18', '-0.22', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.85\n",
      "['+0.03', '+0.78', '+0.11', '-0.76', '+0.21', '+0.24', '+0.00', '+0.00']\n",
      "step 20 total_reward -55.45\n",
      "['+0.03', '+0.56', '-0.26', '-0.58', '+0.46', '+0.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.22\n",
      "['-0.09', '+0.43', '-0.84', '-0.47', '+0.79', '+0.55', '+0.00', '+0.00']\n",
      "step 60 total_reward -107.53\n",
      "['-0.31', '+0.22', '-1.42', '-0.93', '+1.69', '+0.91', '+0.00', '+0.00']\n",
      "step 80 total_reward -267.34\n",
      "['-0.50', '+0.02', '-1.68', '-0.41', '+2.19', '+0.01', '+0.00', '+1.00']\n",
      "step 93 total_reward -424.70\n",
      "['+0.01', '+0.94', '+0.63', '-0.04', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.12', '+0.84', '+0.48', '-0.57', '+0.27', '+0.51', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.28\n",
      "['+0.23', '+0.59', '+0.55', '-1.07', '+0.67', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -92.59\n",
      "['+0.35', '+0.20', '+0.72', '-1.54', '+0.31', '-0.74', '+0.00', '+0.00']\n",
      "step 60 total_reward -83.41\n",
      "['+0.43', '-0.11', '+0.49', '-0.93', '-0.06', '+5.94', '+1.00', '+1.00']\n",
      "step 72 total_reward -172.09\n",
      "['+0.01', '+0.95', '+0.28', '+0.43', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.94\n",
      "['+0.04', '+1.00', '+0.08', '-0.12', '+0.40', '+0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -6.60\n",
      "['+0.05', '+0.87', '-0.02', '-0.74', '+1.63', '+1.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -176.63\n",
      "['+0.03', '+0.55', '-0.23', '-1.62', '+3.16', '+1.49', '+0.00', '+0.00']\n",
      "step 60 total_reward -389.44\n",
      "['+0.01', '-0.01', '-0.06', '+0.24', '+4.58', '+3.78', '+1.00', '+0.00']\n",
      "step 78 total_reward -650.56\n",
      "['+0.01', '+0.94', '+0.59', '-0.07', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.74\n",
      "['+0.13', '+0.83', '+0.59', '-0.61', '-0.14', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.52\n",
      "['+0.25', '+0.57', '+0.69', '-1.16', '-0.41', '-0.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -85.31\n",
      "['+0.40', '+0.13', '+0.81', '-1.78', '-1.46', '-1.42', '+0.00', '+0.00']\n",
      "step 60 total_reward -230.88\n",
      "['+0.41', '+0.08', '-0.08', '-0.52', '-1.49', '+4.82', '+1.00', '+0.00']\n",
      "step 62 total_reward -340.45\n",
      "['+0.01', '+0.92', '+0.40', '-0.57', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.75\n",
      "['+0.08', '+0.67', '+0.39', '-1.01', '+0.06', '+0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.29\n",
      "['+0.18', '+0.28', '+0.58', '-1.52', '-0.28', '-0.67', '+0.00', '+0.00']\n",
      "step 40 total_reward -63.73\n",
      "['+0.27', '-0.02', '+0.79', '-0.53', '-0.63', '+4.26', '+1.00', '+0.00']\n",
      "step 54 total_reward -182.45\n",
      "['+0.01', '+0.94', '+0.56', '-0.03', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.82\n",
      "['+0.11', '+0.85', '+0.40', '-0.56', '+0.32', '+0.56', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.19\n",
      "['+0.19', '+0.60', '+0.47', '-1.06', '+0.77', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -105.27\n",
      "['+0.30', '+0.21', '+0.64', '-1.51', '+0.41', '-0.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -90.97\n",
      "['+0.35', '+0.02', '+0.32', '-0.98', '+0.31', '+5.98', '+1.00', '+1.00']\n",
      "step 68 total_reward -168.18\n",
      "['-0.01', '+0.94', '-0.72', '+0.03', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.06\n",
      "['-0.17', '+0.87', '-0.88', '-0.52', '+0.62', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -86.49\n",
      "['-0.39', '+0.66', '-1.38', '-0.92', '+1.53', '+1.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -231.24\n",
      "['-0.67', '+0.28', '-1.34', '-1.64', '+3.16', '+1.89', '+0.00', '+0.00']\n",
      "step 60 total_reward -438.23\n",
      "['-0.90', '-0.19', '-0.13', '+0.02', '+4.29', '+0.00', '+1.00', '+0.00']\n",
      "step 78 total_reward -610.83\n",
      "['+0.01', '+0.92', '+0.37', '-0.55', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.03\n",
      "['+0.08', '+0.68', '+0.34', '-0.96', '+0.06', '+0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.48\n",
      "['+0.16', '+0.31', '+0.52', '-1.50', '-0.23', '-0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -59.97\n",
      "['+0.27', '-0.05', '+0.91', '-1.03', '-0.50', '+4.22', '+1.00', '+0.00']\n",
      "step 57 total_reward -187.94\n",
      "['-0.00', '+0.94', '-0.06', '+0.05', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.87\n",
      "['-0.03', '+0.87', '-0.22', '-0.50', '+0.45', '+0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.09\n",
      "['-0.09', '+0.66', '-0.70', '-0.81', '+1.16', '+0.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -190.26\n",
      "['-0.28', '+0.35', '-0.97', '-1.40', '+2.23', '+1.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -340.51\n",
      "['-0.50', '-0.14', '-1.32', '-1.43', '+3.52', '+6.41', '+0.00', '+0.00']\n",
      "step 79 total_reward -617.05\n",
      "['+0.01', '+0.93', '+0.37', '-0.39', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.79\n",
      "['+0.07', '+0.73', '+0.32', '-0.93', '+0.12', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -36.50\n",
      "['+0.15', '+0.37', '+0.49', '-1.45', '-0.05', '-0.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -51.61\n",
      "['+0.21', '+0.04', '-0.43', '-0.23', '-0.57', '+0.88', '+1.00', '+0.00']\n",
      "step 60 total_reward +26.35\n",
      "['+0.16', '-0.03', '-0.49', '-0.07', '+0.05', '-0.40', '+1.00', '+1.00']\n",
      "step 72 total_reward -17.73\n",
      "['+0.01', '+0.95', '+0.48', '+0.26', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.58\n",
      "['+0.09', '+0.94', '+0.29', '-0.28', '+0.32', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.69\n",
      "['+0.14', '+0.78', '+0.23', '-0.84', '+1.24', '+0.96', '+0.00', '+0.00']\n",
      "step 40 total_reward -139.15\n",
      "['+0.19', '+0.46', '+0.23', '-1.26', '+1.82', '+0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -210.01\n",
      "['+0.19', '+0.01', '-0.43', '-1.26', '+1.98', '+4.84', '+0.00', '+1.00']\n",
      "step 80 total_reward -192.00\n",
      "['+0.19', '+0.01', '-0.67', '+0.13', '+2.14', '+2.09', '+0.00', '+1.00']\n",
      "step 81 total_reward -292.00\n",
      "['-0.00', '+0.95', '-0.20', '+0.20', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.37\n",
      "['-0.06', '+0.92', '-0.39', '-0.35', '+0.53', '+0.90', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.01\n",
      "['-0.15', '+0.74', '-0.75', '-0.87', '+1.56', '+1.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -224.22\n",
      "['-0.35', '+0.39', '-0.95', '-1.50', '+2.87', '+1.71', '+0.00', '+0.00']\n",
      "step 60 total_reward -396.45\n",
      "['-0.49', '+0.04', '-0.53', '-0.24', '+3.93', '+5.35', '+0.00', '+0.00']\n",
      "step 73 total_reward -633.84\n",
      "['+0.00', '+0.95', '+0.12', '+0.20', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.29\n",
      "['+0.01', '+0.92', '-0.06', '-0.35', '+0.41', '+0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -48.48\n",
      "['-0.00', '+0.74', '-0.25', '-0.86', '+1.30', '+0.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -173.61\n",
      "['-0.16', '+0.41', '-0.94', '-1.40', '+2.30', '+1.32', '+0.00', '+0.00']\n",
      "step 60 total_reward -326.97\n",
      "['-0.36', '-0.02', '-1.14', '-0.83', '+3.23', '-5.80', '+0.00', '+0.00']\n",
      "step 77 total_reward -585.49\n",
      "['-0.01', '+0.95', '-0.75', '+0.27', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.25\n",
      "['-0.18', '+0.94', '-0.95', '-0.30', '+0.67', '+1.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -87.90\n",
      "['-0.38', '+0.77', '-1.27', '-0.99', '+2.19', '+1.84', '+0.00', '+0.00']\n",
      "step 40 total_reward -292.30\n",
      "['-0.66', '+0.36', '-1.17', '-1.57', '+4.40', '+2.56', '+0.00', '+0.00']\n",
      "step 60 total_reward -538.84\n",
      "['-0.82', '-0.09', '+0.46', '-0.25', '+6.73', '+2.10', '+0.00', '+1.00']\n",
      "step 77 total_reward -834.44\n",
      "['-0.02', '+0.93', '-0.81', '-0.41', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.27\n",
      "['-0.19', '+0.73', '-0.99', '-0.76', '+0.42', '+0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -59.90\n",
      "['-0.42', '+0.48', '-1.27', '-1.11', '+1.07', '+1.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -160.31\n",
      "['-0.70', '+0.07', '-1.44', '-1.66', '+2.32', '+1.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -344.52\n",
      "['-0.75', '+0.01', '-1.69', '-0.22', '+2.55', '+2.16', '+0.00', '+0.00']\n",
      "step 63 total_reward -465.19\n",
      "['-0.01', '+0.94', '-0.70', '-0.16', '+0.02', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.98\n",
      "['-0.15', '+0.81', '-0.70', '-0.69', '+0.17', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.57\n",
      "['-0.32', '+0.59', '-0.96', '-0.83', '+0.38', '+0.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -68.46\n",
      "['-0.53', '+0.26', '-1.32', '-1.29', '+1.19', '+0.96', '+0.00', '+0.00']\n",
      "step 60 total_reward -200.81\n",
      "['-0.66', '+0.05', '-0.04', '-0.25', '+1.45', '-3.39', '+0.00', '+1.00']\n",
      "step 71 total_reward -360.42\n",
      "['+0.00', '+0.95', '+0.19', '+0.16', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.13\n",
      "['+0.03', '+0.91', '+0.00', '-0.38', '+0.42', '+0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.38\n",
      "['+0.03', '+0.72', '+0.01', '-0.92', '+1.17', '+0.75', '+0.00', '+0.00']\n",
      "step 40 total_reward -158.97\n",
      "['-0.07', '+0.38', '-0.86', '-1.42', '+1.95', '+0.94', '+0.00', '+0.00']\n",
      "step 60 total_reward -283.38\n",
      "['-0.26', '+0.00', '-1.66', '-0.36', '+2.89', '+5.25', '+0.00', '+0.00']\n",
      "step 75 total_reward -522.94\n",
      "['-0.01', '+0.93', '-0.32', '-0.40', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.52\n",
      "['-0.08', '+0.73', '-0.40', '-0.89', '+0.35', '+0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.49\n",
      "['-0.20', '+0.47', '-0.71', '-1.08', '+0.93', '+0.94', '+0.00', '+0.00']\n",
      "step 40 total_reward -134.25\n",
      "['-0.41', '+0.08', '-0.46', '+0.10', '+2.05', '+1.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -405.02\n",
      "['-0.01', '+0.93', '-0.50', '-0.38', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.12', '+0.73', '-0.61', '-0.83', '+0.39', '+0.41', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.57\n",
      "['-0.28', '+0.49', '-0.92', '-1.05', '+1.00', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -145.26\n",
      "['-0.52', '+0.10', '-0.60', '-0.04', '+2.17', '+3.92', '+0.00', '+0.00']\n",
      "step 60 total_reward -427.94\n",
      "['-0.01', '+0.94', '-0.59', '+0.10', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.38\n",
      "['-0.14', '+0.89', '-0.77', '-0.46', '+0.60', '+0.94', '+0.00', '+0.00']\n",
      "step 20 total_reward -86.63\n",
      "['-0.34', '+0.69', '-1.34', '-0.89', '+1.54', '+1.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -240.66\n",
      "['-0.61', '+0.33', '-1.29', '-1.64', '+3.09', '+1.90', '+0.00', '+0.00']\n",
      "step 60 total_reward -437.18\n",
      "['-0.83', '-0.14', '-0.51', '-0.57', '+4.61', '-5.70', '+1.00', '+0.00']\n",
      "step 76 total_reward -724.05\n",
      "['+0.01', '+0.95', '+0.75', '+0.15', '-0.02', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.43\n",
      "['+0.16', '+0.91', '+0.75', '-0.38', '-0.18', '-0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.50\n",
      "['+0.31', '+0.71', '+0.77', '-0.92', '-0.36', '-0.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -61.24\n",
      "['+0.48', '+0.34', '+0.92', '-1.53', '-1.04', '-1.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -170.08\n",
      "['+0.58', '+0.10', '+0.70', '+0.18', '-2.05', '-2.24', '+1.00', '+0.00']\n",
      "step 71 total_reward -344.39\n",
      "['-0.01', '+0.94', '-0.54', '-0.02', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.84\n",
      "['-0.13', '+0.85', '-0.71', '-0.57', '+0.59', '+0.85', '+0.00', '+0.00']\n",
      "step 20 total_reward -88.69\n",
      "['-0.31', '+0.62', '-1.18', '-0.98', '+1.51', '+1.17', '+0.00', '+0.00']\n",
      "step 40 total_reward -229.54\n",
      "['-0.56', '+0.22', '-1.30', '-1.79', '+3.13', '+1.75', '+0.00', '+0.00']\n",
      "step 60 total_reward -451.39\n",
      "['-0.59', '+0.18', '-1.34', '-0.37', '+3.17', '-3.43', '+0.00', '+0.00']\n",
      "step 62 total_reward -563.08\n",
      "['+0.01', '+0.94', '+0.36', '-0.17', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.14\n",
      "['+0.07', '+0.80', '+0.29', '-0.70', '+0.19', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.19\n",
      "['+0.13', '+0.53', '+0.35', '-1.10', '+0.27', '-0.24', '+0.00', '+0.00']\n",
      "step 40 total_reward -64.44\n",
      "['+0.21', '+0.15', '+0.53', '-1.26', '-0.31', '-0.71', '+0.00', '+0.00']\n",
      "step 60 total_reward -62.56\n",
      "['+0.25', '-0.03', '+0.56', '-0.78', '-0.36', '+4.64', '+1.00', '+0.00']\n",
      "step 69 total_reward -179.30\n",
      "['-0.01', '+0.93', '-0.74', '-0.42', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.56\n",
      "['-0.17', '+0.73', '-0.82', '-0.78', '+0.38', '+0.41', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.03\n",
      "['-0.36', '+0.47', '-1.05', '-1.16', '+1.07', '+1.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -147.85\n",
      "['-0.62', '+0.04', '-1.41', '-1.69', '+2.28', '+1.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -338.00\n",
      "['-0.68', '-0.05', '-1.96', '-0.85', '+2.39', '-1.89', '+0.00', '+1.00']\n",
      "step 64 total_reward -467.76\n",
      "total rewards [-408.6914594551233, -111.63142023571979, -185.24743111367758, -414.4650849715498, -124.3936577031989, -145.10425349343745, -187.1612871254229, -249.87805458447997, -591.492321944316, -382.2907688756848, -371.3504520343859, -653.6493832176707, -176.74442995361272, -525.0711607346111, 1.9137335338994461, -461.54822336023653, -137.84790264959, -720.0671533263775, -785.3777147708415, -915.1477534301001, -228.23370062689318, -834.523978797357, -497.20319239897185, -662.0046715719136, -631.1715016577849, -515.3315093740869, -180.55155863015852, -257.27418493450807, -551.1820852444557, -579.324421814686, -415.1144512585262, -330.1880902143506, -522.9505764375164, -519.5298628620153, -259.72182965477066, -454.4309217488601, -305.36342598353656, -748.6747862081206, -683.3994559853827, -473.39436485381316, -115.03719495850426, -425.23513289147803, -174.15485096759608, -587.3668350818721, -174.86259918741388, -383.49910183901045, -698.4494358555862, -703.0575699977074, -437.6994790667171, -460.466533472848, -533.9119068369122, -945.6266442498368, -452.5465915829549, -588.8264670775168, -310.20470974179716, -796.0640307741356, -461.3356118266611, -318.89024512849926, -561.0522601378852, -206.64699908521357, -6.5031335696886, -309.59892001372316, -1120.9416549149787, -643.2755383924135, -553.8628092717652, -864.5098084239854, -569.4527796344769, -630.4093989493825, -274.7395552268078, -853.2442846254282, -73.4926436946848, -559.4909951898733, -412.92333899527557, -373.618217150568, -178.4272221736265, -584.2754799150588, -645.9884146929937, -754.8743827713445, -449.8308520288626, -177.66319867305396, -182.49695703098084, -467.18199082542446, -638.8422370796953, -769.1422819155513, -512.4712955693913, -502.1796877413197, -605.5825551835294, -494.137632119919, -209.43330893295712, -453.1287477941998, -230.08181557000862, -244.35096464263614, -324.4853486788551, -269.8664408650169, -299.311026440774, -460.55652620090257, -277.34335976100385, -631.83779971313, -552.0125710363675, -593.2404747969567, -577.2893215693655, -67.90442842569246, -405.09693543382, -366.3650932183681, -408.65470073345733, -471.5509019169301, -359.20825658250993, -399.32079731627533, -265.88157659061153, -258.4327503221458, -598.6334575236923, -607.5085084844682, -700.5726470798877, -148.5480856471267, -20.733896077476828, -709.5363987137578, -201.6724392198639, -668.2901058028859, -643.4437623580159, -155.1945918971036, -222.23761943335523, -394.47875714257464, -252.62044600959712, -661.0983247658374, -400.322339157863, -508.78192304470446, -700.4698033773285, -410.37236659014707, -368.349978526506, -523.5070499474593, -279.248389891986, -406.3725114668144, -395.6481318320214, -344.70184485774814, -571.170035711698, -291.9286618246996, -1407.975785763319, -737.5493679073918, -489.00583326291354, -746.04936675987, -504.40469779920676, -748.2132299698383, -553.7519207396213, -483.04322288651286, -165.95043579457143, -506.45092315306215, -462.3846242347887, -433.62938315525514, -866.5541205645728, -865.6195863808687, -238.99962508043973, -203.37941666620935, -380.94944159347403, -357.17969572757585, -661.9707275951466, -549.876718671468, -613.0174083877429, -314.95783248518154, -356.23464919610433, -166.5978185591751, -243.67486434491113, -135.0315181700525, -193.4398782701332, -598.0679465265885, -292.34920791080526, -732.3821027558313, -602.2092242055551, -385.5697801341058, -185.55824207063606, -482.3222256610074, -216.8499129761981, -318.73485275431756, -467.5229351643027, -271.1928332710898, -507.9731813888776, -641.68312343017, -424.6998803925562, -172.09298647714516, -650.561204116554, -340.4469580222325, -182.44928897109799, -168.1812204130374, -610.8322183282464, -187.93918435003653, -617.050385009608, -17.726969731432177, -292.0000149214326, -633.835545783098, -585.486442754252, -834.4398741918928, -465.18652488304593, -360.41831281068136, -522.9359895417208, -405.0208437349476, -427.94327266459464, -724.0460901101776, -344.3906591556182, -563.0830725428245, -179.30328807163187, -467.763776975823]\n",
      "average total reward -446.5792155174349\n"
     ]
    }
   ],
   "source": [
    "!python lunar_lander_ml_images_player_model5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_model_reward_comparisons[\"Model 5\"] = '-446.5792155174349'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-04-23 00:47:58.219968: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "['+0.01', '+0.94', '+0.59', '-0.13', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.20\n",
      "['+0.11', '+0.81', '+0.45', '-0.67', '+0.27', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.73\n",
      "['+0.17', '+0.60', '-0.10', '-0.57', '+0.72', '+0.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.40\n",
      "['+0.06', '+0.44', '-0.70', '-0.65', '+1.13', '+0.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -106.34\n",
      "['-0.09', '+0.18', '-1.03', '-1.03', '+1.01', '-0.21', '+0.00', '+0.00']\n",
      "step 80 total_reward -122.41\n",
      "['-0.32', '+0.03', '-1.42', '-0.21', '+1.77', '+1.41', '+0.00', '+0.00']\n",
      "step 100 total_reward -210.57\n",
      "['-0.34', '+0.03', '-1.23', '+0.20', '+1.90', '+0.94', '+0.00', '+1.00']\n",
      "step 102 total_reward -319.31\n",
      "['+0.01', '+0.93', '+0.45', '-0.47', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.60\n",
      "['+0.09', '+0.70', '+0.49', '-1.00', '+0.05', '-0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.42\n",
      "['+0.21', '+0.32', '+0.68', '-1.56', '-0.58', '-1.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -108.81\n",
      "['+0.29', '+0.02', '+0.36', '+0.18', '-1.72', '-1.33', '+1.00', '+0.00']\n",
      "step 53 total_reward -240.56\n",
      "['-0.00', '+0.93', '-0.04', '-0.51', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.80\n",
      "['-0.00', '+0.69', '+0.01', '-1.05', '-0.21', '-0.23', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.53\n",
      "['+0.01', '+0.29', '+0.11', '-1.61', '-0.69', '-0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -117.28\n",
      "['+0.03', '-0.00', '+0.45', '-0.71', '-1.44', '-7.43', '+1.00', '+0.00']\n",
      "step 52 total_reward -248.77\n",
      "['+0.01', '+0.95', '+0.38', '+0.36', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.22\n",
      "['+0.10', '+0.97', '+0.56', '-0.20', '-0.54', '-0.88', '+0.00', '+0.00']\n",
      "step 20 total_reward -64.08\n",
      "['+0.21', '+0.83', '+0.59', '-0.76', '-1.60', '-1.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -194.89\n",
      "['+0.34', '+0.52', '+0.65', '-1.47', '-2.86', '-1.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -363.10\n",
      "['+0.46', '+0.01', '+0.44', '+0.06', '-4.60', '-1.02', '+0.00', '+1.00']\n",
      "step 80 total_reward -653.48\n",
      "['+0.01', '+0.94', '+0.63', '-0.10', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.83\n",
      "['+0.13', '+0.82', '+0.60', '-0.63', '-0.03', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.74\n",
      "['+0.26', '+0.56', '+0.68', '-0.99', '-0.10', '-0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -34.03\n",
      "['+0.41', '+0.19', '+0.85', '-1.44', '-0.62', '-0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -118.48\n",
      "['+0.52', '-0.11', '+1.50', '-0.68', '-1.00', '+2.25', '+1.00', '+0.00']\n",
      "step 73 total_reward -271.98\n",
      "['+0.01', '+0.93', '+0.39', '-0.32', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.44\n",
      "['+0.07', '+0.75', '+0.18', '-0.86', '+0.37', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.68\n",
      "['+0.10', '+0.44', '+0.18', '-1.26', '+1.00', '+0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -131.11\n",
      "['+0.14', '+0.03', '+0.19', '-0.35', '+1.04', '+1.35', '+0.00', '+1.00']\n",
      "step 60 total_reward -7.36\n",
      "['+0.17', '-0.01', '+0.27', '-0.23', '+0.27', '-1.86', '+0.00', '+1.00']\n",
      "step 73 total_reward -53.90\n",
      "['+0.01', '+0.95', '+0.63', '+0.35', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.28\n",
      "['+0.14', '+0.97', '+0.62', '-0.18', '-0.07', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward +0.14\n",
      "['+0.26', '+0.83', '+0.62', '-0.72', '-0.12', '-0.05', '+0.00', '+0.00']\n",
      "step 40 total_reward -24.77\n",
      "['+0.39', '+0.54', '+0.72', '-1.23', '-0.25', '-0.38', '+0.00', '+0.00']\n",
      "step 60 total_reward -65.49\n",
      "['+0.54', '+0.08', '+0.82', '-1.80', '-0.97', '-0.85', '+0.00', '+0.00']\n",
      "step 80 total_reward -180.98\n",
      "['+0.58', '+0.00', '+1.36', '-0.39', '-1.02', '+1.16', '+1.00', '+0.00']\n",
      "step 84 total_reward -259.96\n",
      "['-0.01', '+0.93', '-0.53', '-0.52', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.53\n",
      "['-0.11', '+0.69', '-0.49', '-1.05', '+0.02', '-0.00', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.82\n",
      "['-0.22', '+0.31', '-0.64', '-1.45', '+0.32', '+0.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -65.33\n",
      "['-0.25', '+0.03', '+0.41', '-0.32', '+0.34', '-1.30', '+0.00', '+0.00']\n",
      "step 60 total_reward +51.39\n",
      "['-0.22', '-0.01', '+0.47', '-0.35', '-0.16', '-1.61', '+1.00', '+0.00']\n",
      "step 67 total_reward -18.85\n",
      "['-0.01', '+0.94', '-0.66', '-0.13', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.38\n",
      "['-0.13', '+0.82', '-0.55', '-0.66', '-0.20', '-0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.54\n",
      "['-0.23', '+0.61', '-0.39', '-0.71', '-0.39', '+0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -24.40\n",
      "['-0.32', '+0.32', '-0.55', '-1.21', '+0.06', '+0.73', '+0.00', '+0.00']\n",
      "step 60 total_reward -25.47\n",
      "['-0.39', '-0.03', '+0.31', '-0.50', '+0.16', '-4.06', '+1.00', '+1.00']\n",
      "step 79 total_reward -75.23\n",
      "['-0.01', '+0.94', '-0.55', '-0.02', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.69\n",
      "['-0.12', '+0.85', '-0.56', '-0.56', '+0.24', '+0.23', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.86\n",
      "['-0.25', '+0.66', '-0.87', '-0.53', '+0.48', '+0.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -75.39\n",
      "['-0.47', '+0.46', '-1.32', '-0.79', '+0.99', '+0.75', '+0.00', '+0.00']\n",
      "step 60 total_reward -176.88\n",
      "['-0.74', '+0.13', '-1.32', '-1.45', '+2.28', '+1.74', '+0.00', '+0.00']\n",
      "step 80 total_reward -357.98\n",
      "['-0.76', '+0.11', '-1.18', '-0.03', '+2.53', '+3.53', '+0.00', '+0.00']\n",
      "step 82 total_reward -469.57\n",
      "['+0.01', '+0.93', '+0.51', '-0.30', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.45\n",
      "['+0.10', '+0.76', '+0.42', '-0.84', '+0.20', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.10\n",
      "['+0.15', '+0.53', '+0.08', '-0.63', '+0.38', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -8.77\n",
      "['+0.15', '+0.32', '+0.03', '-0.88', '+0.27', '-0.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -4.08\n",
      "['+0.15', '-0.01', '-0.09', '-1.28', '-0.13', '+0.08', '+1.00', '+0.00']\n",
      "step 80 total_reward -0.40\n",
      "['+0.15', '-0.03', '-0.15', '-0.80', '-0.02', '+4.63', '+1.00', '+1.00']\n",
      "step 81 total_reward -100.40\n",
      "['+0.01', '+0.93', '+0.63', '-0.47', '-0.01', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['+0.13', '+0.70', '+0.60', '-1.00', '-0.04', '-0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.36\n",
      "['+0.26', '+0.45', '+0.67', '-0.83', '-0.10', '-0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -2.07\n",
      "['+0.39', '+0.13', '+0.32', '-0.71', '-0.59', '-1.06', '+1.00', '+0.00']\n",
      "step 60 total_reward -2.30\n",
      "['+0.40', '+0.11', '+0.06', '-0.21', '-0.34', '+3.00', '+1.00', '+0.00']\n",
      "step 63 total_reward -60.06\n",
      "['+0.01', '+0.93', '+0.73', '-0.26', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.27\n",
      "['+0.15', '+0.77', '+0.69', '-0.79', '+0.01', '+0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.53\n",
      "['+0.28', '+0.58', '+0.63', '-0.42', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward +26.23\n",
      "['+0.42', '+0.43', '+0.80', '-0.72', '-0.26', '-0.63', '+0.00', '+0.00']\n",
      "step 60 total_reward -29.04\n",
      "['+0.59', '+0.13', '+0.91', '-1.30', '-1.29', '-1.23', '+0.00', '+0.00']\n",
      "step 80 total_reward -184.97\n",
      "['+0.64', '+0.04', '+0.26', '+0.13', '-1.72', '-0.96', '+1.00', '+0.00']\n",
      "step 85 total_reward -315.67\n",
      "['-0.00', '+0.93', '-0.16', '-0.31', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.23\n",
      "['-0.02', '+0.75', '-0.03', '-0.85', '-0.35', '-0.54', '+0.00', '+0.00']\n",
      "step 20 total_reward -68.87\n",
      "['-0.02', '+0.42', '+0.20', '-1.37', '-1.25', '-1.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -179.21\n",
      "['+0.02', '+0.00', '+0.04', '-0.39', '-1.90', '+4.43', '+1.00', '+0.00']\n",
      "step 58 total_reward -350.47\n",
      "['-0.00', '+0.95', '-0.12', '+0.14', '+0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.61\n",
      "['-0.01', '+0.90', '+0.08', '-0.41', '-0.44', '-0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -61.15\n",
      "['+0.01', '+0.70', '+0.11', '-0.96', '-1.40', '-0.96', '+0.00', '+0.00']\n",
      "step 40 total_reward -191.39\n",
      "['+0.03', '+0.32', '+0.03', '-1.60', '-2.86', '-1.88', '+0.00', '+0.00']\n",
      "step 60 total_reward -363.48\n",
      "['+0.05', '+0.01', '+0.14', '+0.00', '-3.91', '-0.42', '+0.00', '+0.00']\n",
      "step 72 total_reward -564.67\n",
      "['+0.01', '+0.94', '+0.75', '-0.10', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.22\n",
      "['+0.15', '+0.83', '+0.63', '-0.63', '+0.20', '+0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.25', '+0.62', '+0.21', '-0.55', '+0.52', '+0.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -10.95\n",
      "['+0.21', '+0.48', '-0.62', '-0.38', '+0.79', '+0.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -43.50\n",
      "['+0.04', '+0.34', '-0.83', '-0.67', '+0.85', '-0.33', '+0.00', '+0.00']\n",
      "step 80 total_reward -67.39\n",
      "['-0.17', '+0.11', '-1.26', '-0.77', '+0.37', '-0.66', '+0.00', '+0.00']\n",
      "step 100 total_reward -50.92\n",
      "['-0.39', '-0.04', '-0.85', '-0.11', '-0.14', '-3.02', '+1.00', '+1.00']\n",
      "step 116 total_reward -113.69\n",
      "['-0.00', '+0.94', '-0.10', '+0.01', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.51\n",
      "['-0.01', '+0.86', '+0.02', '-0.52', '-0.18', '-0.44', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.28\n",
      "['-0.01', '+0.62', '+0.06', '-1.08', '-0.65', '-0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -130.03\n",
      "['+0.00', '+0.21', '+0.11', '-1.64', '-1.61', '-0.94', '+0.00', '+0.00']\n",
      "step 60 total_reward -242.13\n",
      "['+0.02', '+0.00', '+0.10', '+0.03', '-1.98', '-0.33', '+1.00', '+0.00']\n",
      "step 69 total_reward -361.36\n",
      "['+0.01', '+0.94', '+0.25', '-0.11', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.24\n",
      "['+0.04', '+0.82', '+0.10', '-0.66', '+0.37', '+0.61', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.95\n",
      "['+0.06', '+0.54', '+0.10', '-1.17', '+1.06', '+0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -156.29\n",
      "['+0.09', '+0.12', '+0.13', '-1.65', '+1.11', '+0.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -170.89\n",
      "['+0.09', '+0.00', '-0.15', '+0.04', '+1.98', '+0.49', '+0.00', '+1.00']\n",
      "step 67 total_reward -246.89\n",
      "['+0.01', '+0.94', '+0.65', '-0.18', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.17\n",
      "['+0.13', '+0.80', '+0.55', '-0.71', '+0.18', '+0.30', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.28\n",
      "['+0.21', '+0.60', '+0.17', '-0.55', '+0.47', '+0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -10.08\n",
      "['+0.16', '+0.48', '-0.71', '-0.25', '+0.74', '+0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -49.58\n",
      "['+0.01', '+0.36', '-0.68', '-0.63', '+0.67', '-0.50', '+0.00', '+0.00']\n",
      "step 80 total_reward -45.52\n",
      "['-0.18', '+0.17', '-1.11', '-0.54', '+0.17', '-0.59', '+0.00', '+0.00']\n",
      "step 100 total_reward -20.41\n",
      "['-0.33', '+0.02', '-0.83', '-0.42', '-0.37', '+3.31', '+1.00', '+0.00']\n",
      "step 114 total_reward -149.92\n",
      "['-0.00', '+0.92', '-0.06', '-0.56', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.58\n",
      "['-0.02', '+0.67', '-0.09', '-1.06', '+0.08', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.02\n",
      "['-0.05', '+0.41', '-0.21', '-0.79', '+0.17', '+0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward +1.09\n",
      "['-0.11', '+0.13', '-0.36', '-1.04', '+0.53', '+0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -40.67\n",
      "['-0.15', '-0.01', '-0.44', '-0.35', '+0.39', '-3.36', '+0.00', '+1.00']\n",
      "step 69 total_reward -121.19\n",
      "['-0.00', '+0.93', '-0.01', '-0.42', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.04\n",
      "['-0.01', '+0.72', '-0.04', '-0.96', '+0.00', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.57\n",
      "['-0.01', '+0.35', '+0.07', '-1.44', '-0.08', '-0.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.50\n",
      "['+0.00', '-0.01', '-0.11', '-0.55', '-0.35', '+5.25', '+1.00', '+0.00']\n",
      "step 55 total_reward -187.27\n",
      "['+0.00', '+0.92', '-0.00', '-0.57', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.91\n",
      "['-0.01', '+0.67', '-0.06', '-1.10', '+0.17', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.16\n",
      "['-0.05', '+0.38', '-0.35', '-0.94', '+0.36', '+0.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -31.92\n",
      "['-0.12', '+0.02', '-0.31', '-1.26', '+0.56', '-0.29', '+0.00', '+1.00']\n",
      "step 60 total_reward -46.47\n",
      "['-0.12', '-0.01', '-0.19', '-0.36', '+0.33', '-3.97', '+0.00', '+0.00']\n",
      "step 62 total_reward -145.33\n",
      "['+0.01', '+0.93', '+0.28', '-0.40', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.47\n",
      "['+0.06', '+0.72', '+0.28', '-0.94', '-0.09', '-0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.29\n",
      "['+0.13', '+0.36', '+0.45', '-1.50', '-0.53', '-0.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -107.96\n",
      "['+0.20', '+0.00', '+0.73', '-0.34', '-1.48', '-4.72', '+1.00', '+0.00']\n",
      "step 55 total_reward -287.51\n",
      "['-0.00', '+0.94', '-0.15', '+0.12', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.34\n",
      "['-0.03', '+0.90', '-0.15', '-0.41', '+0.04', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.18\n",
      "['-0.06', '+0.69', '-0.15', '-0.92', '+0.07', '+0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.67\n",
      "['-0.11', '+0.47', '-0.35', '-0.54', '+0.13', '+0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -16.12\n",
      "['-0.19', '+0.24', '-0.46', '-1.04', '+0.45', '+0.45', '+0.00', '+0.00']\n",
      "step 80 total_reward -80.85\n",
      "['-0.27', '-0.02', '-0.60', '-0.48', '+0.49', '-3.77', '+0.00', '+1.00']\n",
      "step 95 total_reward -196.23\n",
      "['+0.00', '+0.93', '+0.06', '-0.34', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.39\n",
      "['+0.02', '+0.74', '+0.15', '-0.88', '-0.33', '-0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -70.65\n",
      "['+0.06', '+0.39', '+0.26', '-1.47', '-0.92', '-1.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -156.05\n",
      "['+0.10', '+0.00', '+0.10', '-0.02', '-1.97', '+0.20', '+1.00', '+0.00']\n",
      "step 56 total_reward -342.28\n",
      "['+0.00', '+0.94', '+0.10', '-0.00', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.08\n",
      "['+0.00', '+0.86', '-0.10', '-0.56', '+0.45', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -82.23\n",
      "['-0.02', '+0.60', '-0.20', '-1.18', '+1.71', '+1.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -246.48\n",
      "['-0.15', '+0.13', '-0.55', '-2.06', '+3.47', '+1.92', '+0.00', '+0.00']\n",
      "step 60 total_reward -479.84\n",
      "['-0.17', '+0.01', '-0.29', '+0.02', '+3.82', '+0.87', '+0.00', '+0.00']\n",
      "step 64 total_reward -613.59\n",
      "['-0.01', '+0.95', '-0.28', '+0.27', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.90\n",
      "['-0.05', '+0.94', '-0.09', '-0.28', '-0.38', '-0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.07\n",
      "['-0.06', '+0.78', '-0.03', '-0.84', '-1.39', '-1.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -165.90\n",
      "['-0.04', '+0.44', '+0.46', '-1.52', '-2.28', '-0.78', '+0.00', '+0.00']\n",
      "step 60 total_reward -298.71\n",
      "['+0.06', '+0.00', '+0.86', '-0.46', '-2.93', '-5.74', '+0.00', '+0.00']\n",
      "step 77 total_reward -457.77\n",
      "['+0.00', '+0.93', '+0.15', '-0.40', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.46\n",
      "['+0.03', '+0.72', '+0.13', '-0.93', '-0.01', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.93\n",
      "['+0.07', '+0.36', '+0.32', '-1.48', '-0.35', '-0.73', '+0.00', '+0.00']\n",
      "step 40 total_reward -89.76\n",
      "['+0.11', '+0.02', '-0.03', '-0.09', '-0.92', '+0.73', '+1.00', '+0.00']\n",
      "step 60 total_reward +30.19\n",
      "['+0.11', '-0.01', '-0.07', '-0.21', '-0.31', '+1.79', '+1.00', '+0.00']\n",
      "step 71 total_reward -38.25\n",
      "['-0.00', '+0.93', '-0.22', '-0.52', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.51\n",
      "['-0.06', '+0.68', '-0.37', '-1.07', '+0.40', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.72\n",
      "['-0.17', '+0.30', '-0.72', '-1.53', '+1.47', '+1.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -205.40\n",
      "['-0.25', '+0.04', '+0.21', '-0.14', '+2.06', '+1.90', '+0.00', '+0.00']\n",
      "step 51 total_reward -367.74\n",
      "['+0.01', '+0.95', '+0.37', '+0.18', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.55\n",
      "['+0.08', '+0.92', '+0.37', '-0.36', '-0.09', '-0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.39\n",
      "['+0.15', '+0.72', '+0.37', '-0.89', '-0.17', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -50.88\n",
      "['+0.25', '+0.50', '+0.63', '-0.64', '-0.28', '-0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -42.38\n",
      "['+0.39', '+0.22', '+0.78', '-1.22', '-0.87', '-0.85', '+0.00', '+0.00']\n",
      "step 80 total_reward -145.57\n",
      "['+0.46', '+0.09', '+0.74', '+0.12', '-1.80', '-0.93', '+1.00', '+0.00']\n",
      "step 89 total_reward -284.57\n",
      "['-0.01', '+0.96', '-0.32', '+0.48', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.68\n",
      "['-0.05', '+1.02', '-0.12', '-0.06', '-0.38', '-0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward +1.53\n",
      "['-0.07', '+0.91', '-0.01', '-0.70', '-1.62', '-1.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -168.44\n",
      "['-0.06', '+0.62', '+0.05', '-1.31', '-3.37', '-1.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -376.25\n",
      "['-0.06', '+0.07', '-0.19', '-2.06', '-4.80', '-1.53', '+0.00', '+0.00']\n",
      "step 80 total_reward -545.29\n",
      "['-0.08', '+0.00', '-0.24', '+0.06', '-4.27', '+0.78', '+0.00', '+1.00']\n",
      "step 84 total_reward -516.80\n",
      "['-0.01', '+0.92', '-0.38', '-0.55', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.10', '+0.68', '-0.55', '-1.10', '+0.51', '+0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -85.39\n",
      "['-0.24', '+0.27', '-0.82', '-1.62', '+1.71', '+1.47', '+0.00', '+0.00']\n",
      "step 40 total_reward -233.34\n",
      "['-0.31', '+0.06', '-0.20', '-0.25', '+2.36', '+4.60', '+0.00', '+0.00']\n",
      "step 49 total_reward -396.01\n",
      "['+0.01', '+0.93', '+0.47', '-0.49', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.56\n",
      "['+0.09', '+0.70', '+0.38', '-0.88', '+0.18', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.86\n",
      "['+0.16', '+0.43', '+0.24', '-0.87', '+0.21', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -0.56\n",
      "['+0.22', '+0.09', '+0.31', '-1.32', '-0.15', '-0.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -19.46\n",
      "['+0.24', '-0.03', '+0.23', '-0.81', '-0.05', '+5.04', '+1.00', '+1.00']\n",
      "step 66 total_reward -123.05\n",
      "['-0.00', '+0.95', '-0.08', '+0.18', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.01\n",
      "['-0.02', '+0.92', '-0.06', '-0.36', '+0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.45\n",
      "['-0.03', '+0.72', '-0.04', '-0.89', '-0.14', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -59.56\n",
      "['-0.03', '+0.44', '+0.13', '-0.82', '-0.18', '+0.02', '+0.00', '+0.00']\n",
      "step 60 total_reward -33.01\n",
      "['-0.00', '+0.13', '+0.14', '-1.16', '+0.05', '+0.21', '+0.00', '+0.00']\n",
      "step 80 total_reward -24.33\n",
      "['+0.01', '-0.03', '+0.13', '+0.00', '-0.00', '+0.00', '+1.00', '+1.00']\n",
      "step 89 total_reward -112.26\n",
      "['-0.01', '+0.95', '-0.64', '+0.41', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.29\n",
      "['-0.12', '+0.99', '-0.45', '-0.13', '-0.32', '-0.70', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.76\n",
      "['-0.21', '+0.87', '-0.42', '-0.64', '-1.21', '-0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -113.78\n",
      "['-0.27', '+0.62', '-0.11', '-1.04', '-1.35', '+0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -135.94\n",
      "['-0.29', '+0.24', '+0.12', '-1.36', '-0.69', '+0.94', '+0.00', '+0.00']\n",
      "step 80 total_reward -73.97\n",
      "['-0.27', '+0.02', '+0.54', '-1.16', '-0.45', '-5.05', '+1.00', '+1.00']\n",
      "step 90 total_reward -147.93\n",
      "['-0.01', '+0.93', '-0.45', '-0.26', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.29\n",
      "['-0.09', '+0.77', '-0.41', '-0.79', '+0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -22.84\n",
      "['-0.17', '+0.52', '-0.37', '-0.74', '+0.05', '+0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -1.63\n",
      "['-0.26', '+0.25', '-0.55', '-1.07', '+0.39', '+0.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -57.26\n",
      "['-0.37', '-0.08', '-0.73', '-0.86', '+0.49', '-3.93', '+1.00', '+0.00']\n",
      "step 78 total_reward -195.41\n",
      "['+0.01', '+0.93', '+0.34', '-0.50', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.78\n",
      "['+0.07', '+0.69', '+0.28', '-1.04', '+0.15', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.74\n",
      "['+0.14', '+0.30', '+0.48', '-1.56', '-0.08', '-0.59', '+0.00', '+0.00']\n",
      "step 40 total_reward -51.72\n",
      "['+0.20', '-0.03', '+0.88', '-0.37', '-0.20', '+6.52', '+1.00', '+0.00']\n",
      "step 53 total_reward -190.48\n",
      "['-0.02', '+0.94', '-0.77', '-0.19', '+0.02', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.24\n",
      "['-0.16', '+0.80', '-0.67', '-0.72', '-0.16', '-0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.09\n",
      "['-0.28', '+0.58', '-0.62', '-0.81', '-0.26', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -20.75\n",
      "['-0.40', '+0.33', '-0.62', '-1.00', '+0.05', '+0.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -5.09\n",
      "['-0.49', '+0.02', '-0.52', '-0.04', '+0.13', '-0.00', '+1.00', '+1.00']\n",
      "step 77 total_reward -136.78\n",
      "['-0.00', '+0.93', '-0.04', '-0.42', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.85\n",
      "['-0.00', '+0.72', '+0.01', '-0.96', '-0.21', '-0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -54.88\n",
      "['+0.01', '+0.36', '+0.19', '-1.44', '-0.69', '-0.84', '+0.00', '+0.00']\n",
      "step 40 total_reward -118.42\n",
      "['+0.06', '-0.00', '+0.50', '-0.92', '-1.54', '-6.70', '+1.00', '+0.00']\n",
      "step 56 total_reward -267.01\n",
      "['-0.00', '+0.94', '-0.13', '-0.03', '+0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.09\n",
      "['-0.01', '+0.85', '+0.03', '-0.57', '-0.43', '-0.67', '+0.00', '+0.00']\n",
      "step 20 total_reward -76.22\n",
      "['-0.01', '+0.60', '+0.05', '-1.13', '-1.13', '-0.91', '+0.00', '+0.00']\n",
      "step 40 total_reward -178.05\n",
      "['+0.04', '+0.18', '+0.30', '-1.70', '-2.32', '-1.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -315.03\n",
      "['+0.06', '+0.01', '+0.52', '-0.41', '-2.88', '-5.57', '+0.00', '+0.00']\n",
      "step 67 total_reward -453.93\n",
      "['+0.01', '+0.95', '+0.25', '+0.17', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.84\n",
      "['+0.04', '+0.91', '+0.06', '-0.38', '+0.41', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.86\n",
      "['+0.04', '+0.71', '-0.04', '-1.01', '+1.63', '+1.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -209.41\n",
      "['+0.02', '+0.35', '-0.16', '-1.44', '+2.74', '+0.66', '+0.00', '+0.00']\n",
      "step 60 total_reward -328.10\n",
      "['-0.02', '-0.01', '-0.27', '+0.00', '+3.14', '-0.00', '+0.00', '+0.00']\n",
      "step 75 total_reward -471.33\n",
      "['-0.01', '+0.93', '-0.28', '-0.29', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.70\n",
      "['-0.06', '+0.76', '-0.26', '-0.83', '+0.06', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -37.29\n",
      "['-0.12', '+0.43', '-0.39', '-1.31', '+0.26', '+0.49', '+0.00', '+0.00']\n",
      "step 40 total_reward -76.91\n",
      "['-0.20', '-0.00', '-0.43', '-0.09', '+0.72', '-1.67', '+0.00', '+1.00']\n",
      "step 60 total_reward -199.62\n",
      "['-0.00', '+0.92', '+0.00', '-0.58', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.91\n",
      "['+0.01', '+0.67', '+0.18', '-0.93', '-0.28', '-0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.07\n",
      "['+0.06', '+0.31', '+0.34', '-1.51', '-0.96', '-1.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -136.31\n",
      "['+0.14', '+0.01', '+0.63', '+0.08', '-2.05', '-1.10', '+1.00', '+0.00']\n",
      "step 54 total_reward -325.52\n",
      "['-0.00', '+0.93', '-0.19', '-0.37', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.63\n",
      "['-0.04', '+0.74', '-0.16', '-0.90', '+0.03', '-0.06', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.56\n",
      "['-0.09', '+0.41', '-0.36', '-1.23', '+0.20', '+0.38', '+0.00', '+0.00']\n",
      "step 40 total_reward -60.12\n",
      "['-0.16', '-0.00', '-0.27', '-0.18', '+0.62', '-2.87', '+0.00', '+1.00']\n",
      "step 59 total_reward -186.03\n",
      "['+0.01', '+0.95', '+0.68', '+0.13', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.66\n",
      "['+0.15', '+0.90', '+0.66', '-0.40', '-0.03', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.84\n",
      "['+0.28', '+0.70', '+0.68', '-0.93', '-0.04', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -28.39\n",
      "['+0.43', '+0.33', '+0.87', '-1.50', '-0.56', '-0.88', '+0.00', '+0.00']\n",
      "step 60 total_reward -116.90\n",
      "['+0.59', '-0.12', '+1.34', '-0.88', '-1.74', '-7.47', '+1.00', '+0.00']\n",
      "step 78 total_reward -335.38\n",
      "['-0.01', '+0.93', '-0.69', '-0.23', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.23\n",
      "['-0.14', '+0.78', '-0.63', '-0.75', '-0.17', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.26\n",
      "['-0.27', '+0.53', '-0.72', '-1.01', '-0.01', '+0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -19.27\n",
      "['-0.42', '+0.14', '-0.76', '-1.52', '+0.71', '+0.55', '+0.00', '+1.00']\n",
      "step 60 total_reward -112.27\n",
      "['-0.43', '+0.10', '+0.17', '-0.05', '+1.34', '+0.06', '+0.00', '+1.00']\n",
      "step 64 total_reward -185.46\n",
      "['-0.00', '+0.94', '-0.17', '-0.14', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.49\n",
      "['-0.04', '+0.81', '-0.17', '-0.67', '+0.04', '+0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.15\n",
      "['-0.07', '+0.55', '-0.24', '-0.89', '+0.12', '+0.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -46.53\n",
      "['-0.14', '+0.26', '-0.43', '-1.20', '+0.51', '+0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -97.70\n",
      "['-0.22', '-0.02', '-1.07', '-0.72', '+0.72', '-3.13', '+0.00', '+1.00']\n",
      "step 75 total_reward -231.69\n",
      "['+0.01', '+0.94', '+0.26', '+0.06', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.27\n",
      "['+0.04', '+0.88', '+0.14', '-0.47', '+0.36', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -50.84\n",
      "['+0.07', '+0.65', '+0.15', '-1.00', '+0.79', '+0.39', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.15\n",
      "['+0.12', '+0.28', '+0.29', '-1.44', '+0.70', '-0.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -126.73\n",
      "['+0.14', '-0.01', '+0.35', '-0.78', '+0.28', '-5.91', '+0.00', '+1.00']\n",
      "step 73 total_reward -185.35\n",
      "['-0.01', '+0.94', '-0.36', '-0.18', '+0.01', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.05\n",
      "['-0.07', '+0.80', '-0.28', '-0.71', '-0.17', '-0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.12', '+0.53', '-0.18', '-0.97', '-0.34', '+0.02', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.75\n",
      "['-0.16', '+0.19', '-0.18', '-1.29', '-0.10', '+0.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -35.74\n",
      "['-0.17', '-0.02', '+0.22', '-0.02', '-0.08', '+0.14', '+1.00', '+1.00']\n",
      "step 71 total_reward -110.37\n",
      "['+0.01', '+0.95', '+0.64', '+0.14', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.44\n",
      "['+0.15', '+0.90', '+0.81', '-0.42', '-0.61', '-0.91', '+0.00', '+0.00']\n",
      "step 20 total_reward -84.21\n",
      "['+0.31', '+0.70', '+0.82', '-1.00', '-1.56', '-1.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -202.19\n",
      "['+0.48', '+0.31', '+0.69', '-1.61', '-3.26', '-2.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -399.46\n",
      "['+0.57', '-0.03', '+0.64', '-0.13', '-4.65', '+1.48', '+0.00', '+0.00']\n",
      "step 73 total_reward -653.07\n",
      "['+0.01', '+0.95', '+0.33', '+0.16', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.20\n",
      "['+0.09', '+0.91', '+0.51', '-0.40', '-0.52', '-0.89', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.45\n",
      "['+0.19', '+0.71', '+0.81', '-0.90', '-1.43', '-0.90', '+0.00', '+0.00']\n",
      "step 40 total_reward -208.47\n",
      "['+0.48', '+0.36', '+1.55', '-1.53', '-2.45', '-1.34', '+0.00', '+0.00']\n",
      "step 60 total_reward -397.15\n",
      "['+0.71', '-0.02', '+2.05', '-0.58', '-3.54', '+0.00', '+0.00', '+0.00']\n",
      "step 75 total_reward -642.51\n",
      "['+0.01', '+0.95', '+0.63', '+0.21', '-0.02', '-0.18', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.35\n",
      "['+0.15', '+0.93', '+0.81', '-0.35', '-0.61', '-0.92', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.50\n",
      "['+0.31', '+0.74', '+0.81', '-0.91', '-1.54', '-1.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -194.78\n",
      "['+0.47', '+0.38', '+0.69', '-1.53', '-3.10', '-1.97', '+0.00', '+0.00']\n",
      "step 60 total_reward -378.00\n",
      "['+0.56', '+0.02', '-0.60', '+0.01', '-3.88', '+1.79', '+0.00', '+0.00']\n",
      "step 76 total_reward -465.49\n",
      "['-0.00', '+0.94', '-0.16', '-0.11', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.10\n",
      "['-0.03', '+0.82', '-0.18', '-0.65', '-0.06', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.11\n",
      "['-0.09', '+0.54', '-0.43', '-1.10', '+0.45', '+0.80', '+0.00', '+0.00']\n",
      "step 40 total_reward -107.06\n",
      "['-0.19', '+0.16', '-0.55', '-1.57', '+1.50', '+1.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -230.24\n",
      "['-0.21', '+0.02', '+0.57', '-0.28', '+1.66', '-2.12', '+0.00', '+1.00']\n",
      "step 66 total_reward -358.25\n",
      "['+0.00', '+0.95', '+0.13', '+0.20', '-0.00', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.97\n",
      "['+0.04', '+0.92', '+0.30', '-0.35', '-0.46', '-0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -65.42\n",
      "['+0.10', '+0.74', '+0.31', '-0.90', '-1.28', '-0.88', '+0.00', '+0.00']\n",
      "step 40 total_reward -178.51\n",
      "['+0.20', '+0.37', '+0.64', '-1.61', '-2.54', '-1.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -352.15\n",
      "['+0.29', '-0.01', '+0.73', '-0.48', '-3.60', '+4.38', '+0.00', '+0.00']\n",
      "step 74 total_reward -575.41\n",
      "['+0.01', '+0.94', '+0.46', '-0.13', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.91\n",
      "['+0.10', '+0.81', '+0.46', '-0.67', '-0.11', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.48\n",
      "['+0.21', '+0.60', '+0.63', '-0.60', '-0.22', '-0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -34.80\n",
      "['+0.36', '+0.45', '+0.89', '-0.63', '-0.40', '-0.44', '+0.00', '+0.00']\n",
      "step 60 total_reward -71.94\n",
      "['+0.54', '+0.18', '+0.87', '-0.87', '-1.38', '-4.10', '+1.00', '+0.00']\n",
      "step 80 total_reward -173.63\n",
      "['+0.56', '+0.17', '+0.60', '+0.12', '-1.72', '-0.96', '+1.00', '+0.00']\n",
      "step 82 total_reward -296.10\n",
      "['+0.01', '+0.94', '+0.72', '-0.13', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.11\n",
      "['+0.15', '+0.82', '+0.62', '-0.66', '+0.16', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.95\n",
      "['+0.26', '+0.61', '+0.35', '-0.59', '+0.46', '+0.32', '+0.00', '+0.00']\n",
      "step 40 total_reward -17.02\n",
      "['+0.29', '+0.42', '+0.21', '-0.79', '+0.60', '-0.23', '+0.00', '+0.00']\n",
      "step 60 total_reward -32.71\n",
      "['+0.34', '+0.11', '+0.29', '-1.29', '+0.05', '-0.63', '+0.00', '+0.00']\n",
      "step 80 total_reward -12.80\n",
      "['+0.36', '-0.01', '-0.10', '-0.68', '+0.00', '+4.75', '+1.00', '+1.00']\n",
      "step 86 total_reward -112.58\n",
      "['-0.01', '+0.93', '-0.63', '-0.37', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.23\n",
      "['-0.14', '+0.73', '-0.66', '-0.86', '+0.22', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.33\n",
      "['-0.30', '+0.51', '-0.93', '-0.94', '+0.55', '+0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -83.64\n",
      "['-0.49', '+0.14', '-0.96', '-1.47', '+1.41', '+0.65', '+0.00', '+0.00']\n",
      "step 60 total_reward -205.15\n",
      "['-0.52', '+0.06', '+0.17', '-0.40', '+1.42', '-3.84', '+0.00', '+1.00']\n",
      "step 64 total_reward -323.75\n",
      "['-0.00', '+0.93', '-0.14', '-0.31', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.79\n",
      "['-0.04', '+0.76', '-0.19', '-0.84', '+0.26', '+0.26', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.49\n",
      "['-0.11', '+0.52', '-0.64', '-0.63', '+0.53', '+0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.75\n",
      "['-0.28', '+0.31', '-0.93', '-0.97', '+1.06', '+0.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -164.94\n",
      "['-0.50', '+0.05', '-0.40', '-0.08', '+1.62', '-2.70', '+0.00', '+1.00']\n",
      "step 77 total_reward -381.36\n",
      "['+0.01', '+0.92', '+0.68', '-0.58', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.50\n",
      "['+0.14', '+0.68', '+0.66', '-0.82', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 20 total_reward +4.39\n",
      "['+0.28', '+0.49', '+0.75', '-0.54', '+0.03', '-0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward +22.40\n",
      "['+0.45', '+0.25', '+0.95', '-1.09', '-0.48', '-0.91', '+0.00', '+0.00']\n",
      "step 60 total_reward -69.06\n",
      "['+0.53', '+0.11', '+0.26', '+0.19', '-1.54', '-1.17', '+1.00', '+0.00']\n",
      "step 70 total_reward -203.51\n",
      "['+0.00', '+0.94', '+0.17', '-0.11', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.15\n",
      "['+0.02', '+0.82', '+0.05', '-0.64', '+0.33', '+0.46', '+0.00', '+0.00']\n",
      "step 20 total_reward -66.71\n",
      "['+0.04', '+0.55', '+0.09', '-1.16', '+0.80', '+0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -137.77\n",
      "['+0.06', '+0.14', '-0.10', '-1.45', '+0.63', '-0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -112.23\n",
      "['+0.03', '-0.02', '-0.18', '-0.53', '+0.19', '-4.74', '+0.00', '+1.00']\n",
      "step 68 total_reward -161.65\n",
      "['+0.01', '+0.92', '+0.42', '-0.56', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.75\n",
      "['+0.08', '+0.68', '+0.34', '-0.97', '+0.11', '+0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.76\n",
      "['+0.15', '+0.41', '+0.31', '-0.94', '+0.15', '-0.15', '+0.00', '+0.00']\n",
      "step 40 total_reward -0.69\n",
      "['+0.21', '+0.05', '+0.35', '-1.47', '-0.16', '-0.33', '+1.00', '+0.00']\n",
      "step 60 total_reward -22.25\n",
      "['+0.22', '+0.01', '-0.56', '-0.51', '-0.09', '+3.79', '+1.00', '+0.00']\n",
      "step 62 total_reward -116.44\n",
      "['-0.01', '+0.95', '-0.50', '+0.35', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.63\n",
      "['-0.11', '+0.97', '-0.50', '-0.19', '+0.17', '+0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -11.11\n",
      "['-0.21', '+0.83', '-0.51', '-0.72', '+0.35', '+0.19', '+0.00', '+0.00']\n",
      "step 40 total_reward -52.01\n",
      "['-0.35', '+0.60', '-1.00', '-0.84', '+0.64', '+0.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -111.55\n",
      "['-0.60', '+0.32', '-1.34', '-1.22', '+1.31', '+1.08', '+0.00', '+0.00']\n",
      "step 80 total_reward -229.94\n",
      "['-0.81', '-0.01', '-0.68', '-0.88', '+2.45', '+7.47', '+0.00', '+0.00']\n",
      "step 95 total_reward -475.46\n",
      "['+0.01', '+0.95', '+0.28', '+0.25', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.07\n",
      "['+0.07', '+0.94', '+0.43', '-0.29', '-0.50', '-0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.93\n",
      "['+0.16', '+0.77', '+0.42', '-0.83', '-1.20', '-0.71', '+0.00', '+0.00']\n",
      "step 40 total_reward -159.53\n",
      "['+0.38', '+0.46', '+1.61', '-1.34', '-1.91', '-0.86', '+0.00', '+0.00']\n",
      "step 60 total_reward -332.24\n",
      "['+0.69', '+0.00', '+0.38', '-0.49', '-3.01', '+5.51', '+0.00', '+0.00']\n",
      "step 79 total_reward -583.85\n",
      "['-0.01', '+0.95', '-0.42', '+0.14', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.21\n",
      "['-0.08', '+0.90', '-0.24', '-0.41', '-0.34', '-0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.83\n",
      "['-0.13', '+0.70', '-0.18', '-0.91', '-1.00', '-0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -123.54\n",
      "['-0.13', '+0.37', '+0.18', '-1.27', '-1.28', '+0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -157.81\n",
      "['-0.07', '+0.00', '+0.66', '-0.66', '-1.97', '-3.78', '+1.00', '+0.00']\n",
      "step 80 total_reward -151.85\n",
      "['-0.07', '+0.00', '+0.55', '+0.11', '-2.02', '-1.26', '+1.00', '+0.00']\n",
      "step 81 total_reward -251.85\n",
      "['-0.00', '+0.94', '-0.20', '-0.06', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.04', '+0.84', '-0.07', '-0.59', '-0.08', '-0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.05\n",
      "['-0.05', '+0.60', '+0.02', '-0.87', '-0.52', '-0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -87.87\n",
      "['+0.03', '+0.36', '+0.63', '-0.93', '-0.87', '-0.61', '+0.00', '+0.00']\n",
      "step 60 total_reward -128.43\n",
      "['+0.17', '+0.02', '+0.76', '-1.04', '-1.63', '-3.75', '+1.00', '+0.00']\n",
      "step 80 total_reward -192.79\n",
      "['+0.18', '+0.01', '+0.52', '+0.16', '-1.98', '-1.71', '+1.00', '+0.00']\n",
      "step 82 total_reward -314.58\n",
      "['-0.01', '+0.92', '-0.27', '-0.58', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.73\n",
      "['-0.06', '+0.67', '-0.25', '-1.11', '-0.02', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.57\n",
      "['-0.12', '+0.31', '-0.40', '-1.39', '+0.20', '+0.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.12\n",
      "['-0.19', '-0.01', '-0.43', '-0.48', '+0.35', '-4.35', '+0.00', '+1.00']\n",
      "step 56 total_reward -154.10\n",
      "['+0.02', '+0.94', '+0.79', '+0.03', '-0.02', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.60\n",
      "['+0.16', '+0.87', '+0.69', '-0.50', '+0.17', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.66\n",
      "['+0.30', '+0.63', '+0.73', '-1.02', '+0.41', '+0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -61.83\n",
      "['+0.43', '+0.30', '+0.71', '-1.33', '+0.19', '-0.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -48.55\n",
      "['+0.51', '+0.01', '-0.41', '-0.43', '-0.10', '+3.72', '+1.00', '+0.00']\n",
      "step 73 total_reward -151.96\n",
      "['-0.01', '+0.94', '-0.53', '-0.08', '+0.01', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.73\n",
      "['-0.12', '+0.83', '-0.53', '-0.62', '+0.21', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.66\n",
      "['-0.23', '+0.61', '-0.68', '-0.75', '+0.39', '+0.34', '+0.00', '+0.00']\n",
      "step 40 total_reward -62.15\n",
      "['-0.38', '+0.30', '-0.80', '-1.34', '+1.15', '+0.98', '+0.00', '+0.00']\n",
      "step 60 total_reward -175.90\n",
      "['-0.55', '-0.15', '-1.67', '-0.38', '+2.45', '+1.68', '+0.00', '+1.00']\n",
      "step 80 total_reward -413.86\n",
      "['-0.01', '+0.93', '-0.62', '-0.28', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.22\n",
      "['-0.14', '+0.76', '-0.62', '-0.81', '+0.15', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.72\n",
      "['-0.28', '+0.52', '-0.83', '-1.00', '+0.41', '+0.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -70.43\n",
      "['-0.45', '+0.13', '-0.86', '-1.53', '+1.19', '+0.63', '+0.00', '+0.00']\n",
      "step 60 total_reward -182.27\n",
      "['-0.56', '-0.14', '-1.51', '-0.15', '+2.00', '+0.74', '+0.00', '+1.00']\n",
      "step 72 total_reward -335.60\n",
      "['-0.01', '+0.94', '-0.45', '+0.13', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.13\n",
      "['-0.10', '+0.90', '-0.45', '-0.40', '+0.11', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.67\n",
      "['-0.19', '+0.70', '-0.54', '-0.75', '+0.21', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.44\n",
      "['-0.31', '+0.46', '-0.72', '-1.06', '+0.51', '+0.65', '+0.00', '+0.00']\n",
      "step 60 total_reward -95.16\n",
      "['-0.46', '+0.06', '-0.72', '-1.56', '+1.24', '+0.47', '+0.00', '+0.00']\n",
      "step 80 total_reward -203.25\n",
      "['-0.53', '-0.06', '-0.87', '+0.07', '+2.14', '+1.34', '+0.00', '+0.00']\n",
      "step 88 total_reward -344.99\n",
      "['-0.00', '+0.95', '-0.22', '+0.41', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.07\n",
      "['-0.04', '+0.99', '-0.19', '-0.12', '-0.09', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward +13.36\n",
      "['-0.08', '+0.87', '-0.11', '-0.66', '-0.32', '-0.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.78\n",
      "['-0.10', '+0.60', '-0.04', '-1.08', '-0.72', '-0.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -97.58\n",
      "['-0.07', '+0.25', '+0.25', '-1.29', '-0.72', '+0.28', '+0.00', '+0.00']\n",
      "step 80 total_reward -88.53\n",
      "['-0.02', '-0.01', '+0.20', '-0.27', '-0.41', '+3.68', '+1.00', '+0.00']\n",
      "step 93 total_reward -153.64\n",
      "['-0.02', '+0.95', '-0.80', '+0.46', '+0.02', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.58\n",
      "['-0.17', '+1.01', '-0.78', '-0.08', '+0.09', '+0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward +0.45\n",
      "['-0.32', '+0.90', '-0.78', '-0.61', '+0.16', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -20.78\n",
      "['-0.49', '+0.68', '-0.98', '-0.68', '+0.21', '+0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -38.00\n",
      "['-0.71', '+0.39', '-1.17', '-1.27', '+0.70', '+0.91', '+0.00', '+0.00']\n",
      "step 80 total_reward -137.19\n",
      "['-0.92', '+0.03', '-1.99', '-0.16', '+2.17', '+2.96', '+0.00', '+1.00']\n",
      "step 97 total_reward -376.76\n",
      "['+0.01', '+0.93', '+0.44', '-0.46', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.85\n",
      "['+0.08', '+0.71', '+0.26', '-1.00', '+0.33', '+0.63', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.28\n",
      "['+0.15', '+0.33', '+0.44', '-1.47', '+0.52', '-0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -87.56\n",
      "['+0.22', '-0.03', '+0.79', '+0.18', '-0.10', '-1.22', '+1.00', '+1.00']\n",
      "step 55 total_reward -161.12\n",
      "['+0.01', '+0.95', '+0.52', '+0.37', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.55\n",
      "['+0.10', '+0.98', '+0.32', '-0.18', '+0.33', '+0.72', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.40\n",
      "['+0.16', '+0.84', '+0.29', '-0.71', '+1.22', '+0.84', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.28\n",
      "['+0.16', '+0.55', '-0.55', '-1.36', '+2.06', '+0.83', '+0.00', '+0.00']\n",
      "step 60 total_reward -253.63\n",
      "['+0.02', '+0.06', '-0.65', '-1.88', '+2.54', '+0.45', '+0.00', '+0.00']\n",
      "step 80 total_reward -304.03\n",
      "['+0.01', '+0.01', '-0.92', '-0.17', '+2.61', '+3.35', '+0.00', '+0.00']\n",
      "step 82 total_reward -405.90\n",
      "['+0.01', '+0.94', '+0.56', '+0.08', '-0.01', '-0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.69\n",
      "['+0.14', '+0.88', '+0.72', '-0.47', '-0.58', '-0.84', '+0.00', '+0.00']\n",
      "step 20 total_reward -83.52\n",
      "['+0.28', '+0.66', '+0.74', '-1.06', '-1.51', '-1.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -203.01\n",
      "['+0.43', '+0.25', '+0.63', '-1.67', '-3.23', '-2.10', '+0.00', '+0.00']\n",
      "step 60 total_reward -402.29\n",
      "['+0.49', '+0.02', '+1.04', '-0.24', '-4.30', '-4.23', '+0.00', '+1.00']\n",
      "step 69 total_reward -607.21\n",
      "['+0.00', '+0.92', '+0.23', '-0.58', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.25\n",
      "['+0.04', '+0.67', '+0.07', '-1.06', '+0.27', '+0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.05\n",
      "['+0.07', '+0.27', '+0.25', '-1.56', '+0.30', '-0.31', '+0.00', '+0.00']\n",
      "step 40 total_reward -63.40\n",
      "['+0.09', '-0.03', '+0.17', '-0.87', '+0.02', '-4.85', '+1.00', '+1.00']\n",
      "step 54 total_reward -95.64\n",
      "['-0.00', '+0.93', '-0.01', '-0.39', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.16\n",
      "['-0.01', '+0.73', '-0.05', '-0.93', '+0.20', '+0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -55.46\n",
      "['-0.06', '+0.50', '-0.50', '-0.57', '+0.39', '+0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward -39.88\n",
      "['-0.18', '+0.26', '-0.67', '-1.06', '+0.87', '+0.75', '+0.00', '+0.00']\n",
      "step 60 total_reward -120.54\n",
      "['-0.33', '+0.02', '-1.11', '+0.12', '+2.22', '+2.65', '+0.00', '+0.00']\n",
      "step 79 total_reward -354.93\n",
      "['+0.01', '+0.95', '+0.64', '+0.34', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.08\n",
      "['+0.12', '+0.97', '+0.44', '-0.20', '+0.29', '+0.67', '+0.00', '+0.00']\n",
      "step 20 total_reward -6.31\n",
      "['+0.21', '+0.83', '+0.43', '-0.73', '+1.07', '+0.70', '+0.00', '+0.00']\n",
      "step 40 total_reward -107.88\n",
      "['+0.30', '+0.54', '+0.20', '-1.15', '+1.45', '+0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -156.85\n",
      "['+0.34', '+0.13', '+0.22', '-1.61', '+1.17', '-0.45', '+0.00', '+0.00']\n",
      "step 80 total_reward -147.97\n",
      "['+0.37', '-0.05', '+0.81', '-0.08', '+1.65', '+1.29', '+0.00', '+0.00']\n",
      "step 89 total_reward -227.07\n",
      "['-0.01', '+0.95', '-0.71', '+0.40', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.07\n",
      "['-0.15', '+0.99', '-0.71', '-0.13', '+0.13', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -6.23\n",
      "['-0.29', '+0.86', '-0.71', '-0.67', '+0.25', '+0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -34.58\n",
      "['-0.45', '+0.63', '-0.97', '-0.75', '+0.37', '+0.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -61.33\n",
      "['-0.66', '+0.31', '-1.13', '-1.36', '+0.99', '+1.02', '+0.00', '+0.00']\n",
      "step 80 total_reward -173.55\n",
      "['-0.83', '+0.02', '-1.73', '-0.35', '+2.15', '+0.74', '+0.00', '+1.00']\n",
      "step 94 total_reward -386.81\n",
      "['+0.00', '+0.93', '+0.14', '-0.31', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.32\n",
      "['+0.02', '+0.76', '+0.08', '-0.84', '+0.21', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -56.22\n",
      "['+0.04', '+0.42', '+0.16', '-1.35', '+0.41', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -94.96\n",
      "['+0.07', '-0.03', '+0.12', '-0.00', '-0.00', '-0.00', '+1.00', '+1.00']\n",
      "step 59 total_reward -172.32\n",
      "['+0.01', '+0.95', '+0.58', '+0.36', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.13', '+0.98', '+0.58', '-0.17', '-0.11', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -4.03\n",
      "['+0.24', '+0.84', '+0.58', '-0.70', '-0.21', '-0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -33.64\n",
      "['+0.36', '+0.55', '+0.67', '-1.26', '-0.41', '-0.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -83.71\n",
      "['+0.51', '+0.08', '+0.77', '-1.84', '-1.30', '-1.06', '+0.00', '+0.00']\n",
      "step 80 total_reward -216.11\n",
      "['+0.54', '-0.02', '+1.57', '-0.70', '-1.61', '-4.73', '+1.00', '+0.00']\n",
      "step 84 total_reward -330.67\n",
      "['+0.00', '+0.94', '+0.22', '+0.11', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.82\n",
      "['+0.03', '+0.89', '+0.03', '-0.45', '+0.40', '+0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -52.77\n",
      "['+0.04', '+0.67', '-0.04', '-1.04', '+1.43', '+1.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -193.97\n",
      "['+0.02', '+0.31', '-0.06', '-1.48', '+2.24', '+0.62', '+0.00', '+0.00']\n",
      "step 60 total_reward -283.00\n",
      "['-0.02', '+0.01', '-0.65', '-0.35', '+2.89', '+5.22', '+0.00', '+0.00']\n",
      "step 72 total_reward -453.80\n",
      "['+0.00', '+0.93', '+0.04', '-0.44', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.24\n",
      "['+0.02', '+0.71', '+0.09', '-0.98', '-0.25', '-0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -59.61\n",
      "['+0.04', '+0.33', '+0.24', '-1.56', '-0.78', '-0.96', '+0.00', '+0.00']\n",
      "step 40 total_reward -135.25\n",
      "['+0.08', '-0.01', '+0.36', '+0.14', '-1.75', '-1.11', '+1.00', '+0.00']\n",
      "step 54 total_reward -297.96\n",
      "['-0.02', '+0.93', '-0.79', '-0.47', '+0.02', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.41\n",
      "['-0.16', '+0.71', '-0.70', '-0.84', '-0.05', '-0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.08\n",
      "['-0.30', '+0.42', '-0.80', '-1.21', '+0.22', '+0.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -35.81\n",
      "['-0.45', '+0.07', '-1.08', '-0.46', '+0.75', '-3.34', '+0.00', '+1.00']\n",
      "step 57 total_reward -193.60\n",
      "['+0.00', '+0.93', '+0.23', '-0.44', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.44\n",
      "['+0.04', '+0.71', '+0.15', '-0.98', '+0.14', '+0.23', '+0.00', '+0.00']\n",
      "step 20 total_reward -43.59\n",
      "['+0.08', '+0.34', '+0.31', '-1.49', '+0.04', '-0.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -51.88\n",
      "['+0.13', '-0.03', '+0.22', '-0.19', '-0.01', '-1.01', '+0.00', '+1.00']\n",
      "step 55 total_reward -176.87\n",
      "['-0.01', '+0.94', '-0.52', '-0.05', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.65\n",
      "['-0.10', '+0.84', '-0.44', '-0.58', '-0.19', '-0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.49\n",
      "['-0.18', '+0.62', '-0.22', '-0.72', '-0.46', '-0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -41.24\n",
      "['-0.14', '+0.45', '+0.54', '-0.51', '-0.70', '-0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -53.06\n",
      "['-0.03', '+0.24', '+0.70', '-0.71', '-0.47', '+0.42', '+0.00', '+0.00']\n",
      "step 80 total_reward -33.85\n",
      "['+0.17', '+0.04', '+1.07', '-0.75', '-0.03', '+0.29', '+0.00', '+0.00']\n",
      "step 100 total_reward -17.98\n",
      "['+0.22', '-0.01', '+0.59', '-0.33', '+0.28', '+3.64', '+1.00', '+1.00']\n",
      "step 105 total_reward -95.94\n",
      "['-0.01', '+0.93', '-0.54', '-0.33', '+0.01', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.62\n",
      "['-0.11', '+0.75', '-0.49', '-0.86', '+0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -20.11\n",
      "['-0.21', '+0.54', '-0.47', '-0.50', '-0.02', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward +20.53\n",
      "['-0.31', '+0.38', '-0.60', '-0.77', '+0.19', '+0.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -18.44\n",
      "['-0.44', '+0.09', '-0.65', '-1.14', '+0.69', '+0.16', '+0.00', '+0.00']\n",
      "step 80 total_reward -98.59\n",
      "['-0.50', '+0.02', '-0.34', '+0.13', '+1.87', '+1.22', '+0.00', '+0.00']\n",
      "step 91 total_reward -250.85\n",
      "['+0.01', '+0.95', '+0.59', '+0.35', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.21\n",
      "['+0.13', '+0.97', '+0.60', '-0.18', '-0.15', '-0.13', '+0.00', '+0.00']\n",
      "step 20 total_reward -9.35\n",
      "['+0.25', '+0.83', '+0.59', '-0.72', '-0.28', '-0.13', '+0.00', '+0.00']\n",
      "step 40 total_reward -42.46\n",
      "['+0.37', '+0.55', '+0.72', '-1.15', '-0.49', '-0.46', '+0.00', '+0.00']\n",
      "step 60 total_reward -86.84\n",
      "['+0.51', '+0.16', '-0.24', '-0.25', '-1.31', '+1.36', '+1.00', '+0.00']\n",
      "step 79 total_reward -289.85\n",
      "['+0.01', '+0.93', '+0.42', '-0.48', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.62\n",
      "['+0.09', '+0.70', '+0.39', '-1.01', '+0.01', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.52\n",
      "['+0.18', '+0.33', '+0.59', '-1.46', '-0.34', '-0.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -74.96\n",
      "['+0.25', '+0.02', '-0.25', '+0.26', '-1.39', '-2.44', '+1.00', '+0.00']\n",
      "step 54 total_reward -192.45\n",
      "['+0.01', '+0.94', '+0.39', '-0.12', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.82\n",
      "['+0.08', '+0.82', '+0.38', '-0.65', '-0.06', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.20\n",
      "['+0.16', '+0.54', '+0.53', '-1.06', '-0.27', '-0.41', '+0.00', '+0.00']\n",
      "step 40 total_reward -70.16\n",
      "['+0.29', '+0.20', '+0.69', '-1.38', '-0.89', '-0.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -147.79\n",
      "['+0.38', '-0.02', '+1.09', '+0.08', '-1.96', '-2.09', '+1.00', '+0.00']\n",
      "step 72 total_reward -305.47\n",
      "['+0.00', '+0.94', '+0.13', '+0.11', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.09\n",
      "['+0.01', '+0.89', '-0.01', '-0.43', '+0.38', '+0.59', '+0.00', '+0.00']\n",
      "step 20 total_reward -57.46\n",
      "['+0.02', '+0.68', '-0.02', '-0.98', '+0.85', '+0.69', '+0.00', '+0.00']\n",
      "step 40 total_reward -138.91\n",
      "['+0.03', '+0.32', '+0.08', '-1.41', '+1.06', '-0.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -166.88\n",
      "['+0.04', '+0.00', '-0.27', '-0.19', '+1.21', '+5.20', '+0.00', '+1.00']\n",
      "step 74 total_reward -253.58\n",
      "['-0.00', '+0.95', '-0.10', '+0.45', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.99\n",
      "['-0.00', '+1.01', '+0.10', '-0.10', '-0.44', '-0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.80\n",
      "['+0.02', '+0.90', '+0.12', '-0.60', '-1.40', '-0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -148.45\n",
      "['+0.04', '+0.65', '+0.11', '-1.06', '-1.56', '-0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -186.61\n",
      "['+0.06', '+0.23', '+0.07', '-1.73', '-2.18', '-1.10', '+0.00', '+0.00']\n",
      "step 80 total_reward -273.81\n",
      "['+0.07', '+0.00', '+0.27', '-0.42', '-2.95', '-5.79', '+0.00', '+0.00']\n",
      "step 89 total_reward -429.54\n",
      "['-0.01', '+0.94', '-0.61', '-0.02', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.70\n",
      "['-0.14', '+0.85', '-0.62', '-0.55', '+0.27', '+0.23', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.43\n",
      "['-0.28', '+0.65', '-0.94', '-0.67', '+0.58', '+0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -93.23\n",
      "['-0.51', '+0.38', '-1.30', '-1.15', '+1.35', '+1.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -222.39\n",
      "['-0.72', '+0.07', '-1.48', '-0.16', '+2.54', '+1.83', '+0.00', '+0.00']\n",
      "step 76 total_reward -473.04\n",
      "['-0.00', '+0.94', '-0.18', '-0.07', '+0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.55\n",
      "['-0.02', '+0.83', '-0.00', '-0.62', '-0.41', '-0.73', '+0.00', '+0.00']\n",
      "step 20 total_reward -72.79\n",
      "['-0.02', '+0.58', '+0.16', '-1.10', '-1.14', '-0.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -170.88\n",
      "['+0.02', '+0.15', '+0.33', '-1.82', '-2.54', '-1.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -342.77\n",
      "['+0.05', '-0.00', '+0.35', '+0.03', '-3.15', '-0.20', '+0.00', '+0.00']\n",
      "step 66 total_reward -482.74\n",
      "['+0.00', '+0.93', '+0.17', '-0.34', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.32\n",
      "['+0.03', '+0.75', '+0.10', '-0.87', '+0.20', '+0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -53.10\n",
      "['+0.05', '+0.41', '+0.20', '-1.37', '+0.36', '-0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -87.80\n",
      "['+0.09', '-0.03', '+0.29', '-1.09', '-0.00', '-6.41', '+1.00', '+1.00']\n",
      "step 58 total_reward -168.58\n",
      "['-0.01', '+0.95', '-0.49', '+0.18', '+0.01', '+0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.87\n",
      "['-0.12', '+0.92', '-0.67', '-0.38', '+0.55', '+0.92', '+0.00', '+0.00']\n",
      "step 20 total_reward -77.96\n",
      "['-0.26', '+0.71', '-0.74', '-1.03', '+1.97', '+1.89', '+0.00', '+0.00']\n",
      "step 40 total_reward -254.41\n",
      "['-0.41', '+0.31', '-0.61', '-1.62', '+4.26', '+2.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -505.89\n",
      "['-0.51', '+0.01', '-1.22', '-0.25', '+4.62', '-0.56', '+1.00', '+0.00']\n",
      "step 73 total_reward -604.50\n",
      "['-0.00', '+0.95', '-0.04', '+0.27', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.28\n",
      "['+0.01', '+0.94', '+0.16', '-0.29', '-0.48', '-0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -51.65\n",
      "['+0.05', '+0.77', '+0.25', '-0.92', '-1.79', '-1.67', '+0.00', '+0.00']\n",
      "step 40 total_reward -228.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.10', '+0.41', '+0.11', '-1.47', '-3.73', '-2.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -440.32\n",
      "['+0.13', '-0.01', '+0.37', '-0.46', '-5.99', '-4.25', '+0.00', '+1.00']\n",
      "step 79 total_reward -653.63\n",
      "['+0.01', '+0.94', '+0.32', '+0.11', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.44\n",
      "['+0.07', '+0.89', '+0.32', '-0.42', '-0.08', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -21.24\n",
      "['+0.13', '+0.68', '+0.35', '-0.85', '-0.15', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -48.18\n",
      "['+0.23', '+0.43', '+0.66', '-0.70', '-0.34', '-0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -55.92\n",
      "['+0.37', '+0.13', '+0.73', '-1.26', '-0.83', '-0.54', '+0.00', '+0.00']\n",
      "step 80 total_reward -144.06\n",
      "['+0.45', '-0.08', '+0.05', '+0.03', '-1.59', '-0.20', '+1.00', '+0.00']\n",
      "step 92 total_reward -289.49\n",
      "['+0.01', '+0.94', '+0.33', '+0.02', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.02\n",
      "['+0.06', '+0.86', '+0.16', '-0.52', '+0.36', '+0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.00\n",
      "['+0.11', '+0.63', '+0.30', '-1.00', '+0.66', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -106.71\n",
      "['+0.18', '+0.26', '+0.47', '-1.47', '+0.14', '-0.87', '+0.00', '+0.00']\n",
      "step 60 total_reward -72.78\n",
      "['+0.22', '+0.00', '-0.54', '-0.45', '-0.11', '+3.94', '+1.00', '+0.00']\n",
      "step 71 total_reward -180.75\n",
      "['+0.01', '+0.95', '+0.34', '+0.39', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.10\n",
      "['+0.09', '+0.99', '+0.53', '-0.17', '-0.55', '-0.93', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.05\n",
      "['+0.20', '+0.85', '+0.60', '-0.79', '-1.92', '-1.66', '+0.00', '+0.00']\n",
      "step 40 total_reward -231.53\n",
      "['+0.38', '+0.45', '+0.94', '-1.85', '-3.57', '-1.80', '+0.00', '+0.00']\n",
      "step 60 total_reward -480.58\n",
      "['+0.47', '+0.10', '-0.65', '-0.02', '-3.83', '+1.94', '+0.00', '+1.00']\n",
      "step 73 total_reward -516.26\n",
      "['-0.00', '+0.92', '-0.03', '-0.57', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.98\n",
      "['-0.01', '+0.67', '-0.08', '-1.07', '+0.15', '+0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.97\n",
      "['-0.05', '+0.41', '-0.38', '-0.76', '+0.33', '+0.23', '+0.00', '+0.00']\n",
      "step 40 total_reward -18.76\n",
      "['-0.14', '+0.11', '-0.48', '-1.20', '+0.76', '+0.22', '+0.00', '+0.00']\n",
      "step 60 total_reward -82.98\n",
      "['-0.29', '-0.02', '-0.91', '-0.31', '+0.64', '-0.53', '+0.00', '+0.00']\n",
      "step 80 total_reward -51.55\n",
      "['-0.38', '-0.07', '-0.77', '-0.13', '+0.21', '-1.73', '+1.00', '+1.00']\n",
      "step 90 total_reward -106.34\n",
      "['+0.01', '+0.93', '+0.75', '-0.43', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.44\n",
      "['+0.16', '+0.72', '+0.72', '-0.96', '-0.01', '+0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.79\n",
      "['+0.31', '+0.48', '+0.83', '-0.75', '+0.02', '-0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward +5.20\n",
      "['+0.49', '+0.17', '+0.98', '-1.29', '-0.47', '-0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -85.96\n",
      "['+0.60', '+0.02', '+0.28', '-0.26', '-2.78', '-3.18', '+0.00', '+0.00']\n",
      "step 80 total_reward -201.79\n",
      "['+0.62', '+0.00', '+0.21', '+0.10', '-3.47', '-1.11', '+0.00', '+0.00']\n",
      "step 85 total_reward -373.81\n",
      "['-0.00', '+0.95', '-0.21', '+0.15', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.01\n",
      "['-0.03', '+0.91', '-0.01', '-0.40', '-0.41', '-0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.64\n",
      "['-0.03', '+0.71', '+0.02', '-0.94', '-1.32', '-0.92', '+0.00', '+0.00']\n",
      "step 40 total_reward -175.53\n",
      "['+0.06', '+0.35', '+0.51', '-1.51', '-2.38', '-1.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -314.59\n",
      "['+0.14', '-0.00', '+0.36', '-0.78', '-3.23', '+6.08', '+0.00', '+0.00']\n",
      "step 74 total_reward -521.87\n",
      "['+0.01', '+0.93', '+0.40', '-0.20', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.00\n",
      "['+0.08', '+0.79', '+0.29', '-0.74', '+0.27', '+0.39', '+0.00', '+0.00']\n",
      "step 20 total_reward -46.69\n",
      "['+0.12', '+0.55', '+0.04', '-0.83', '+0.58', '+0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -62.11\n",
      "['+0.12', '+0.24', '+0.06', '-1.19', '+0.42', '-0.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -53.54\n",
      "['+0.12', '-0.02', '-0.03', '-0.75', '+0.13', '-4.55', '+1.00', '+1.00']\n",
      "step 74 total_reward -108.96\n",
      "['+0.00', '+0.95', '+0.18', '+0.27', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.33\n",
      "['+0.02', '+0.95', '-0.03', '-0.28', '+0.42', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.71\n",
      "['+0.01', '+0.78', '-0.06', '-0.83', '+1.41', '+1.01', '+0.00', '+0.00']\n",
      "step 40 total_reward -173.34\n",
      "['-0.00', '+0.45', '-0.12', '-1.34', '+2.51', '+0.78', '+0.00', '+0.00']\n",
      "step 60 total_reward -302.76\n",
      "['-0.03', '+0.00', '+0.08', '-0.81', '+3.31', '-6.32', '+0.00', '+0.00']\n",
      "step 78 total_reward -506.49\n",
      "['+0.00', '+0.92', '+0.05', '-0.56', '-0.00', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.17\n",
      "['+0.02', '+0.67', '+0.10', '-1.09', '-0.27', '-0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -57.55\n",
      "['+0.05', '+0.25', '+0.27', '-1.70', '-0.93', '-1.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -144.25\n",
      "['+0.08', '+0.00', '+0.17', '+0.05', '-1.97', '-0.56', '+1.00', '+0.00']\n",
      "step 50 total_reward -268.84\n",
      "['-0.00', '+0.94', '-0.06', '-0.15', '+0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.89\n",
      "['-0.03', '+0.81', '-0.25', '-0.70', '+0.46', '+0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -95.21\n",
      "['-0.09', '+0.51', '-0.58', '-1.32', '+1.73', '+1.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -264.44\n",
      "['-0.24', '+0.03', '+0.38', '-0.15', '+3.37', '-1.31', '+0.00', '+0.00']\n",
      "step 60 total_reward -557.85\n",
      "['+0.00', '+0.94', '+0.17', '+0.10', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.04\n",
      "['+0.02', '+0.89', '-0.02', '-0.45', '+0.40', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.39\n",
      "['+0.01', '+0.67', '-0.12', '-1.04', '+1.59', '+1.37', '+0.00', '+0.00']\n",
      "step 40 total_reward -215.64\n",
      "['-0.02', '+0.30', '-0.17', '-1.48', '+2.43', '+0.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -307.08\n",
      "['-0.07', '+0.01', '-0.64', '-0.32', '+2.82', '+4.56', '+0.00', '+0.00']\n",
      "step 72 total_reward -451.75\n",
      "['+0.01', '+0.94', '+0.30', '-0.11', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.41\n",
      "['+0.05', '+0.82', '+0.22', '-0.64', '+0.27', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -51.62\n",
      "['+0.11', '+0.55', '+0.33', '-1.15', '+0.38', '-0.23', '+0.00', '+0.00']\n",
      "step 40 total_reward -87.27\n",
      "['+0.19', '+0.12', '+0.46', '-1.66', '-0.23', '-0.77', '+0.00', '+0.00']\n",
      "step 60 total_reward -92.78\n",
      "['+0.20', '+0.00', '-0.38', '-0.27', '-0.36', '+3.10', '+1.00', '+0.00']\n",
      "step 65 total_reward -206.72\n",
      "['-0.00', '+0.95', '-0.21', '+0.18', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.95\n",
      "['-0.03', '+0.92', '-0.01', '-0.37', '-0.41', '-0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -44.55\n",
      "['-0.03', '+0.72', '+0.03', '-0.92', '-1.41', '-1.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -181.53\n",
      "['+0.09', '+0.36', '+0.71', '-1.55', '-2.57', '-1.48', '+0.00', '+0.00']\n",
      "step 60 total_reward -343.68\n",
      "['+0.19', '+0.01', '-0.12', '-0.03', '-3.58', '+0.39', '+0.00', '+0.00']\n",
      "step 73 total_reward -573.25\n",
      "['-0.01', '+0.95', '-0.40', '+0.19', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.47\n",
      "['-0.09', '+0.92', '-0.40', '-0.34', '+0.10', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -14.48\n",
      "['-0.17', '+0.74', '-0.41', '-0.81', '+0.19', '+0.11', '+0.00', '+0.00']\n",
      "step 40 total_reward -44.91\n",
      "['-0.27', '+0.50', '-0.62', '-0.96', '+0.46', '+0.60', '+0.00', '+0.00']\n",
      "step 60 total_reward -79.96\n",
      "['-0.40', '+0.13', '-0.68', '-1.50', '+1.35', '+0.79', '+0.00', '+0.00']\n",
      "step 80 total_reward -205.07\n",
      "['-0.43', '+0.04', '+0.15', '-0.10', '+1.53', '-0.63', '+0.00', '+1.00']\n",
      "step 85 total_reward -322.73\n",
      "['-0.00', '+0.95', '-0.01', '+0.27', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.89\n",
      "['-0.01', '+0.95', '-0.08', '-0.27', '+0.19', '+0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -17.81\n",
      "['-0.02', '+0.78', '-0.06', '-0.80', '+0.42', '+0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -77.27\n",
      "['-0.05', '+0.48', '-0.52', '-0.99', '+0.78', '+0.45', '+0.00', '+0.00']\n",
      "step 60 total_reward -118.30\n",
      "['-0.17', '+0.11', '-0.60', '-1.52', '+1.55', '+0.68', '+0.00', '+0.00']\n",
      "step 80 total_reward -217.87\n",
      "['-0.22', '-0.01', '-1.34', '-0.34', '+2.21', '+5.38', '+0.00', '+1.00']\n",
      "step 86 total_reward -324.58\n",
      "['-0.00', '+0.94', '-0.03', '-0.07', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.78\n",
      "['+0.01', '+0.83', '+0.12', '-0.61', '-0.41', '-0.61', '+0.00', '+0.00']\n",
      "step 20 total_reward -86.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.03', '+0.56', '+0.17', '-1.20', '-1.12', '-1.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -190.04\n",
      "['+0.07', '+0.11', '+0.23', '-1.86', '-2.65', '-1.79', '+0.00', '+0.00']\n",
      "step 60 total_reward -367.95\n",
      "['+0.08', '-0.00', '+0.34', '-1.08', '-3.05', '-7.86', '+0.00', '+0.00']\n",
      "step 64 total_reward -498.68\n",
      "['+0.01', '+0.95', '+0.37', '+0.35', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.16\n",
      "['+0.08', '+0.97', '+0.36', '-0.18', '-0.09', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward +0.51\n",
      "['+0.15', '+0.83', '+0.36', '-0.72', '-0.16', '-0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -33.83\n",
      "['+0.23', '+0.58', '+0.52', '-0.74', '-0.24', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -32.48\n",
      "['+0.36', '+0.34', '+0.77', '-1.07', '-0.57', '-0.73', '+0.00', '+0.00']\n",
      "step 80 total_reward -95.63\n",
      "['+0.49', '+0.04', '+0.32', '+0.11', '-1.84', '-0.92', '+1.00', '+0.00']\n",
      "step 97 total_reward -293.53\n",
      "['-0.00', '+0.95', '-0.13', '+0.45', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.67\n",
      "['-0.03', '+1.00', '-0.13', '-0.09', '+0.03', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward +24.68\n",
      "['-0.05', '+0.89', '-0.13', '-0.62', '+0.06', '+0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -15.22\n",
      "['-0.08', '+0.63', '-0.14', '-1.00', '+0.09', '+0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -30.83\n",
      "['-0.12', '+0.31', '-0.25', '-1.25', '+0.26', '+0.32', '+0.00', '+0.00']\n",
      "step 80 total_reward -45.95\n",
      "['-0.15', '-0.01', '-0.01', '-0.72', '+0.29', '-5.33', '+0.00', '+1.00']\n",
      "step 95 total_reward -154.74\n",
      "['+0.00', '+0.94', '+0.21', '-0.07', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.16\n",
      "['+0.03', '+0.83', '+0.11', '-0.60', '+0.30', '+0.40', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.06\n",
      "['+0.04', '+0.61', '-0.16', '-0.81', '+0.68', '+0.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -97.38\n",
      "['-0.04', '+0.34', '-0.54', '-1.05', '+0.73', '+0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -113.49\n",
      "['-0.22', '+0.04', '-1.24', '-0.24', '+0.93', '+2.08', '+0.00', '+1.00']\n",
      "step 80 total_reward -127.00\n",
      "['-0.51', '+0.03', '-1.28', '+0.11', '+2.84', '-1.16', '+0.00', '+0.00']\n",
      "step 98 total_reward -485.73\n",
      "['-0.01', '+0.93', '-0.45', '-0.41', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.80\n",
      "['-0.09', '+0.72', '-0.36', '-0.94', '-0.20', '-0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -40.28\n",
      "['-0.14', '+0.45', '-0.11', '-0.82', '-0.37', '-0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -19.37\n",
      "['-0.16', '+0.15', '-0.04', '-1.13', '-0.18', '+0.15', '+0.00', '+0.00']\n",
      "step 60 total_reward -7.31\n",
      "['-0.16', '-0.03', '+0.00', '-0.00', '-0.00', '-0.00', '+1.00', '+1.00']\n",
      "step 71 total_reward -78.19\n",
      "['-0.00', '+0.94', '-0.23', '+0.07', '+0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.63\n",
      "['-0.03', '+0.88', '-0.04', '-0.48', '-0.41', '-0.75', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.51\n",
      "['-0.04', '+0.66', '+0.03', '-0.99', '-1.16', '-0.64', '+0.00', '+0.00']\n",
      "step 40 total_reward -161.19\n",
      "['+0.04', '+0.30', '+0.58', '-1.45', '-1.82', '-1.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -253.72\n",
      "['+0.11', '+0.01', '+0.52', '-0.00', '-2.41', '-1.55', '+0.00', '+0.00']\n",
      "step 73 total_reward -414.62\n",
      "['+0.01', '+0.95', '+0.37', '+0.20', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.75\n",
      "['+0.06', '+0.92', '+0.17', '-0.35', '+0.39', '+0.78', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.05\n",
      "['+0.10', '+0.74', '+0.16', '-0.88', '+1.24', '+0.81', '+0.00', '+0.00']\n",
      "step 40 total_reward -149.60\n",
      "['+0.12', '+0.41', '-0.04', '-1.31', '+1.55', '-0.05', '+0.00', '+0.00']\n",
      "step 60 total_reward -192.30\n",
      "['+0.10', '+0.00', '-0.34', '+0.11', '+1.95', '+1.14', '+0.00', '+1.00']\n",
      "step 80 total_reward -221.01\n",
      "['+0.00', '+0.94', '+0.15', '+0.01', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.12\n",
      "['+0.02', '+0.86', '+0.09', '-0.51', '+0.31', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -58.64\n",
      "['+0.04', '+0.62', '+0.09', '-1.05', '+0.54', '+0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -111.93\n",
      "['+0.08', '+0.24', '+0.24', '-1.52', '+0.33', '-0.53', '+0.00', '+0.00']\n",
      "step 60 total_reward -102.51\n",
      "['+0.10', '-0.03', '+0.17', '+0.00', '+0.00', '+0.00', '+1.00', '+1.00']\n",
      "step 71 total_reward -167.57\n",
      "['-0.01', '+0.94', '-0.53', '-0.03', '+0.01', '+0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.57\n",
      "['-0.10', '+0.85', '-0.40', '-0.57', '-0.26', '-0.42', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.83\n",
      "['-0.17', '+0.63', '-0.14', '-0.76', '-0.69', '-0.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -65.64\n",
      "['-0.15', '+0.38', '+0.16', '-0.96', '-0.85', '+0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -81.01\n",
      "['-0.11', '+0.02', '+0.23', '-1.18', '-0.67', '+0.01', '+1.00', '+0.00']\n",
      "step 80 total_reward -47.05\n",
      "['-0.11', '-0.01', '+0.16', '-0.18', '-0.50', '+2.78', '+1.00', '+0.00']\n",
      "step 82 total_reward -143.08\n",
      "['+0.01', '+0.94', '+0.58', '+0.12', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.72\n",
      "['+0.11', '+0.90', '+0.43', '-0.42', '+0.28', '+0.50', '+0.00', '+0.00']\n",
      "step 20 total_reward -23.00\n",
      "['+0.20', '+0.70', '+0.25', '-0.78', '+0.78', '+0.52', '+0.00', '+0.00']\n",
      "step 40 total_reward -78.87\n",
      "['+0.13', '+0.48', '-0.91', '-0.78', '+1.33', '+0.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -154.86\n",
      "['-0.07', '+0.18', '-1.27', '-1.24', '+1.49', '+0.09', '+0.00', '+0.00']\n",
      "step 80 total_reward -199.71\n",
      "['-0.22', '-0.00', '-1.83', '-0.18', '+2.24', '+2.05', '+0.00', '+1.00']\n",
      "step 91 total_reward -355.61\n",
      "['-0.01', '+0.93', '-0.68', '-0.49', '+0.02', '+0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.43\n",
      "['-0.15', '+0.70', '-0.72', '-0.93', '+0.17', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.69\n",
      "['-0.31', '+0.42', '-0.92', '-1.11', '+0.45', '+0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -69.64\n",
      "['-0.49', '+0.01', '-0.90', '-1.62', '+1.04', '+0.40', '+0.00', '+0.00']\n",
      "step 60 total_reward -165.72\n",
      "['-0.53', '-0.07', '-0.89', '-0.66', '+1.43', '+6.92', '+0.00', '+1.00']\n",
      "step 64 total_reward -271.69\n",
      "['+0.00', '+0.93', '+0.25', '-0.34', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.79\n",
      "['+0.05', '+0.75', '+0.20', '-0.87', '+0.11', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.84\n",
      "['+0.08', '+0.45', '+0.23', '-1.12', '+0.08', '-0.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -37.35\n",
      "['+0.14', '+0.04', '+0.31', '-1.65', '-0.48', '-0.62', '+0.00', '+0.00']\n",
      "step 60 total_reward -100.03\n",
      "['+0.14', '-0.01', '+0.07', '-0.41', '-0.44', '+4.42', '+1.00', '+0.00']\n",
      "step 62 total_reward -195.62\n",
      "['+0.01', '+0.95', '+0.30', '+0.31', '-0.01', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.97\n",
      "['+0.06', '+0.96', '+0.26', '-0.23', '+0.12', '+0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.44\n",
      "['+0.11', '+0.80', '+0.22', '-0.77', '+0.40', '+0.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -60.68\n",
      "['+0.13', '+0.54', '-0.11', '-0.88', '+0.62', '+0.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -69.43\n",
      "['+0.10', '+0.23', '-0.08', '-1.26', '+0.40', '-0.48', '+0.00', '+0.00']\n",
      "step 80 total_reward -55.92\n",
      "['+0.09', '-0.03', '-0.03', '-0.91', '+0.01', '-5.70', '+1.00', '+1.00']\n",
      "step 92 total_reward -135.85\n",
      "['-0.00', '+0.95', '-0.17', '+0.14', '+0.00', '+0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.35\n",
      "['-0.03', '+0.90', '-0.05', '-0.40', '-0.12', '-0.45', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.42\n",
      "['-0.04', '+0.70', '-0.05', '-0.93', '-0.56', '-0.45', '+0.00', '+0.00']\n",
      "step 40 total_reward -102.41\n",
      "['-0.00', '+0.41', '+0.51', '-1.05', '-0.92', '-0.39', '+0.00', '+0.00']\n",
      "step 60 total_reward -136.74\n",
      "['+0.10', '+0.03', '+0.63', '-1.04', '-1.57', '-4.33', '+1.00', '+0.00']\n",
      "step 80 total_reward -165.73\n",
      "['+0.12', '+0.00', '+0.55', '+0.13', '-2.02', '-1.73', '+1.00', '+0.00']\n",
      "step 82 total_reward -290.45\n",
      "['-0.00', '+0.94', '-0.16', '+0.05', '+0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.97\n",
      "['-0.02', '+0.87', '+0.04', '-0.51', '-0.41', '-0.80', '+0.00', '+0.00']\n",
      "step 20 total_reward -66.91\n",
      "['-0.01', '+0.64', '+0.08', '-1.09', '-1.39', '-1.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -200.12\n",
      "['+0.01', '+0.22', '-0.00', '-1.72', '-3.15', '-2.07', '+0.00', '+0.00']\n",
      "step 60 total_reward -397.25\n",
      "['+0.01', '+0.01', '+0.19', '-0.00', '-3.94', '-0.57', '+0.00', '+0.00']\n",
      "step 68 total_reward -569.85\n",
      "['-0.01', '+0.93', '-0.60', '-0.23', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.14\n",
      "['-0.12', '+0.78', '-0.51', '-0.76', '-0.17', '-0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.19', '+0.58', '-0.16', '-0.53', '-0.42', '-0.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -4.96\n",
      "['-0.18', '+0.43', '+0.12', '-0.63', '-0.54', '+0.20', '+0.00', '+0.00']\n",
      "step 60 total_reward -14.39\n",
      "['-0.16', '+0.16', '+0.11', '-1.15', '-0.11', '+0.26', '+0.00', '+0.00']\n",
      "step 80 total_reward +0.73\n",
      "['-0.14', '-0.03', '+0.03', '-0.00', '-0.00', '-0.00', '+1.00', '+1.00']\n",
      "step 91 total_reward -68.51\n",
      "['+0.01', '+0.95', '+0.28', '+0.17', '-0.00', '-0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.88\n",
      "['+0.05', '+0.91', '+0.15', '-0.37', '+0.23', '+0.49', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.99\n",
      "['+0.08', '+0.72', '+0.09', '-0.86', '+0.71', '+0.48', '+0.00', '+0.00']\n",
      "step 40 total_reward -101.34\n",
      "['+0.08', '+0.42', '+0.05', '-1.19', '+0.83', '-0.30', '+0.00', '+0.00']\n",
      "step 60 total_reward -118.47\n",
      "['+0.09', '+0.00', '-0.01', '-1.38', '+0.31', '-0.74', '+0.00', '+1.00']\n",
      "step 80 total_reward -43.10\n",
      "['+0.09', '-0.02', '+0.05', '-0.80', '+0.24', '-5.38', '+0.00', '+1.00']\n",
      "step 81 total_reward -143.10\n",
      "['+0.01', '+0.93', '+0.28', '-0.22', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.31\n",
      "['+0.05', '+0.79', '+0.20', '-0.75', '+0.13', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.13\n",
      "['+0.09', '+0.51', '+0.27', '-1.11', '+0.22', '-0.23', '+0.00', '+0.00']\n",
      "step 40 total_reward -63.34\n",
      "['+0.16', '+0.10', '+0.39', '-1.64', '-0.37', '-0.73', '+0.00', '+0.00']\n",
      "step 60 total_reward -99.07\n",
      "['+0.18', '-0.02', '+0.71', '-0.99', '-0.27', '+5.20', '+1.00', '+0.00']\n",
      "step 65 total_reward -205.68\n",
      "['-0.01', '+0.94', '-0.51', '-0.13', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.88\n",
      "['-0.11', '+0.81', '-0.51', '-0.66', '+0.13', '+0.11', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.74\n",
      "['-0.22', '+0.57', '-0.68', '-0.92', '+0.28', '+0.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -58.82\n",
      "['-0.37', '+0.21', '-0.77', '-1.48', '+0.94', '+0.76', '+0.00', '+0.00']\n",
      "step 60 total_reward -158.63\n",
      "['-0.59', '-0.06', '-1.39', '-0.54', '+2.77', '+2.88', '+0.00', '+0.00']\n",
      "step 80 total_reward -340.95\n",
      "['-0.70', '-0.12', '-1.49', '-0.18', '+3.67', '+0.39', '+0.00', '+0.00']\n",
      "step 87 total_reward -542.64\n",
      "['+0.01', '+0.95', '+0.36', '+0.33', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.90\n",
      "['+0.08', '+0.97', '+0.37', '-0.20', '-0.09', '-0.08', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.27\n",
      "['+0.15', '+0.82', '+0.36', '-0.73', '-0.17', '-0.08', '+0.00', '+0.00']\n",
      "step 40 total_reward -36.42\n",
      "['+0.23', '+0.56', '+0.52', '-0.81', '-0.26', '-0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -39.69\n",
      "['+0.36', '+0.27', '+0.73', '-1.25', '-0.62', '-0.70', '+0.00', '+0.00']\n",
      "step 80 total_reward -110.59\n",
      "['+0.42', '+0.11', '+0.24', '+0.16', '-1.58', '-1.02', '+0.00', '+0.00']\n",
      "step 90 total_reward -213.41\n",
      "['-0.01', '+0.93', '-0.33', '-0.41', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.67\n",
      "['-0.07', '+0.72', '-0.29', '-0.94', '+0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -30.71\n",
      "['-0.14', '+0.48', '-0.38', '-0.63', '+0.07', '+0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward +7.31\n",
      "['-0.22', '+0.25', '-0.56', '-0.92', '+0.42', '+0.47', '+0.00', '+0.00']\n",
      "step 60 total_reward -47.05\n",
      "['-0.31', '-0.02', '-0.42', '-0.43', '+0.45', '-3.82', '+0.00', '+1.00']\n",
      "step 77 total_reward -157.43\n",
      "['+0.01', '+0.94', '+0.75', '+0.12', '-0.02', '-0.14', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.39\n",
      "['+0.15', '+0.90', '+0.64', '-0.41', '+0.17', '+0.28', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.41\n",
      "['+0.28', '+0.69', '+0.67', '-0.93', '+0.43', '+0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -61.39\n",
      "['+0.41', '+0.36', '+0.72', '-1.31', '+0.22', '-0.55', '+0.00', '+0.00']\n",
      "step 60 total_reward -56.92\n",
      "['+0.55', '-0.11', '+0.38', '-0.60', '-0.29', '+5.25', '+1.00', '+0.00']\n",
      "step 80 total_reward -183.41\n",
      "['-0.01', '+0.93', '-0.57', '-0.31', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.24\n",
      "['-0.12', '+0.76', '-0.53', '-0.84', '+0.07', '-0.01', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.38\n",
      "['-0.24', '+0.49', '-0.72', '-1.05', '+0.23', '+0.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -51.99\n",
      "['-0.41', '+0.13', '-0.99', '-1.35', '+0.96', '+0.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -154.69\n",
      "['-0.45', '+0.05', '-1.29', '-0.23', '+1.06', '+0.11', '+0.00', '+1.00']\n",
      "step 64 total_reward -257.13\n",
      "['+0.00', '+0.93', '+0.17', '-0.41', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.56\n",
      "['+0.03', '+0.72', '+0.13', '-0.94', '+0.07', '+0.09', '+0.00', '+0.00']\n",
      "step 20 total_reward -38.94\n",
      "['+0.07', '+0.36', '+0.28', '-1.47', '-0.08', '-0.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -59.00\n",
      "['+0.10', '-0.01', '+0.04', '-0.64', '-0.31', '+5.67', '+1.00', '+0.00']\n",
      "step 55 total_reward -195.14\n",
      "['-0.01', '+0.94', '-0.50', '+0.00', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.80\n",
      "['-0.09', '+0.86', '-0.33', '-0.54', '-0.32', '-0.62', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.42\n",
      "['-0.14', '+0.64', '+0.07', '-0.80', '-0.93', '-0.62', '+0.00', '+0.00']\n",
      "step 40 total_reward -96.59\n",
      "['-0.07', '+0.36', '+0.63', '-1.09', '-1.39', '-0.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -161.17\n",
      "['+0.19', '+0.00', '+1.84', '-0.01', '-2.01', '-0.39', '+1.00', '+0.00']\n",
      "step 80 total_reward -382.49\n",
      "['+0.00', '+0.94', '+0.06', '+0.02', '-0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.94\n",
      "['-0.00', '+0.86', '-0.11', '-0.52', '+0.44', '+0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -81.63\n",
      "['-0.02', '+0.62', '-0.16', '-1.12', '+1.25', '+1.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -198.17\n",
      "['-0.12', '+0.21', '-0.49', '-1.72', '+2.77', '+1.88', '+0.00', '+0.00']\n",
      "step 60 total_reward -379.33\n",
      "['-0.16', '+0.00', '-0.36', '-0.59', '+3.27', '-5.23', '+0.00', '+0.00']\n",
      "step 68 total_reward -550.20\n",
      "['-0.01', '+0.94', '-0.61', '-0.13', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.04\n",
      "['-0.12', '+0.81', '-0.49', '-0.67', '-0.22', '-0.35', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.81\n",
      "['-0.19', '+0.60', '-0.13', '-0.62', '-0.52', '-0.30', '+0.00', '+0.00']\n",
      "step 40 total_reward -25.42\n",
      "['-0.13', '+0.45', '+0.54', '-0.57', '-0.75', '-0.06', '+0.00', '+0.00']\n",
      "step 60 total_reward -52.31\n",
      "['-0.03', '+0.22', '+0.64', '-0.78', '-0.44', '+0.45', '+0.00', '+0.00']\n",
      "step 80 total_reward -21.21\n",
      "['+0.15', '+0.03', '+0.94', '-0.51', '+0.03', '+0.48', '+0.00', '+0.00']\n",
      "step 100 total_reward +14.60\n",
      "['+0.21', '-0.03', '+0.86', '-0.08', '-0.07', '-3.00', '+1.00', '+1.00']\n",
      "step 107 total_reward -77.62\n",
      "['+0.00', '+0.92', '+0.17', '-0.53', '-0.01', '-0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.23\n",
      "['+0.05', '+0.69', '+0.29', '-0.91', '-0.25', '-0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -44.32\n",
      "['+0.14', '+0.40', '+0.62', '-1.17', '-0.67', '-0.71', '+0.00', '+0.00']\n",
      "step 40 total_reward -99.07\n",
      "['+0.26', '+0.04', '-0.12', '-0.09', '-1.48', '+0.56', '+1.00', '+0.00']\n",
      "step 58 total_reward -303.30\n",
      "['+0.01', '+0.95', '+0.75', '+0.31', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.11\n",
      "['+0.18', '+0.96', '+0.93', '-0.25', '-0.48', '-0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.43\n",
      "['+0.37', '+0.79', '+1.01', '-0.89', '-1.79', '-1.78', '+0.00', '+0.00']\n",
      "step 40 total_reward -224.03\n",
      "['+0.57', '+0.44', '+0.85', '-1.45', '-4.03', '-2.64', '+0.00', '+0.00']\n",
      "step 60 total_reward -466.45\n",
      "['+0.67', '+0.15', '+0.65', '-1.07', '-5.87', '-6.86', '+1.00', '+0.00']\n",
      "step 72 total_reward -710.23\n",
      "['-0.01', '+0.95', '-0.62', '+0.40', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.35\n",
      "['-0.14', '+0.99', '-0.62', '-0.14', '+0.11', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -2.61\n",
      "['-0.26', '+0.86', '-0.62', '-0.67', '+0.20', '+0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -30.38\n",
      "['-0.40', '+0.66', '-0.85', '-0.49', '+0.32', '+0.16', '+0.00', '+0.00']\n",
      "step 60 total_reward -40.15\n",
      "['-0.60', '+0.49', '-1.13', '-0.83', '+0.73', '+0.81', '+0.00', '+0.00']\n",
      "step 80 total_reward -125.56\n",
      "['-0.83', '+0.15', '-1.18', '-1.45', '+2.02', '+1.52', '+0.00', '+0.00']\n",
      "step 100 total_reward -309.10\n",
      "['-0.91', '+0.04', '-0.78', '-0.30', '+2.68', '+5.68', '+0.00', '+0.00']\n",
      "step 106 total_reward -462.26\n",
      "['-0.01', '+0.93', '-0.74', '-0.43', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.37\n",
      "['-0.16', '+0.72', '-0.77', '-0.90', '+0.18', '+0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.47\n",
      "['-0.34', '+0.45', '-1.05', '-0.99', '+0.49', '+0.57', '+0.00', '+0.00']\n",
      "step 40 total_reward -74.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.55', '+0.08', '-0.92', '-1.15', '+1.33', '+4.58', '+0.00', '+1.00']\n",
      "step 60 total_reward -151.92\n",
      "['-0.56', '+0.07', '-0.19', '+0.11', '+1.65', '+0.72', '+0.00', '+1.00']\n",
      "step 62 total_reward -265.61\n",
      "['-0.01', '+0.93', '-0.74', '-0.42', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.13\n",
      "['-0.15', '+0.72', '-0.67', '-0.85', '-0.09', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.57\n",
      "['-0.30', '+0.46', '-0.80', '-1.07', '-0.06', '+0.39', '+0.00', '+0.00']\n",
      "step 40 total_reward -18.07\n",
      "['-0.46', '+0.06', '-0.81', '-1.53', '+0.47', '+0.26', '+0.00', '+1.00']\n",
      "step 60 total_reward -81.21\n",
      "['-0.48', '+0.03', '-0.87', '-0.85', '+0.30', '-5.02', '+0.00', '+1.00']\n",
      "step 62 total_reward -175.39\n",
      "['-0.01', '+0.92', '-0.44', '-0.56', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.83\n",
      "['-0.09', '+0.67', '-0.36', '-1.04', '-0.14', '-0.16', '+0.00', '+0.00']\n",
      "step 20 total_reward -27.77\n",
      "['-0.17', '+0.33', '-0.45', '-1.29', '-0.09', '+0.28', '+0.00', '+0.00']\n",
      "step 40 total_reward -22.08\n",
      "['-0.24', '-0.04', '-0.69', '-0.05', '+0.25', '+0.34', '+1.00', '+1.00']\n",
      "step 57 total_reward -119.65\n",
      "['+0.00', '+0.93', '+0.09', '-0.24', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.43\n",
      "['+0.00', '+0.78', '-0.10', '-0.80', '+0.45', '+0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -86.10\n",
      "['-0.02', '+0.46', '-0.10', '-1.28', '+1.32', '+0.51', '+0.00', '+0.00']\n",
      "step 40 total_reward -190.28\n",
      "['-0.04', '+0.02', '-0.19', '-1.13', '+1.55', '+4.39', '+0.00', '+1.00']\n",
      "step 60 total_reward -149.11\n",
      "['-0.04', '+0.00', '-0.21', '+0.06', '+1.97', '+0.69', '+0.00', '+1.00']\n",
      "step 62 total_reward -273.71\n",
      "['-0.01', '+0.93', '-0.26', '-0.25', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.59\n",
      "['-0.05', '+0.78', '-0.17', '-0.78', '-0.16', '-0.34', '+0.00', '+0.00']\n",
      "step 20 total_reward -45.22\n",
      "['-0.09', '+0.50', '-0.16', '-0.92', '-0.25', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -43.92\n",
      "['-0.12', '+0.20', '-0.15', '-1.26', '+0.06', '+0.33', '+0.00', '+0.00']\n",
      "step 60 total_reward -31.99\n",
      "['-0.13', '-0.03', '+0.00', '+0.00', '+0.00', '-0.00', '+1.00', '+1.00']\n",
      "step 71 total_reward -124.45\n",
      "['-0.01', '+0.93', '-0.76', '-0.52', '+0.02', '+0.21', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.49\n",
      "['-0.17', '+0.69', '-0.81', '-0.93', '+0.28', '+0.32', '+0.00', '+0.00']\n",
      "step 20 total_reward -39.20\n",
      "['-0.35', '+0.36', '-0.99', '-1.39', '+0.89', '+1.00', '+0.00', '+0.00']\n",
      "step 40 total_reward -127.89\n",
      "['-0.49', '+0.08', '-0.83', '+0.14', '+2.06', '+2.07', '+0.00', '+0.00']\n",
      "step 54 total_reward -290.55\n",
      "['+0.00', '+0.94', '+0.17', '+0.01', '-0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.21\n",
      "['+0.02', '+0.86', '-0.02', '-0.54', '+0.40', '+0.79', '+0.00', '+0.00']\n",
      "step 20 total_reward -67.64\n",
      "['+0.02', '+0.62', '-0.09', '-1.07', '+1.28', '+0.95', '+0.00', '+0.00']\n",
      "step 40 total_reward -185.20\n",
      "['-0.10', '+0.23', '-0.93', '-1.66', '+2.16', '+0.88', '+0.00', '+0.00']\n",
      "step 60 total_reward -323.28\n",
      "['-0.19', '+0.01', '-1.04', '-0.06', '+2.54', '+3.13', '+0.00', '+0.00']\n",
      "step 69 total_reward -470.90\n",
      "['-0.00', '+0.92', '-0.22', '-0.58', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.45\n",
      "['-0.04', '+0.67', '-0.12', '-1.01', '-0.16', '-0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.75\n",
      "['-0.03', '+0.40', '+0.14', '-0.89', '-0.35', '-0.14', '+0.00', '+0.00']\n",
      "step 40 total_reward -19.23\n",
      "['+0.00', '+0.09', '+0.20', '-1.14', '-0.22', '+0.26', '+0.00', '+0.00']\n",
      "step 60 total_reward -2.97\n",
      "['+0.02', '-0.03', '+0.01', '+0.00', '+0.00', '-0.00', '+1.00', '+1.00']\n",
      "step 67 total_reward -76.96\n",
      "['-0.00', '+0.95', '-0.02', '+0.22', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.25\n",
      "['+0.00', '+0.93', '-0.00', '-0.31', '-0.28', '-0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -33.40\n",
      "['+0.01', '+0.75', '+0.05', '-0.85', '-0.51', '-0.31', '+0.00', '+0.00']\n",
      "step 40 total_reward -93.33\n",
      "['+0.02', '+0.41', '+0.14', '-1.44', '-0.95', '-0.86', '+0.00', '+0.00']\n",
      "step 60 total_reward -164.05\n",
      "['+0.06', '+0.01', '+0.28', '+0.07', '-2.02', '-0.91', '+0.00', '+0.00']\n",
      "step 78 total_reward -310.41\n",
      "['+0.01', '+0.94', '+0.49', '-0.12', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.20\n",
      "['+0.09', '+0.82', '+0.38', '-0.65', '+0.28', '+0.35', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.24\n",
      "['+0.15', '+0.60', '-0.00', '-0.63', '+0.65', '+0.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -49.40\n",
      "['+0.10', '+0.40', '-0.27', '-0.85', '+0.90', '-0.11', '+0.00', '+0.00']\n",
      "step 60 total_reward -82.06\n",
      "['+0.05', '+0.07', '-0.31', '-1.26', '+0.46', '-0.47', '+0.00', '+0.00']\n",
      "step 80 total_reward -47.53\n",
      "['+0.03', '-0.02', '-0.20', '-0.64', '+0.15', '-4.99', '+0.00', '+1.00']\n",
      "step 85 total_reward -128.58\n",
      "['-0.00', '+0.94', '-0.24', '-0.19', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.42\n",
      "['-0.04', '+0.80', '-0.13', '-0.72', '-0.30', '-0.43', '+0.00', '+0.00']\n",
      "step 20 total_reward -59.23\n",
      "['-0.04', '+0.55', '+0.41', '-0.78', '-0.73', '-0.38', '+0.00', '+0.00']\n",
      "step 40 total_reward -95.26\n",
      "['+0.09', '+0.27', '+0.75', '-1.24', '-1.31', '-1.00', '+0.00', '+0.00']\n",
      "step 60 total_reward -186.61\n",
      "['+0.19', '-0.00', '+0.82', '-0.47', '-1.93', '+3.89', '+1.00', '+0.00']\n",
      "step 73 total_reward -375.66\n",
      "['-0.01', '+0.94', '-0.32', '+0.04', '+0.01', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.24\n",
      "['-0.05', '+0.87', '-0.13', '-0.51', '-0.37', '-0.76', '+0.00', '+0.00']\n",
      "step 20 total_reward -48.67\n",
      "['-0.09', '+0.64', '-0.10', '-0.94', '-0.98', '-0.25', '+0.00', '+0.00']\n",
      "step 40 total_reward -130.78\n",
      "['-0.08', '+0.30', '+0.05', '-1.32', '-0.89', '+0.44', '+0.00', '+0.00']\n",
      "step 60 total_reward -126.94\n",
      "['-0.07', '-0.00', '-0.10', '-0.12', '-0.68', '+2.34', '+1.00', '+0.00']\n",
      "step 74 total_reward -171.88\n",
      "['-0.01', '+0.95', '-0.39', '+0.15', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.87\n",
      "['-0.07', '+0.91', '-0.24', '-0.38', '-0.08', '-0.44', '+0.00', '+0.00']\n",
      "step 20 total_reward -8.39\n",
      "['-0.12', '+0.71', '-0.22', '-0.91', '-0.66', '-0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -96.53\n",
      "['-0.16', '+0.39', '-0.16', '-1.23', '-0.76', '+0.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -108.38\n",
      "['-0.19', '-0.01', '-0.30', '-0.43', '-0.44', '+4.83', '+1.00', '+0.00']\n",
      "step 78 total_reward -191.78\n",
      "['-0.01', '+0.94', '-0.64', '-0.13', '+0.01', '+0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.04\n",
      "['-0.13', '+0.81', '-0.55', '-0.67', '-0.16', '-0.25', '+0.00', '+0.00']\n",
      "step 20 total_reward -24.73\n",
      "['-0.22', '+0.60', '-0.35', '-0.68', '-0.36', '-0.06', '+0.00', '+0.00']\n",
      "step 40 total_reward -19.86\n",
      "['-0.30', '+0.36', '-0.44', '-1.00', '-0.10', '+0.61', '+0.00', '+0.00']\n",
      "step 60 total_reward -11.44\n",
      "['-0.37', '+0.02', '-0.04', '-1.07', '+0.37', '+0.24', '+0.00', '+1.00']\n",
      "step 80 total_reward -17.92\n",
      "['-0.37', '+0.00', '+0.53', '-0.23', '+0.34', '-1.76', '+0.00', '+1.00']\n",
      "step 81 total_reward -117.92\n",
      "['-0.01', '+0.93', '-0.56', '-0.40', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.42\n",
      "['-0.11', '+0.73', '-0.49', '-0.93', '-0.13', '-0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.59\n",
      "['-0.20', '+0.43', '-0.43', '-1.03', '-0.14', '+0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -14.75\n",
      "['-0.29', '+0.05', '-0.39', '-1.47', '+0.10', '-0.03', '+0.00', '+1.00']\n",
      "step 60 total_reward -23.46\n",
      "['-0.29', '+0.02', '+0.23', '-0.70', '-0.13', '-5.16', '+0.00', '+1.00']\n",
      "step 62 total_reward -117.09\n",
      "['+0.01', '+0.94', '+0.40', '+0.07', '-0.01', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.02\n",
      "['+0.07', '+0.88', '+0.27', '-0.47', '+0.30', '+0.47', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.91\n",
      "['+0.12', '+0.67', '+0.01', '-0.80', '+0.77', '+0.46', '+0.00', '+0.00']\n",
      "step 40 total_reward -89.48\n",
      "['+0.05', '+0.41', '-0.48', '-1.00', '+1.05', '-0.03', '+0.00', '+0.00']\n",
      "step 60 total_reward -125.95\n",
      "['-0.08', '+0.07', '-0.94', '-1.19', '+0.78', '-0.40', '+0.00', '+0.00']\n",
      "step 80 total_reward -110.88\n",
      "['-0.13', '-0.00', '-0.93', '-0.03', '+0.68', '-0.56', '+0.00', '+1.00']\n",
      "step 85 total_reward -204.05\n",
      "['-0.01', '+0.92', '-0.45', '-0.54', '+0.01', '+0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.73\n",
      "['-0.09', '+0.68', '-0.36', '-1.01', '-0.16', '-0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.95\n",
      "['-0.15', '+0.39', '-0.26', '-1.00', '-0.18', '+0.23', '+0.00', '+0.00']\n",
      "step 40 total_reward -4.08\n",
      "['-0.20', '+0.01', '-0.19', '-1.53', '+0.03', '-0.04', '+0.00', '+0.00']\n",
      "step 60 total_reward -18.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.20', '-0.03', '-0.01', '-0.00', '+0.02', '+0.00', '+1.00', '+1.00']\n",
      "step 62 total_reward -101.28\n",
      "['+0.00', '+0.95', '+0.22', '+0.17', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.12\n",
      "['+0.04', '+0.91', '+0.13', '-0.37', '+0.07', '+0.33', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.18\n",
      "['+0.07', '+0.72', '+0.13', '-0.90', '+0.44', '+0.32', '+0.00', '+0.00']\n",
      "step 40 total_reward -82.84\n",
      "['+0.11', '+0.37', '+0.31', '-1.38', '+0.32', '-0.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -89.16\n",
      "['+0.17', '-0.03', '+0.29', '+0.00', '+0.00', '+0.00', '+1.00', '+1.00']\n",
      "step 77 total_reward -184.28\n",
      "['-0.01', '+0.94', '-0.56', '+0.07', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.82\n",
      "['-0.11', '+0.88', '-0.41', '-0.46', '-0.30', '-0.52', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.01\n",
      "['-0.18', '+0.68', '-0.04', '-0.72', '-0.81', '-0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -73.86\n",
      "['-0.08', '+0.45', '+1.04', '-0.85', '-1.31', '-0.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -167.76\n",
      "['+0.15', '+0.13', '+1.50', '-1.31', '-1.54', '-0.18', '+0.00', '+0.00']\n",
      "step 80 total_reward -230.56\n",
      "['+0.25', '+0.01', '+1.09', '+0.15', '-1.88', '-0.60', '+1.00', '+0.00']\n",
      "step 87 total_reward -332.50\n",
      "['+0.01', '+0.95', '+0.35', '+0.14', '-0.01', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.53\n",
      "['+0.06', '+0.90', '+0.16', '-0.41', '+0.34', '+0.71', '+0.00', '+0.00']\n",
      "step 20 total_reward -34.66\n",
      "['+0.09', '+0.70', '-0.03', '-0.87', '+1.08', '+0.74', '+0.00', '+0.00']\n",
      "step 40 total_reward -133.89\n",
      "['+0.08', '+0.38', '-0.06', '-1.26', '+1.34', '-0.21', '+0.00', '+0.00']\n",
      "step 60 total_reward -168.81\n",
      "['+0.07', '+0.00', '-0.34', '-0.04', '+1.07', '+2.63', '+0.00', '+1.00']\n",
      "step 78 total_reward -223.15\n",
      "['-0.01', '+0.93', '-0.60', '-0.29', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.00\n",
      "['-0.12', '+0.76', '-0.52', '-0.83', '-0.14', '-0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -28.36\n",
      "['-0.20', '+0.55', '-0.19', '-0.50', '-0.35', '-0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward +7.85\n",
      "['-0.21', '+0.41', '-0.05', '-0.65', '-0.40', '+0.29', '+0.00', '+0.00']\n",
      "step 60 total_reward +1.74\n",
      "['-0.22', '+0.13', '+0.01', '-1.18', '-0.08', '+0.05', '+0.00', '+0.00']\n",
      "step 80 total_reward +0.68\n",
      "['-0.21', '-0.02', '+0.02', '-0.00', '-0.10', '+0.01', '+1.00', '+1.00']\n",
      "step 89 total_reward -88.78\n",
      "['-0.01', '+0.96', '-0.72', '+0.48', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.18\n",
      "['-0.14', '+1.01', '-0.53', '-0.07', '-0.27', '-0.66', '+0.00', '+0.00']\n",
      "step 20 total_reward +0.90\n",
      "['-0.24', '+0.91', '-0.45', '-0.61', '-1.27', '-0.91', '+0.00', '+0.00']\n",
      "step 40 total_reward -113.27\n",
      "['-0.33', '+0.67', '-0.46', '-1.01', '-1.66', '+0.08', '+0.00', '+0.00']\n",
      "step 60 total_reward -168.85\n",
      "['-0.37', '+0.29', '-0.04', '-1.45', '-1.24', '+0.70', '+0.00', '+0.00']\n",
      "step 80 total_reward -135.53\n",
      "['-0.36', '+0.07', '+0.60', '-0.54', '-0.79', '+3.97', '+1.00', '+0.00']\n",
      "step 90 total_reward -206.96\n",
      "['+0.00', '+0.95', '+0.13', '+0.26', '-0.00', '+0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.34\n",
      "['+0.01', '+0.94', '-0.07', '-0.30', '+0.43', '+0.82', '+0.00', '+0.00']\n",
      "step 20 total_reward -42.00\n",
      "['-0.01', '+0.77', '-0.13', '-0.86', '+1.54', '+1.16', '+0.00', '+0.00']\n",
      "step 40 total_reward -192.44\n",
      "['-0.07', '+0.40', '-0.49', '-1.71', '+2.92', '+1.58', '+0.00', '+0.00']\n",
      "step 60 total_reward -387.19\n",
      "['-0.14', '+0.01', '-0.65', '-0.15', '+4.21', '+3.68', '+1.00', '+0.00']\n",
      "step 74 total_reward -611.35\n",
      "['-0.00', '+0.95', '-0.24', '+0.33', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.68\n",
      "['-0.06', '+0.96', '-0.31', '-0.21', '+0.33', '+0.36', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.87\n",
      "['-0.12', '+0.82', '-0.31', '-0.75', '+0.69', '+0.36', '+0.00', '+0.00']\n",
      "step 40 total_reward -95.58\n",
      "['-0.22', '+0.55', '-0.73', '-1.10', '+1.11', '+0.70', '+0.00', '+0.00']\n",
      "step 60 total_reward -168.49\n",
      "['-0.37', '+0.13', '-0.76', '-1.67', '+2.14', '+0.97', '+0.00', '+0.00']\n",
      "step 80 total_reward -303.39\n",
      "['-0.46', '-0.13', '-1.61', '-0.73', '+2.47', '-2.21', '+0.00', '+1.00']\n",
      "step 90 total_reward -479.85\n",
      "['+0.00', '+0.95', '+0.11', '+0.14', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.75\n",
      "['+0.02', '+0.90', '+0.11', '-0.39', '-0.03', '-0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -19.26\n",
      "['+0.05', '+0.70', '+0.11', '-0.93', '-0.05', '-0.03', '+0.00', '+0.00']\n",
      "step 40 total_reward -54.26\n",
      "['+0.08', '+0.34', '+0.30', '-1.48', '-0.47', '-0.80', '+0.00', '+0.00']\n",
      "step 60 total_reward -118.80\n",
      "['+0.16', '+0.02', '+0.51', '-0.37', '-2.42', '-3.64', '+0.00', '+0.00']\n",
      "step 80 total_reward -207.22\n",
      "['+0.18', '+0.01', '+0.07', '+0.07', '-2.99', '-0.37', '+0.00', '+0.00']\n",
      "step 84 total_reward -367.38\n",
      "['-0.01', '+0.93', '-0.59', '-0.36', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.43\n",
      "['-0.12', '+0.74', '-0.55', '-0.89', '-0.16', '-0.07', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.96\n",
      "['-0.22', '+0.48', '-0.50', '-0.84', '-0.09', '+0.20', '+0.00', '+0.00']\n",
      "step 40 total_reward -0.54\n",
      "['-0.33', '+0.14', '-0.53', '-1.33', '+0.34', '+0.27', '+0.00', '+0.00']\n",
      "step 60 total_reward -55.13\n",
      "['-0.25', '+0.00', '+0.67', '-0.52', '-0.41', '-1.54', '+1.00', '+0.00']\n",
      "step 78 total_reward -77.29\n",
      "['-0.01', '+0.95', '-0.75', '+0.24', '+0.02', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.92\n",
      "['-0.15', '+0.94', '-0.57', '-0.30', '-0.26', '-0.60', '+0.00', '+0.00']\n",
      "step 20 total_reward -9.60\n",
      "['-0.27', '+0.77', '-0.59', '-0.82', '-0.87', '-0.44', '+0.00', '+0.00']\n",
      "step 40 total_reward -94.02\n",
      "['-0.32', '+0.50', '-0.04', '-1.04', '-1.16', '-0.01', '+0.00', '+0.00']\n",
      "step 60 total_reward -108.09\n",
      "['-0.32', '+0.12', '+0.10', '-1.51', '-0.90', '+0.10', '+0.00', '+0.00']\n",
      "step 80 total_reward -104.99\n",
      "['-0.31', '-0.05', '-0.39', '-0.10', '-1.37', '+0.00', '+1.00', '+0.00']\n",
      "step 88 total_reward -177.18\n",
      "['+0.00', '+0.93', '+0.11', '-0.29', '-0.00', '+0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.66\n",
      "['+0.01', '+0.76', '+0.04', '-0.82', '+0.23', '+0.27', '+0.00', '+0.00']\n",
      "step 20 total_reward -59.07\n",
      "['+0.03', '+0.43', '+0.12', '-1.33', '+0.45', '-0.10', '+0.00', '+0.00']\n",
      "step 40 total_reward -100.03\n",
      "['+0.05', '-0.03', '+0.09', '+0.00', '-0.00', '-0.00', '+1.00', '+1.00']\n",
      "step 60 total_reward -169.02\n",
      "['+0.00', '+0.95', '+0.03', '+0.19', '-0.00', '-0.01', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.23\n",
      "['+0.00', '+0.92', '-0.02', '-0.34', '+0.02', '+0.18', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.07\n",
      "['-0.00', '+0.73', '-0.05', '-0.88', '+0.30', '+0.29', '+0.00', '+0.00']\n",
      "step 40 total_reward -75.14\n",
      "['-0.04', '+0.43', '-0.51', '-0.98', '+0.74', '+0.52', '+0.00', '+0.00']\n",
      "step 60 total_reward -115.79\n",
      "['-0.18', '+0.06', '-1.07', '-1.46', '+1.54', '+0.79', '+0.00', '+0.00']\n",
      "step 80 total_reward -243.87\n",
      "['-0.24', '-0.01', '-1.52', '-0.05', '+2.32', '+2.81', '+0.00', '+1.00']\n",
      "step 85 total_reward -368.57\n",
      "['+0.01', '+0.94', '+0.59', '+0.12', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.16\n",
      "['+0.11', '+0.90', '+0.47', '-0.41', '+0.27', '+0.35', '+0.00', '+0.00']\n",
      "step 20 total_reward -23.94\n",
      "['+0.21', '+0.71', '+0.25', '-0.64', '+0.51', '+0.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -40.05\n",
      "['+0.18', '+0.56', '-0.54', '-0.36', '+0.75', '+0.28', '+0.00', '+0.00']\n",
      "step 60 total_reward -51.52\n",
      "['+0.02', '+0.43', '-0.80', '-0.64', '+0.80', '-0.34', '+0.00', '+0.00']\n",
      "step 80 total_reward -79.77\n",
      "['-0.14', '+0.17', '-0.86', '-0.97', '+0.25', '-0.71', '+0.00', '+0.00']\n",
      "step 100 total_reward -33.51\n",
      "['-0.23', '-0.00', '-0.32', '-0.77', '-0.48', '-4.94', '+1.00', '+1.00']\n",
      "step 111 total_reward -119.48\n",
      "['-0.01', '+0.93', '-0.27', '-0.20', '+0.00', '+0.02', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.44\n",
      "['-0.04', '+0.79', '-0.15', '-0.74', '-0.33', '-0.47', '+0.00', '+0.00']\n",
      "step 20 total_reward -60.72\n",
      "['-0.08', '+0.49', '-0.09', '-1.11', '-0.67', '-0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -103.16\n",
      "['-0.10', '+0.09', '-0.07', '-1.59', '-0.48', '+0.09', '+0.00', '+0.00']\n",
      "step 60 total_reward -96.16\n",
      "['-0.10', '-0.01', '-0.25', '-0.55', '-0.42', '+5.25', '+1.00', '+0.00']\n",
      "step 64 total_reward -191.78\n",
      "['-0.01', '+0.95', '-0.43', '+0.29', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.42\n",
      "['-0.08', '+0.95', '-0.28', '-0.25', '-0.24', '-0.55', '+0.00', '+0.00']\n",
      "step 20 total_reward -8.10\n",
      "['-0.14', '+0.80', '-0.24', '-0.79', '-0.94', '-0.71', '+0.00', '+0.00']\n",
      "step 40 total_reward -108.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.14', '+0.50', '+0.33', '-1.20', '-1.55', '-0.42', '+0.00', '+0.00']\n",
      "step 60 total_reward -184.97\n",
      "['-0.07', '+0.07', '+0.33', '-1.69', '-1.58', '-0.13', '+0.00', '+0.00']\n",
      "step 80 total_reward -195.40\n",
      "['-0.06', '+0.01', '+0.56', '+0.14', '-2.04', '-1.81', '+1.00', '+0.00']\n",
      "step 84 total_reward -262.37\n",
      "['-0.00', '+0.95', '-0.14', '+0.19', '+0.00', '-0.00', '+0.00', '+0.00']\n",
      "step 0 total_reward +2.31\n",
      "['-0.02', '+0.92', '+0.05', '-0.35', '-0.41', '-0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward -47.84\n",
      "['+0.00', '+0.73', '+0.16', '-1.00', '-1.66', '-1.72', '+0.00', '+0.00']\n",
      "step 40 total_reward -218.84\n",
      "['+0.04', '+0.34', '+0.00', '-1.57', '-3.88', '-2.62', '+0.00', '+0.00']\n",
      "step 60 total_reward -459.94\n",
      "['-0.00', '+0.00', '-0.90', '-0.00', '-5.29', '+2.96', '+0.00', '+1.00']\n",
      "step 73 total_reward -713.62\n",
      "['+0.01', '+0.94', '+0.42', '-0.13', '-0.01', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.43\n",
      "['+0.08', '+0.81', '+0.26', '-0.67', '+0.34', '+0.59', '+0.00', '+0.00']\n",
      "step 20 total_reward -48.64\n",
      "['+0.11', '+0.56', '+0.03', '-0.97', '+0.86', '+0.27', '+0.00', '+0.00']\n",
      "step 40 total_reward -103.41\n",
      "['+0.13', '+0.20', '+0.15', '-1.41', '+0.63', '-0.54', '+0.00', '+0.00']\n",
      "step 60 total_reward -93.58\n",
      "['+0.14', '-0.02', '+0.27', '-0.71', '+0.15', '-5.89', '+1.00', '+1.00']\n",
      "step 70 total_reward -168.98\n",
      "['-0.01', '+0.94', '-0.28', '+0.03', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.18\n",
      "['-0.05', '+0.87', '-0.13', '-0.51', '-0.33', '-0.54', '+0.00', '+0.00']\n",
      "step 20 total_reward -49.50\n",
      "['-0.08', '+0.64', '-0.10', '-0.99', '-0.86', '-0.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -126.92\n",
      "['-0.06', '+0.31', '+0.16', '-1.26', '-1.00', '+0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -137.73\n",
      "['+0.00', '+0.05', '+0.20', '+0.03', '-1.50', '-1.74', '+0.00', '+0.00']\n",
      "step 80 total_reward -57.34\n",
      "['+0.04', '+0.00', '+0.07', '-0.06', '-3.28', '+0.42', '+0.00', '+0.00']\n",
      "step 97 total_reward -352.23\n",
      "['-0.01', '+0.95', '-0.46', '+0.17', '+0.01', '+0.07', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.07\n",
      "['-0.08', '+0.91', '-0.26', '-0.37', '-0.35', '-0.74', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.94\n",
      "['-0.14', '+0.72', '-0.19', '-0.89', '-1.17', '-0.83', '+0.00', '+0.00']\n",
      "step 40 total_reward -137.04\n",
      "['-0.11', '+0.40', '+0.39', '-1.32', '-1.74', '-0.30', '+0.00', '+0.00']\n",
      "step 60 total_reward -211.14\n",
      "['+0.01', '-0.00', '+0.89', '-0.09', '-1.78', '+0.80', '+1.00', '+0.00']\n",
      "step 78 total_reward -333.66\n",
      "['+0.00', '+0.93', '+0.21', '-0.40', '-0.01', '-0.08', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.52\n",
      "['+0.06', '+0.72', '+0.31', '-0.94', '-0.43', '-0.48', '+0.00', '+0.00']\n",
      "step 20 total_reward -78.19\n",
      "['+0.15', '+0.41', '+0.59', '-1.33', '-1.22', '-1.21', '+0.00', '+0.00']\n",
      "step 40 total_reward -177.72\n",
      "['+0.25', '+0.02', '+0.12', '-0.73', '-2.76', '-6.99', '+0.00', '+0.00']\n",
      "step 57 total_reward -427.50\n",
      "['+0.01', '+0.93', '+0.64', '-0.35', '-0.01', '-0.11', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.30\n",
      "['+0.13', '+0.74', '+0.56', '-0.89', '+0.13', '+0.19', '+0.00', '+0.00']\n",
      "step 20 total_reward -26.80\n",
      "['+0.23', '+0.47', '+0.43', '-0.96', '+0.25', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -19.31\n",
      "['+0.32', '+0.10', '+0.51', '-1.48', '-0.05', '-0.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -32.17\n",
      "['+0.33', '+0.06', '-0.30', '-0.59', '+0.02', '+4.50', '+1.00', '+0.00']\n",
      "step 62 total_reward -126.36\n",
      "['-0.01', '+0.94', '-0.54', '+0.02', '+0.01', '+0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.49\n",
      "['-0.10', '+0.87', '-0.40', '-0.51', '-0.27', '-0.47', '+0.00', '+0.00']\n",
      "step 20 total_reward -29.70\n",
      "['-0.17', '+0.66', '-0.01', '-0.73', '-0.73', '-0.42', '+0.00', '+0.00']\n",
      "step 40 total_reward -66.86\n",
      "['-0.10', '+0.44', '+0.62', '-0.82', '-1.07', '-0.19', '+0.00', '+0.00']\n",
      "step 60 total_reward -111.65\n",
      "['+0.05', '+0.14', '+1.19', '-1.05', '-0.89', '+0.32', '+0.00', '+0.00']\n",
      "step 80 total_reward -122.60\n",
      "['+0.16', '+0.01', '+0.94', '+0.01', '-0.79', '-0.32', '+1.00', '+0.00']\n",
      "step 89 total_reward -211.29\n",
      "['+0.01', '+0.94', '+0.58', '+0.08', '-0.01', '-0.09', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.76\n",
      "['+0.11', '+0.88', '+0.44', '-0.46', '+0.27', '+0.44', '+0.00', '+0.00']\n",
      "step 20 total_reward -25.21\n",
      "['+0.20', '+0.67', '+0.19', '-0.81', '+0.70', '+0.40', '+0.00', '+0.00']\n",
      "step 40 total_reward -71.42\n",
      "['+0.13', '+0.45', '-0.72', '-0.81', '+1.11', '+0.29', '+0.00', '+0.00']\n",
      "step 60 total_reward -119.11\n",
      "['-0.01', '+0.13', '-0.68', '-1.29', '+1.02', '-0.20', '+0.00', '+0.00']\n",
      "step 80 total_reward -114.25\n",
      "['-0.06', '+0.00', '-1.03', '-0.31', '+1.23', '+5.03', '+0.00', '+1.00']\n",
      "step 87 total_reward -209.96\n",
      "['-0.01', '+0.95', '-0.52', '+0.45', '+0.01', '+0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.48\n",
      "['-0.11', '+1.00', '-0.53', '-0.09', '+0.13', '+0.12', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.22\n",
      "['-0.22', '+0.89', '-0.53', '-0.62', '+0.25', '+0.12', '+0.00', '+0.00']\n",
      "step 40 total_reward -32.00\n",
      "['-0.33', '+0.67', '-0.71', '-0.69', '+0.38', '+0.17', '+0.00', '+0.00']\n",
      "step 60 total_reward -48.91\n",
      "['-0.53', '+0.48', '-1.13', '-0.82', '+0.63', '+0.53', '+0.00', '+0.00']\n",
      "step 80 total_reward -114.87\n",
      "['-0.76', '+0.14', '-1.23', '-1.46', '+1.65', '+1.45', '+0.00', '+0.00']\n",
      "step 100 total_reward -274.98\n",
      "['-0.91', '-0.12', '-1.15', '-0.07', '+2.62', '+3.47', '+0.00', '+0.00']\n",
      "step 112 total_reward -490.78\n",
      "['+0.00', '+0.95', '+0.06', '+0.43', '-0.00', '-0.05', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.52\n",
      "['+0.03', '+1.00', '+0.25', '-0.13', '-0.47', '-0.86', '+0.00', '+0.00']\n",
      "step 20 total_reward -35.01\n",
      "['+0.09', '+0.87', '+0.34', '-0.74', '-1.76', '-1.55', '+0.00', '+0.00']\n",
      "step 40 total_reward -205.07\n",
      "['+0.18', '+0.54', '+0.53', '-1.70', '-3.29', '-1.50', '+0.00', '+0.00']\n",
      "step 60 total_reward -426.73\n",
      "['+0.26', '+0.04', '-0.95', '-0.68', '-4.58', '+4.33', '+0.00', '+1.00']\n",
      "step 76 total_reward -661.14\n",
      "['+0.01', '+0.94', '+0.49', '+0.03', '-0.01', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.66\n",
      "['+0.12', '+0.87', '+0.57', '-0.50', '-0.43', '-0.47', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.91\n",
      "['+0.24', '+0.65', '+0.78', '-0.89', '-0.90', '-0.54', '+0.00', '+0.00']\n",
      "step 40 total_reward -137.09\n",
      "['+0.47', '+0.34', '+1.29', '-1.31', '-1.66', '-1.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -270.25\n",
      "['+0.65', '+0.03', '+1.78', '-0.45', '-2.75', '-2.85', '+0.00', '+0.00']\n",
      "step 74 total_reward -502.44\n",
      "['+0.01', '+0.94', '+0.56', '-0.01', '-0.01', '-0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.40\n",
      "['+0.11', '+0.85', '+0.37', '-0.56', '+0.31', '+0.69', '+0.00', '+0.00']\n",
      "step 20 total_reward -32.39\n",
      "['+0.15', '+0.63', '-0.24', '-0.81', '+1.19', '+0.93', '+0.00', '+0.00']\n",
      "step 40 total_reward -119.86\n",
      "['+0.04', '+0.34', '-0.59', '-1.18', '+1.79', '+0.14', '+0.00', '+0.00']\n",
      "step 60 total_reward -199.43\n",
      "['-0.08', '+0.01', '-0.65', '+0.13', '+2.12', '+2.04', '+0.00', '+0.00']\n",
      "step 78 total_reward -274.09\n",
      "['+0.01', '+0.93', '+0.73', '-0.41', '-0.01', '-0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.41\n",
      "['+0.15', '+0.72', '+0.69', '-0.94', '+0.00', '+0.03', '+0.00', '+0.00']\n",
      "step 20 total_reward -13.25\n",
      "['+0.28', '+0.50', '+0.65', '-0.71', '-0.03', '-0.18', '+0.00', '+0.00']\n",
      "step 40 total_reward +16.17\n",
      "['+0.43', '+0.20', '+0.80', '-1.26', '-0.61', '-0.81', '+0.00', '+0.00']\n",
      "step 60 total_reward -85.82\n",
      "['+0.53', '+0.10', '+0.41', '-0.16', '-0.51', '+1.24', '+0.00', '+0.00']\n",
      "step 80 total_reward +22.07\n",
      "['+0.57', '+0.07', '+0.36', '-0.25', '-0.02', '+2.00', '+0.00', '+1.00']\n",
      "step 87 total_reward -45.69\n",
      "['+0.01', '+0.93', '+0.75', '-0.35', '-0.02', '-0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.27\n",
      "['+0.15', '+0.74', '+0.69', '-0.88', '-0.00', '+0.05', '+0.00', '+0.00']\n",
      "step 20 total_reward -12.51\n",
      "['+0.30', '+0.52', '+0.75', '-0.66', '+0.11', '+0.07', '+0.00', '+0.00']\n",
      "step 40 total_reward -1.40\n",
      "['+0.47', '+0.24', '+0.96', '-1.19', '-0.26', '-0.75', '+0.00', '+0.00']\n",
      "step 60 total_reward -62.73\n",
      "['+0.66', '-0.03', '+0.96', '-0.02', '-1.19', '+0.03', '+1.00', '+0.00']\n",
      "step 80 total_reward -102.40\n",
      "['+0.86', '-0.10', '+1.05', '-0.48', '-0.84', '+0.13', '+0.00', '+0.00']\n",
      "step 100 total_reward -118.72\n",
      "['+0.93', '-0.15', '+1.14', '-0.52', '-0.57', '+1.40', '+1.00', '+0.00']\n",
      "step 106 total_reward -203.75\n",
      "['+0.00', '+0.96', '+0.25', '+0.47', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.77\n",
      "['+0.04', '+1.01', '+0.05', '-0.08', '+0.38', '+0.77', '+0.00', '+0.00']\n",
      "step 20 total_reward +1.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+0.04', '+0.90', '-0.06', '-0.71', '+1.60', '+1.65', '+0.00', '+0.00']\n",
      "step 40 total_reward -171.13\n",
      "['+0.02', '+0.61', '-0.03', '-1.27', '+3.46', '+1.87', '+0.00', '+0.00']\n",
      "step 60 total_reward -384.98\n",
      "['-0.00', '+0.12', '-0.12', '-1.80', '+5.02', '+1.72', '+0.00', '+0.00']\n",
      "step 80 total_reward -546.40\n",
      "['-0.01', '+0.00', '+0.20', '-0.11', '+5.15', '-4.52', '+1.00', '+0.00']\n",
      "step 85 total_reward -668.18\n",
      "['-0.00', '+0.93', '-0.03', '-0.23', '-0.00', '-0.03', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.06\n",
      "['+0.01', '+0.78', '+0.08', '-0.76', '-0.34', '-0.47', '+0.00', '+0.00']\n",
      "step 20 total_reward -75.15\n",
      "['+0.02', '+0.47', '+0.14', '-1.33', '-0.85', '-0.82', '+0.00', '+0.00']\n",
      "step 40 total_reward -152.99\n",
      "['+0.05', '+0.00', '+0.26', '+0.06', '-2.02', '-0.84', '+1.00', '+0.00']\n",
      "step 60 total_reward -340.53\n",
      "['+0.00', '+0.95', '+0.18', '+0.43', '-0.00', '-0.04', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.53\n",
      "['+0.04', '+1.00', '+0.18', '-0.10', '-0.05', '-0.04', '+0.00', '+0.00']\n",
      "step 20 total_reward +19.04\n",
      "['+0.08', '+0.89', '+0.18', '-0.63', '-0.09', '-0.04', '+0.00', '+0.00']\n",
      "step 40 total_reward -19.02\n",
      "['+0.11', '+0.62', '+0.25', '-1.11', '-0.14', '-0.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -46.93\n",
      "['+0.18', '+0.24', '+0.44', '-1.52', '-0.62', '-0.71', '+0.00', '+0.00']\n",
      "step 80 total_reward -106.88\n",
      "['+0.21', '+0.02', '-0.48', '-0.09', '-1.74', '-1.76', '+1.00', '+0.00']\n",
      "step 91 total_reward -210.92\n",
      "['-0.01', '+0.93', '-0.52', '-0.45', '+0.01', '+0.16', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.66\n",
      "['-0.10', '+0.71', '-0.46', '-0.88', '-0.06', '-0.14', '+0.00', '+0.00']\n",
      "step 20 total_reward -18.18\n",
      "['-0.19', '+0.50', '-0.36', '-0.57', '-0.11', '+0.01', '+0.00', '+0.00']\n",
      "step 40 total_reward +22.74\n",
      "['-0.27', '+0.26', '-0.48', '-0.95', '+0.20', '+0.43', '+0.00', '+0.00']\n",
      "step 60 total_reward -11.20\n",
      "['-0.36', '-0.04', '-0.43', '-1.24', '+0.37', '-0.11', '+0.00', '+0.00']\n",
      "step 80 total_reward -53.08\n",
      "['-0.39', '-0.11', '-0.94', '-0.26', '+0.40', '-0.09', '+0.00', '+1.00']\n",
      "step 84 total_reward -138.27\n",
      "['+0.00', '+0.95', '+0.13', '+0.44', '-0.00', '-0.06', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.28\n",
      "['+0.05', '+1.00', '+0.33', '-0.12', '-0.49', '-0.87', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.31\n",
      "['+0.12', '+0.88', '+0.39', '-0.69', '-1.67', '-1.26', '+0.00', '+0.00']\n",
      "step 40 total_reward -192.78\n",
      "['+0.22', '+0.58', '+0.66', '-1.54', '-2.92', '-1.25', '+0.00', '+0.00']\n",
      "step 60 total_reward -381.33\n",
      "['+0.36', '+0.01', '+0.63', '-2.13', '-4.39', '-1.54', '+0.00', '+0.00']\n",
      "step 80 total_reward -558.21\n",
      "['+0.36', '+0.00', '+0.31', '-0.08', '-4.32', '-1.01', '+0.00', '+1.00']\n",
      "step 81 total_reward -658.21\n",
      "['-0.01', '+0.94', '-0.74', '-0.11', '+0.02', '+0.20', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.86\n",
      "['-0.15', '+0.82', '-0.64', '-0.64', '-0.10', '-0.21', '+0.00', '+0.00']\n",
      "step 20 total_reward -15.72\n",
      "['-0.27', '+0.59', '-0.64', '-0.93', '-0.13', '+0.23', '+0.00', '+0.00']\n",
      "step 40 total_reward -24.29\n",
      "['-0.41', '+0.22', '-0.77', '-1.46', '+0.48', '+0.72', '+0.00', '+0.00']\n",
      "step 60 total_reward -94.13\n",
      "['-0.48', '+0.02', '-0.97', '-0.98', '+0.58', '-4.68', '+0.00', '+1.00']\n",
      "step 69 total_reward -210.61\n",
      "['-0.01', '+0.94', '-0.45', '-0.09', '+0.01', '+0.10', '+0.00', '+0.00']\n",
      "step 0 total_reward -0.75\n",
      "['-0.10', '+0.83', '-0.45', '-0.62', '+0.11', '+0.10', '+0.00', '+0.00']\n",
      "step 20 total_reward -31.24\n",
      "['-0.20', '+0.61', '-0.63', '-0.58', '+0.21', '+0.09', '+0.00', '+0.00']\n",
      "step 40 total_reward -33.38\n",
      "['-0.34', '+0.35', '-0.80', '-1.11', '+0.66', '+0.74', '+0.00', '+0.00']\n",
      "step 60 total_reward -116.44\n",
      "['-0.50', '-0.04', '-1.08', '-0.67', '+1.47', '+6.04', '+0.00', '+1.00']\n",
      "step 80 total_reward -298.67\n",
      "['-0.01', '+0.94', '-0.41', '-0.17', '+0.01', '+0.13', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.10\n",
      "['-0.10', '+0.80', '-0.47', '-0.70', '+0.35', '+0.36', '+0.00', '+0.00']\n",
      "step 20 total_reward -63.80\n",
      "['-0.23', '+0.57', '-0.90', '-0.78', '+0.77', '+0.61', '+0.00', '+0.00']\n",
      "step 40 total_reward -124.33\n",
      "['-0.41', '+0.25', '-0.96', '-1.38', '+1.82', '+1.18', '+0.00', '+0.00']\n",
      "step 60 total_reward -265.19\n",
      "['-0.50', '+0.08', '-0.48', '+0.05', '+2.27', '+1.47', '+0.00', '+0.00']\n",
      "step 69 total_reward -420.50\n",
      "['-0.01', '+0.95', '-0.61', '+0.24', '+0.02', '+0.17', '+0.00', '+0.00']\n",
      "step 0 total_reward -1.00\n",
      "['-0.12', '+0.93', '-0.53', '-0.30', '-0.07', '-0.17', '+0.00', '+0.00']\n",
      "step 20 total_reward -1.05\n",
      "['-0.24', '+0.76', '-0.62', '-0.83', '-0.01', '+0.22', '+0.00', '+0.00']\n",
      "step 40 total_reward -24.96\n",
      "['-0.37', '+0.50', '-0.84', '-0.90', '+0.40', '+0.59', '+0.00', '+0.00']\n",
      "step 60 total_reward -69.74\n",
      "['-0.55', '+0.15', '-0.92', '-1.42', '+1.24', '+0.70', '+0.00', '+0.00']\n",
      "step 80 total_reward -194.38\n",
      "['-0.62', '+0.05', '-1.28', '+0.07', '+2.19', '+2.84', '+0.00', '+0.00']\n",
      "step 87 total_reward -325.51\n",
      "['+0.02', '+0.94', '+0.80', '+0.05', '-0.02', '-0.15', '+0.00', '+0.00']\n",
      "step 0 total_reward +0.18\n",
      "['+0.17', '+0.87', '+0.80', '-0.49', '-0.00', '-0.15', '+0.00', '+0.00']\n",
      "step 20 total_reward -7.10\n",
      "['+0.33', '+0.64', '+0.88', '-1.03', '-0.23', '-0.50', '+0.00', '+0.00']\n",
      "step 40 total_reward -55.25\n",
      "['+0.51', '+0.25', '+1.03', '-1.64', '-1.15', '-1.35', '+0.00', '+0.00']\n",
      "step 60 total_reward -190.81\n",
      "['+0.64', '-0.07', '+1.31', '-0.17', '-2.20', '+0.04', '+1.00', '+0.00']\n",
      "step 72 total_reward -406.66\n",
      "['-0.02', '+0.94', '-0.77', '-0.07', '+0.02', '+0.22', '+0.00', '+0.00']\n",
      "step 0 total_reward -2.35\n",
      "['-0.17', '+0.83', '-0.77', '-0.61', '+0.29', '+0.22', '+0.00', '+0.00']\n",
      "step 20 total_reward -41.73\n",
      "['-0.36', '+0.65', '-1.27', '-0.50', '+0.62', '+0.35', '+0.00', '+0.00']\n",
      "step 40 total_reward -106.78\n",
      "['-0.64', '+0.43', '-1.47', '-1.04', '+1.33', '+1.13', '+0.00', '+0.00']\n",
      "step 60 total_reward -225.60\n",
      "['-0.91', '+0.07', '-0.08', '-0.63', '+2.60', '-4.90', '+0.00', '+0.00']\n",
      "step 79 total_reward -517.37\n",
      "['+0.01', '+0.95', '+0.74', '+0.31', '-0.01', '-0.12', '+0.00', '+0.00']\n",
      "step 0 total_reward +1.02\n",
      "['+0.17', '+0.96', '+0.91', '-0.24', '-0.48', '-0.83', '+0.00', '+0.00']\n",
      "step 20 total_reward -62.02\n",
      "['+0.35', '+0.81', '+0.93', '-0.79', '-1.43', '-0.96', '+0.00', '+0.00']\n",
      "step 40 total_reward -176.11\n",
      "['+0.54', '+0.48', '+0.83', '-1.43', '-2.89', '-1.91', '+0.00', '+0.00']\n",
      "step 60 total_reward -350.49\n",
      "['+0.66', '+0.13', '+1.23', '-0.57', '-4.45', '-5.87', '+0.00', '+1.00']\n",
      "step 74 total_reward -610.52\n",
      "total rewards [-319.311672750462, -240.55629203986155, -248.7745175017353, -653.4753848973066, -271.9841156896605, -53.90009833351996, -259.957438894839, -18.854223764892325, -75.2306408859653, -469.5683603022238, -100.40026253181571, -60.0649457743998, -315.6749675594042, -350.47497236543353, -564.6697273813304, -113.68864891704308, -361.3645628315952, -246.88755417845712, -149.92027832648546, -121.1878700625337, -187.26653827732568, -145.33105066358206, -287.5084325011211, -196.23006736731804, -342.2837317770229, -613.594562560396, -457.7670476484617, -38.249226980375596, -367.73918219338435, -284.57086307620295, -516.7955913842109, -396.0118612951918, -123.04532395511005, -112.25562990770055, -147.93495011122815, -195.41331086314392, -190.4835923816111, -136.78052047529638, -267.00936707050835, -453.9257749028552, -471.3347029876934, -199.61694912777887, -325.5169654557757, -186.0312111574777, -335.3782959388831, -185.46265180736356, -231.68805991422474, -185.3521249886329, -110.36758494442094, -653.0726902525006, -642.5130487628759, -465.49093079124054, -358.25056467740205, -575.4110831347532, -296.09632573975364, -112.5820205789401, -323.7528135496082, -381.3615812618813, -203.50807384064734, -161.6514479969922, -116.43647793334286, -475.4634209143024, -583.8541804804959, -251.8489428968923, -314.57540300115977, -154.10204609852954, -151.9647808454494, -413.8570046467134, -335.60095739053224, -344.9892022235233, -153.63551397457934, -376.76342699845526, -161.11588633584506, -405.9034336525361, -607.210414469399, -95.64250366465205, -354.92771197089974, -227.0712394650278, -386.809663267587, -172.32191563156664, -330.67072878001443, -453.8013001143665, -297.95867195197627, -193.59666255350092, -176.8680672215396, -95.9413008798559, -250.84930903414974, -289.8468186830489, -192.45221960869003, -305.47243711714776, -253.5756662329822, -429.536034600774, -473.03820828956657, -482.73525203223204, -168.5756130707229, -604.5037742036254, -653.6271344531898, -289.490723406818, -180.7468891025392, -516.2572765274538, -106.34245818689149, -373.81313824709844, -521.8734310471339, -108.96277662656053, -506.4875952938378, -268.83516875486737, -557.8481556810368, -451.7481975782979, -206.715040470759, -573.2541140316246, -322.7349436827502, -324.58411906056836, -498.6812793262585, -293.52936916985834, -154.7435970976869, -485.72911164639635, -78.19229051523668, -414.62362847098507, -221.00783442063138, -167.5702162371162, -143.08341649181702, -355.6065269057349, -271.6884565708209, -195.6245672802475, -135.84537685748336, -290.44904549630667, -569.8474680054571, -68.51076282649552, -143.10257045857645, -205.67540276148122, -542.6396415965316, -213.41259710155393, -157.42765978857489, -183.4084718761772, -257.1302672757264, -195.13824559086498, -382.48947080883664, -550.198222627209, -77.61872288373178, -303.30283278622653, -710.2345667510976, -462.2649102416406, -265.6089786397883, -175.39307534696943, -119.64881881071804, -273.7074057909145, -124.45360463513511, -290.5542568704739, -470.89562649942854, -76.9588982907039, -310.40975623961236, -128.58473654448747, -375.6578790401622, -171.8774854572194, -191.7804661190484, -117.9222765740715, -117.08667100165367, -204.04764158258678, -101.2787656205579, -184.2811873978151, -332.49612264012467, -223.15459208080804, -88.78182330125358, -206.95856048913157, -611.3491893715045, -479.8524571207232, -367.38211304786574, -77.29417165396704, -177.17768138669567, -169.01704472985125, -368.5673085129897, -119.48224330861221, -191.78291936474653, -262.3749392862205, -713.6216942009426, -168.9791271853805, -352.228240732458, -333.6648043779306, -427.5011103651396, -126.36156257876979, -211.29454047342074, -209.95622274411176, -490.7785067539051, -661.1396058352567, -502.4359620818351, -274.0903079551888, -45.693471818357935, -203.75400064655713, -668.1796846061866, -340.5274160719446, -210.92371437059003, -138.26900070783958, -658.2114095002656, -210.6141567331862, -298.67430342243307, -420.5041189876796, -325.50881766273767, -406.66473401834634, -517.3669618866222, -610.5189326130946]\n",
      "average total reward -297.6973863731488\n"
     ]
    }
   ],
   "source": [
    "!python lunar_lander_ml_images_player_model6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_model_reward_comparisons[\"Model 6\"] = '-297.6973863731488'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list=[model_test_accuracy_comparisons,\n",
    "model_test_precision_comparisons,\n",
    "model_test_recall_comparisons,\n",
    "model_test_f1_comparisons,\n",
    "model_train_time_comparisons,\n",
    "task1_model_reward_comparisons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Model 1': 0.73992673992674, 'Model 2': 0.5013082155939299, 'Model 3': 0.5170068027210885, 'Model 4': 0.5463108320251178, 'Model 5': 0.5133437990580848, 'Model 6': 0.456305599162742}, {'Model 1': ' 0.73', 'Model 2': ' 0.67', 'Model 3': ' 0.68', 'Model 4': ' 0.64', 'Model 5': ' 0.67', 'Model 6': ' 0.64'}, {'Model 1': '0.74', 'Model 2': '0.50', 'Model 3': '0.52', 'Model 4': '0.55', 'Model 5': '0.51', 'Model 6': '0.46'}, {'Model 1': '0.72', 'Model 2': '0.56', 'Model 3': '0.57', 'Model 4': '0.58', 'Model 5': '0.57', 'Model 6': '0.51'}, {'Model 1': 2378.4542820453644, 'Model 2': 312.3986496925354, 'Model 3': 305.4613618850708, 'Model 4': 341.64512634277344, 'Model 5': 333.92546582221985, 'Model 6': 310.34310507774353}, {'Model 1': '-314.5653621855106', 'Model 2': '-371.15323528526295', 'Model 3': '-454.32385020153396', 'Model 4': '-420.58271429697925', 'Model 5': '-446.5792155174349', 'Model 6': '-297.6973863731488'}]\n"
     ]
    }
   ],
   "source": [
    "filename = 'part1_model_results.txt'\n",
    "\n",
    "with open(filename, 'w') as fd:\n",
    "    fd.write(json.dumps(dict_list))\n",
    "\n",
    "with open(filename, 'r') as fd:\n",
    "    print(json.load(fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
